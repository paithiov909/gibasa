[{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"gibasaパッケージについて","dir":"Articles","previous_headings":"","what":"gibasaパッケージについて","title":"Introduction to gibasa","text":"gibasaは、RからMeCabを利用して形態素解析をおこなうためのパッケージです。 モチベーションとしては、tidytext::unnest_tokensを意識した処理をMeCabを利用しつつできるようにしたいということで開発しています。","code":""},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"インストールの仕方","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"インストールの仕方","title":"Introduction to gibasa","text":"r-universeからインストールできます。 バイナリパッケージが用意されていない環境では、ソースパッケージをビルドしてインストールします。ビルド時にMECAB_DEFAULT_RCという環境変数を内部的に指定するため、正しく動作させるにはmecabrcとMeCabの辞書があらかじめ適切な位置に配置されている必要があります。使っているOSのパッケージマネージャなどからインストールしておいてください。 Windowsの場合、ソースビルドにはRtools42が必要です。また、インストールの方法によらず、gibasaを使用するにはMeCabの辞書が必要なため、これなどを使ってMeCabをインストールしておいてください。","code":"# Enable repository from paithiov909 options(repos = c(   paithiov909 = \"https://paithiov909.r-universe.dev\",   CRAN = \"https://cloud.r-project.org\"))  # Download and install gibasa in R install.packages(\"gibasa\") # Sys.setenv(MECAB_DEFAULT_RC = \"/fullpath/to/your/mecabrc\") # if necessary remotes::install_github(\"paithiov909/gibasa\")"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"基本的な使い方","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"基本的な使い方","title":"Introduction to gibasa","text":"gibasaは、次にあげる関数を使って、CJKテキストの分かち書きをすることができるというパッケージです。 gibasa::tokenize gibasa::prettify gibasa::pack まず、doc_id列とtext列をもつデータフレームについて、gibasa::tokenizeでtidy textのかたちにできます（以下の例ではIPA辞書を使っています）。ちなみに、元のデータフレームのdoc_id列とtext列以外の列は戻り値にも保持されます。 gibasa::prettifyでfeature列の素性情報をパースして分割できます。このとき、col_select引数でパースしたい列を指定すると、それらの列だけをパースすることができます。 gibasa::packを使うと、pull引数で指定した列について、いわゆる「分かち書き」にすることができます。デフォルトではtoken列について分かち書きにします。","code":"audubon::polano[5] #> [1] \"そのころわたくしは、モリーオ市の博物局に勤めて居りました。\"  dat <- data.frame(   doc_id = seq_along(audubon::polano[5:8]),   text = audubon::polano[5:8],   meta = c(\"aaa\", \"bbb\", \"ccc\", \"ddd\") )  res <- gibasa::tokenize(dat, text, doc_id)  head(res) #>   doc_id meta sentence_id token_id    token #> 1      1  aaa           1        1     その #> 2      1  aaa           1        2     ころ #> 3      1  aaa           1        3 わたくし #> 4      1  aaa           1        4       は #> 5      1  aaa           1        5       、 #> 6      1  aaa           1        6 モリーオ #>                                             feature #> 1                   連体詞,*,*,*,*,*,その,ソノ,ソノ #> 2         名詞,非自立,副詞可能,*,*,*,ころ,コロ,コロ #> 3 名詞,代名詞,一般,*,*,*,わたくし,ワタクシ,ワタクシ #> 4                      助詞,係助詞,*,*,*,*,は,ハ,ワ #> 5                        記号,読点,*,*,*,*,、,、,、 #> 6                     名詞,固有名詞,地域,一般,*,*,* head(gibasa::prettify(res)) #>   doc_id meta sentence_id token_id    token   POS1     POS2     POS3 POS4 #> 1      1  aaa           1        1     その 連体詞     <NA>     <NA> <NA> #> 2      1  aaa           1        2     ころ   名詞   非自立 副詞可能 <NA> #> 3      1  aaa           1        3 わたくし   名詞   代名詞     一般 <NA> #> 4      1  aaa           1        4       は   助詞   係助詞     <NA> <NA> #> 5      1  aaa           1        5       、   記号     読点     <NA> <NA> #> 6      1  aaa           1        6 モリーオ   名詞 固有名詞     地域 一般 #>   X5StageUse1 X5StageUse2 Original    Yomi1    Yomi2 #> 1        <NA>        <NA>     その     ソノ     ソノ #> 2        <NA>        <NA>     ころ     コロ     コロ #> 3        <NA>        <NA> わたくし ワタクシ ワタクシ #> 4        <NA>        <NA>       は       ハ       ワ #> 5        <NA>        <NA>       、       、       、 #> 6        <NA>        <NA>     <NA> head(gibasa::prettify(res, col_select = 1:3)) #>   doc_id meta sentence_id token_id    token   POS1     POS2     POS3 #> 1      1  aaa           1        1     その 連体詞     <NA>     <NA> #> 2      1  aaa           1        2     ころ   名詞   非自立 副詞可能 #> 3      1  aaa           1        3 わたくし   名詞   代名詞     一般 #> 4      1  aaa           1        4       は   助詞   係助詞     <NA> #> 5      1  aaa           1        5       、   記号     読点     <NA> #> 6      1  aaa           1        6 モリーオ   名詞 固有名詞     地域 head(gibasa::prettify(res, col_select = c(1, 3, 5))) #>   doc_id meta sentence_id token_id    token   POS1     POS3 X5StageUse1 #> 1      1  aaa           1        1     その 連体詞     <NA>        <NA> #> 2      1  aaa           1        2     ころ   名詞 副詞可能        <NA> #> 3      1  aaa           1        3 わたくし   名詞     一般        <NA> #> 4      1  aaa           1        4       は   助詞     <NA>        <NA> #> 5      1  aaa           1        5       、   記号     <NA>        <NA> #> 6      1  aaa           1        6 モリーオ   名詞     地域        <NA> head(gibasa::prettify(res, col_select = c(\"POS1\", \"Original\"))) #>   doc_id meta sentence_id token_id    token   POS1 Original #> 1      1  aaa           1        1     その 連体詞     その #> 2      1  aaa           1        2     ころ   名詞     ころ #> 3      1  aaa           1        3 わたくし   名詞 わたくし #> 4      1  aaa           1        4       は   助詞       は #> 5      1  aaa           1        5       、   記号       、 #> 6      1  aaa           1        6 モリーオ   名詞     <NA> res <- gibasa::prettify(res) gibasa::pack(res) #>   doc_id #> 1      1 #> 2      2 #> 3      3 #> 4      4 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text #> 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     その ころ わたくし は 、 モリーオ 市 の 博物 局 に 勤め て 居り まし た 。 #> 2 十 八 等 官 でし た から 役所 の なか でも 、 ず うっ と 下 の 方 でし た し 俸給 も ほんの わずか でし た が 、 受持ち が 標本 の 採集 や 整理 で 生れ 付き 好き な こと でし た から 、 わたくし は 毎日 ずいぶん 愉快 に はたらき まし た 。 殊に その ころ 、 モリーオ 市 で は 競馬 場 を 植物 園 に 拵え 直す と いう ので 、 その 景色 の いい まわり に アカシヤ を 植え込ん だ 広い 地面 が 、 切符 売場 や 信号 所 の 建物 の つい た まま 、 わたくし ども の 役所 の 方 へ まわっ て 来 た もの です から 、 わたくし は すぐ 宿直 という 名前 で 月賦 で 買っ た 小さな 蓄音器 と 二 十 枚 ばかり の レコード を もっ て 、 その 番小屋 に ひとり 住む こと に なり まし た 。 わたくし は そこ の 馬 を 置く 場所 に 板 で 小さな し きい を つけ て 一疋 の 山羊 を 飼い まし た 。 毎朝 その 乳 を しぼっ て つめたい パン を ひたし て た べ 、 それ から 黒い 革 の かばん へ すこし の 書類 や 雑誌 を 入れ 、 靴 も きれい に みがき 、 並木 の ポプラ の 影法師 を 大股 にわたって 市 の 役所 へ 出 て 行く の でし た 。 #> 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          あの イーハトーヴォ の すきとおっ た 風 、 夏 で も 底 に 冷た さ を もつ 青い そら 、 うつくしい 森 で 飾ら れ た モリーオ 市 、 郊外 の ぎらぎら ひかる 草 の 波 。 #> 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           また その なか で いっしょ に なっ た たくさん の ひと たち 、 ファゼーロ と ロザーロ 、 羊 飼 の ミーロ や 、 顔 の 赤い こども たち 、 地主 の テーモ 、 山猫 博士 の ボーガント・デストゥパーゴ など 、 いま この 暗い 巨 き な 石 の 建物 の なか で 考え て いる と 、 みんな むかし 風 の なつかしい 青い 幻 燈 の よう に 思わ れ ます 。 で は 、 わたくし は いつか の 小さな み だし を つけ ながら 、 しずか に あの 年 の イーハトーヴォ の 五月 から 十月 まで を 書きつけ ましょ う 。 gibasa::pack(res, POS1) #>   doc_id #> 1      1 #> 2      2 #> 3      3 #> 4      4 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #> 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 連体詞 名詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 名詞 助詞 動詞 助詞 動詞 助動詞 助動詞 記号 #> 2 名詞 名詞 名詞 名詞 助動詞 助動詞 助詞 名詞 助詞 名詞 助詞 記号 助動詞 動詞 助詞 名詞 助詞 名詞 助動詞 助動詞 助詞 名詞 助詞 連体詞 副詞 助動詞 助動詞 助詞 記号 動詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 名詞 名詞 助動詞 名詞 助動詞 助動詞 助詞 記号 名詞 助詞 名詞 副詞 名詞 助詞 動詞 助動詞 助動詞 記号 副詞 連体詞 名詞 記号 名詞 名詞 助詞 助詞 名詞 名詞 助詞 名詞 名詞 助詞 動詞 動詞 助詞 動詞 助詞 記号 連体詞 名詞 助詞 形容詞 名詞 助詞 名詞 助詞 動詞 助動詞 形容詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 名詞 助詞 名詞 助詞 動詞 助動詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 助動詞 名詞 助動詞 助詞 記号 名詞 助詞 副詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助動詞 連体詞 名詞 助詞 名詞 名詞 名詞 助詞 助詞 名詞 助詞 動詞 助詞 記号 連体詞 名詞 助詞 副詞 動詞 名詞 助詞 動詞 助動詞 助動詞 記号 名詞 助詞 名詞 助詞 名詞 助詞 動詞 名詞 助詞 名詞 助詞 連体詞 助動詞 名詞 助詞 動詞 助詞 名詞 助詞 名詞 助詞 動詞 助動詞 助動詞 記号 名詞 連体詞 名詞 助詞 動詞 助詞 形容詞 名詞 助詞 動詞 動詞 助動詞 助詞 記号 名詞 助詞 形容詞 名詞 助詞 名詞 助詞 副詞 助詞 名詞 助詞 名詞 助詞 動詞 記号 名詞 助詞 名詞 助詞 動詞 記号 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 名詞 助動詞 助動詞 記号 #> 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          連体詞 名詞 助詞 動詞 助動詞 名詞 記号 名詞 助詞 助詞 名詞 助詞 形容詞 名詞 助詞 動詞 形容詞 感動詞 記号 形容詞 名詞 助詞 動詞 動詞 助動詞 名詞 名詞 記号 名詞 助詞 副詞 動詞 名詞 助詞 名詞 記号 #> 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   接続詞 連体詞 名詞 助詞 名詞 助詞 動詞 助動詞 名詞 助詞 名詞 名詞 記号 名詞 助詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 記号 名詞 助詞 形容詞 名詞 名詞 記号 名詞 助詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 記号 名詞 連体詞 形容詞 名詞 助動詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 助詞 記号 名詞 名詞 名詞 助詞 形容詞 形容詞 名詞 名詞 助詞 名詞 助詞 動詞 動詞 助動詞 記号 助動詞 助詞 記号 名詞 助詞 名詞 助詞 連体詞 接頭詞 名詞 助詞 動詞 助詞 記号 名詞 助詞 連体詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 助詞 動詞 助動詞 助動詞 記号"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"ベンチマーク","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"ベンチマーク","title":"Introduction to gibasa","text":"TokyoR #98でのLTのスライドでも紹介しましたが、ある程度の分量がある文字列ベクトルに対して、ごくふつうに形態素解析だけをやるかぎりにおいては、RMeCabよりもgibasaのほうが解析速度が速いと思います（というより、RMeCabでもmecabを呼んでいる部分はおそらく十分速いのですが、欲しいかたちに加工するためのRの処理に時間がかかることが多いです）。 ただ、gibasaには形態素解析する以外の機能はないので、目的にあわせて好みで使ってください。 benchmark1","code":"dat <- data.frame(   doc_id = seq_along(audubon::polano[5:800]),   text = audubon::polano[5:800] )  gibasa <- function() {   gibasa::tokenize(dat) |>     gibasa::prettify(col_select = \"POS1\") }  rmecab <- function() {   purrr::imap_dfr(     RMeCab::RMeCabDF(dat, 2),     ~ data.frame(doc_id = .y, token = unname(.x), POS1 = names(.x))   ) }  bench <- microbenchmark::microbenchmark(gibasa(),                                         rmecab(),                                         times = 20L) ggplot2::autoplot(bench)"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"デモ使用するデータ","dir":"Articles","previous_headings":"","what":"デモ：使用するデータ","title":"Introduction to gibasa","text":"以下では、gibasaを利用した分析のデモとして、gibasaによるテキストの前処理からLDAによるトピックモデリングまでを試してみます。ここでは、以下の10人の作家による文章を使います。 芥川龍之介（芥川竜之介） 太宰治 泉鏡花 菊池寛 森鴎外 夏目漱石 岡本綺堂 佐々木味津三 島崎藤村 海野十三 青空文庫で公開されているこれらの作家の文章のうち「新字新仮名」のものはあわせて1000篇ほどあるようですが、このうち699篇をランダムに抽出したデータセットをあらかじめ用意しました。次のようにしてダウンロードして読み込みます。 このデータセットのtext列には、それぞれの文章の本文のうち、会話文以外の段落だけをひとまとめにした文字列が含まれています。","code":"tmp <- tempfile(fileext = \".zip\") download.file(\"https://github.com/paithiov909/shiryo/raw/main/data/aozora.csv.zip\", tmp)  tbl <- readr::read_csv(tmp, col_types = \"cccf\") |>   dplyr::mutate(author = as.factor(author)) dplyr::glimpse(tbl) #> Rows: 699 #> Columns: 4 #> $ doc_id <chr> \"25\", \"29\", \"32\", \"33\", \"35\", \"38\", \"39\", \"50\", \"56\", \"59\", \"63… #> $ title  <chr> \"文学好きの家庭から\", \"カルメン\", \"大導寺信輔の半生\", \"第四の夫… #> $ text   <chr> \"　私の家は代々お奥坊主だったのですが、父も母もはなはだ特徴のな… #> $ author <fct> 芥川竜之介, 芥川竜之介, 芥川竜之介, 芥川竜之介, 芥川竜之介, 芥…"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"デモ前処理分かち書きのコツ","dir":"Articles","previous_headings":"","what":"デモ：前処理（分かち書き）のコツ","title":"Introduction to gibasa","text":"これを前処理（分かち書き）していくのですが、注意点として、このくらいの分量のテキストデータを一気にgibasa::tokenizeにわたしても、おそらくメモリ不足になってしまい、結果が返ってきません。 たとえば、このデータセットのtext列について、文字数を確認すると、おおよそ次のような分量があります。 このデータセット中で、文字数ベースでもっとも分量が多い文章は、森鴎外『ファウスト』です。とりあえず『ファウスト』のみについてgibasa::tokenizeしてみると、これだけですでに152,000トークンあります。これくらいの分量の文章はデータセット中にほかにもあるようなので、このデータセット全部を分かち書きするとしたら、総トークン数の目安として45万トークンくらいは軽く超えてきそうです。 gibasa::tokenizeはtext_field引数で指定した列に含まれる総トークン数行×3列のデータフレームを戻り値として返します。その戻り値に対して何も考えずにgibasa::prettifyすると、feature列がさらに複数列に分割されるため、総トークン数が45万行あればそれだけでも45万行×十数列のデータフレームになります。それだけの量のtidy textを一気に扱うのは、あまり現実的ではありません。 また、gibasaは、要素あたり数万字あるような文字列でもいちおう分かち書きできるはずですが、どちらかというと要素あたりの文字数はほどほどである文字列ベクトルを念頭にvectorizeされているため、短めの文書の集合に対してのほうが安定して動作します。そのため、要素ごとに長い文章に対してはsplit=TRUEにしたほうが速いケースが多くなるはずです（デフォルトではFALSEになっています）。 bencmark2 そこで、ここではひとまず単純に形態素解析するテキストの分量を減らします。以下では、文章あたりの文字数が5000字よりも少ないデータだけを使うことにします。これでも、全部で228篇あります。 さらに、実際にこれくらいの規模のテキストデータを形態素解析するにあたっては、最終的に欲しいかたちのデータになるように元のデータセットを小分けにしながら処理するのが無難です。 たとえば、ここでは、トークンのなかでも未知語でないもので、かつ、名詞・形容詞・動詞についてのみを、辞書に収録されている原形で抽出し、品詞情報とあわせて「/」で区切ったタグ付き単語の分かち書きにしています。このとき、以下のように、chunkという新しくつくった列についてグルーピングし、適当なサイズのまとまりごとにtext列を処理しています。 また、このときcol_select引数をわたさずにgibasa::prettifyすると、いちいち9列ある素性（IPA辞書の場合）をすべてパースしてしまいます。必要な列だけをパースしたほうがメモリ使用量的により安心なため、なるべく後から処理に使う列だけをこまめに指定するようにしてください。","code":"tbl |>   dplyr::mutate(nchar = nchar(text)) |>   dplyr::group_by(author) |>   dplyr::summarise(     nchar_mean = mean(nchar),     nchar_median = median(nchar),     nchar_min = min(nchar),     nchar_max = max(nchar),     n = dplyr::n()   ) |>   dplyr::mutate(across(where(is.numeric), trunc)) #> # A tibble: 10 × 6 #>    author       nchar_mean nchar_median nchar_min nchar_max     n #>    <fct>             <dbl>        <dbl>     <dbl>     <dbl> <dbl> #>  1 芥川竜之介         5802         4073       392     38265    81 #>  2 太宰治             9320         3865        95    100124   117 #>  3 岡本綺堂          12391        10208       558    108472   133 #>  4 菊池寛            20994        11819      1244    220311    47 #>  5 泉鏡花            19403        13428      2194    126786    66 #>  6 佐々木味津三       9938         9724      6171     15695    32 #>  7 森鴎外            20505         7212       780    247694    43 #>  8 夏目漱石          23484         5718       432    243570    56 #>  9 島崎藤村          46506        13518      1047    199333    17 #> 10 海野十三          20015        11824       824    115979   107 dplyr::filter(tbl, title == \"ファウスト\") |>   dplyr::mutate(nchar = nchar(text)) |>   dplyr::select(-text) #> # A tibble: 1 × 4 #>   doc_id title      author  nchar #>   <chr>  <chr>      <fct>   <int> #> 1 50909  ファウスト 森鴎外 247694  gbs_split_t <- function() {   dplyr::filter(tbl, title == \"ファウスト\") |>     gibasa::tokenize(split = TRUE) |>     nrow() } gbs_split_f <- function() {   dplyr::filter(tbl, title == \"ファウスト\") |>     gibasa::tokenize(split = FALSE) |>     nrow() }  gbs_split_t() #> [1] 152619 bench <- microbenchmark::microbenchmark(gbs_split_t(),                                         gbs_split_f(),                                         times = 3L) ggplot2::autoplot(bench) tbl <- dplyr::filter(tbl, nchar(text) < 5000) tbl |>   dplyr::mutate(nchar = nchar(text)) |>   dplyr::group_by(author) |>   dplyr::summarise(     nchar_mean = mean(nchar),     nchar_median = median(nchar),     nchar_min = min(nchar),     nchar_max = max(nchar),     n = dplyr::n()   ) |>   dplyr::mutate(across(where(is.numeric), trunc)) #> # A tibble: 9 × 6 #>   author     nchar_mean nchar_median nchar_min nchar_max     n #>   <fct>           <dbl>        <dbl>     <dbl>     <dbl> <dbl> #> 1 芥川竜之介       2337         2167       392      4811    49 #> 2 太宰治           1691         1294        95      4584    63 #> 3 岡本綺堂         2850         2740       558      4979    32 #> 4 菊池寛           2857         2197      1244      4601     7 #> 5 泉鏡花           3731         4002      2194      4800    12 #> 6 森鴎外           2427         2166       780      4574    12 #> 7 夏目漱石         2337         1829       432      4720    24 #> 8 島崎藤村         1978         2172      1047      2993     5 #> 9 海野十三         2949         2475       824      4728    24 corp <- tbl |>   dplyr::mutate(     text = audubon::strj_normalize(text),     chunk = dplyr::ntile(doc_id, 10)   ) |>   dplyr::group_by(chunk) |>   dplyr::group_map(function(df, idx) {     data.frame(       doc_id = df$doc_id,       text = df$text     ) |>       gibasa::tokenize(split = TRUE) |>       gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |>       dplyr::filter(         POS1 %in% c(\"名詞\", \"形容詞\", \"動詞\"),         !is.na(Original)       ) |>       dplyr::mutate(token = stringr::str_c(Original, POS1, sep = \"/\")) |>       gibasa::pack(token)   }) |>   purrr::map_dfr(~.) |>   dplyr::left_join(dplyr::select(tbl, doc_id, author), by = \"doc_id\") |>   quanteda::corpus()  head(corp) #> Corpus consisting of 6 documents and 1 docvar. #> 100 : #> \"むかし/名詞 むかし/名詞 むかし/名詞 深い/形容詞 山/名詞 奥/名詞 大きい/形容詞 桃/名詞 木/名詞 一/名詞...\" #>  #> 102 : #> \"書/名詞 紀/名詞 よる/動詞 日本/名詞 推古天皇/名詞 三/名詞 十/名詞 五/名詞 年/名詞 春/名詞 二月/名詞...\" #>  #> 103 : #> \"冬/名詞 夜/名詞 私/名詞 旧友/名詞 村上/名詞 一/名詞 しょ/名詞 銀座/名詞 通り/名詞 歩く/動詞 いる/動...\" #>  #> 107 : #> \"僕/名詞 何/名詞 雑木/名詞 生える/動詞 寂しい/形容詞 崖/名詞 上/名詞 歩く/動詞 行く/動詞 崖/名詞 下/...\" #>  #> 109 : #> \"舎/名詞 衛/名詞 城/名詞 人口/名詞 多い/形容詞 都/名詞 城/名詞 面積/名詞 人口/名詞 多い/形容詞 割/名...\" #>  #> 110 : #> \"馬返し/名詞 すぎる/動詞 行く/動詞 大谷/名詞 川/名詞 見える/動詞 所/名詞 出る/動詞 落葉/名詞 埋もれる/...\""},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"デモlda","dir":"Articles","previous_headings":"","what":"デモ：LDA","title":"Introduction to gibasa","text":"ここまでで抽出した文書から関心のある品詞だけを分かち書きにしつつ、quantedaのコーパスオブジェクトのかたちにしました。ここからはquantedaの関数を使いながら、簡単にtopicmodels::LDAによるLDAの実行までを試してみます。 quantedaでこのようなかたちのコーパスから文書単語行列を得るには、次のようにします。この時点で、トークン数（異なり語の数）は17,000語ほどあります。 これくらいであればこのままでもよさそうですが、もう少し語彙を減らしたい場合、たとえば、次のようにquanteda::dfm_trimします。これで、ここでは4,000語くらいまで語彙を減らすことができました。 quanteda::convertでdfmオブジェクトを変換してtopicmodels::LDAにわたします。 割り当てられたトピックを確認してみます。それぞれの文章の内容についてあまり理解していないため、ここではそれぞれのトピックの解釈はしませんが、比較的作家ごとにまとまって一つのトピックに属しているようにも見えます。","code":"dtm <- corp |>   quanteda::tokens(what = \"fastestword\") |>   quanteda::dfm() |>   quanteda::dfm_weight()  quanteda::nfeat(dtm) #> [1] 17738 dtm <- dtm |>   quanteda::dfm_trim(min_termfreq = 5L, max_termfreq = 50L)  quanteda::nfeat(dtm) #> [1] 4098 lda_res <-   quanteda::convert(dtm, to = \"topicmodels\") |>   topicmodels::LDA(k = 5) topics <-   topicmodels::topics(lda_res) |>   tibble::enframe(name = \"doc_id\", value = \"topic\") |>   dplyr::right_join(     dplyr::select(tbl, !text),     by = \"doc_id\"   ) |>   dplyr::arrange(topic)  reactable::reactable(   topics,   filterable = TRUE,   searchable = TRUE,   compact = TRUE )"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"セッション情報","dir":"Articles","previous_headings":"","what":"セッション情報","title":"Introduction to gibasa","text":"","code":"sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.2.1 (2022-06-23) #>  os       Ubuntu 20.04.5 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  C.UTF-8 #>  ctype    C.UTF-8 #>  tz       UTC #>  date     2022-10-29 #>  pandoc   2.19.2 @ /usr/bin/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  package      * version    date (UTC) lib source #>  audubon        0.3.0      2022-07-22 [2] RSPM #>  bit            4.0.4      2020-08-04 [2] RSPM #>  bit64          4.0.5      2020-08-30 [2] RSPM #>  bslib          0.4.0      2022-07-16 [2] RSPM #>  cachem         1.0.6      2021-08-19 [2] RSPM #>  cli            3.4.1      2022-09-23 [2] RSPM #>  crayon         1.5.2      2022-09-29 [2] RSPM #>  curl           4.3.3      2022-10-06 [2] RSPM #>  desc           1.4.2      2022-09-08 [2] RSPM #>  digest         0.6.30     2022-10-18 [2] RSPM #>  dplyr          1.0.10     2022-09-01 [2] RSPM #>  ellipsis       0.3.2      2021-04-29 [2] RSPM #>  evaluate       0.17       2022-10-07 [2] RSPM #>  fansi          1.0.3      2022-03-24 [2] RSPM #>  fastmap        1.1.0      2021-01-25 [2] RSPM #>  fastmatch      1.1-3      2021-07-23 [2] RSPM #>  fs             1.5.2      2021-12-08 [2] RSPM #>  generics       0.1.3      2022-07-05 [2] RSPM #>  gibasa         0.5.0.9004 2022-10-29 [1] local #>  glue           1.6.2      2022-02-24 [2] RSPM #>  hms            1.1.2      2022-08-19 [2] RSPM #>  htmltools      0.5.3      2022-07-18 [2] RSPM #>  htmlwidgets    1.5.4      2021-09-08 [2] RSPM #>  jquerylib      0.1.4      2021-04-26 [2] RSPM #>  jsonlite       1.8.3      2022-10-21 [2] RSPM #>  knitr          1.40       2022-08-24 [2] RSPM #>  lattice        0.20-45    2021-09-22 [4] CRAN (R 4.2.1) #>  lifecycle      1.0.3      2022-10-07 [2] RSPM #>  magrittr       2.0.3      2022-03-30 [2] RSPM #>  Matrix         1.5-1      2022-09-13 [2] RSPM #>  memoise        2.0.1      2021-11-26 [2] RSPM #>  modeltools     0.2-23     2020-03-05 [2] RSPM #>  NLP            0.2-1      2020-10-14 [2] RSPM #>  pillar         1.8.1      2022-08-19 [2] RSPM #>  pkgconfig      2.0.3      2019-09-22 [2] RSPM #>  pkgdown        2.0.6      2022-07-16 [2] any (@2.0.6) #>  purrr          0.3.5      2022-10-06 [2] RSPM #>  quanteda       3.2.3      2022-08-29 [2] RSPM #>  R.cache        0.16.0     2022-07-21 [2] RSPM #>  R.methodsS3    1.8.2      2022-06-13 [2] RSPM #>  R.oo           1.25.0     2022-06-12 [2] RSPM #>  R.utils        2.12.0     2022-06-28 [2] RSPM #>  R6             2.5.1      2021-08-19 [2] RSPM #>  ragg           1.2.4      2022-10-24 [2] RSPM #>  Rcpp           1.0.9      2022-07-08 [2] RSPM #>  RcppParallel   5.1.5      2022-01-05 [2] RSPM #>  reactable      0.3.0      2022-05-26 [2] RSPM #>  reactR         0.4.4      2021-02-22 [2] RSPM #>  readr          2.1.3      2022-10-01 [2] RSPM #>  rlang          1.0.6      2022-09-24 [2] RSPM #>  rmarkdown      2.17       2022-10-07 [2] RSPM #>  rprojroot      2.0.3      2022-04-02 [2] RSPM #>  sass           0.4.2      2022-07-16 [2] RSPM #>  sessioninfo    1.2.2      2021-12-06 [2] any (@1.2.2) #>  slam           0.1-50     2022-01-08 [2] RSPM #>  stopwords      2.3        2021-10-28 [2] RSPM #>  stringi        1.7.8      2022-07-11 [2] RSPM #>  stringr        1.4.1      2022-08-20 [2] RSPM #>  styler         1.8.0      2022-10-22 [2] any (@1.8.0) #>  systemfonts    1.0.4      2022-02-11 [2] RSPM #>  textshaping    0.3.6      2021-10-13 [2] RSPM #>  tibble         3.1.8      2022-07-22 [2] RSPM #>  tidyselect     1.2.0      2022-10-10 [2] RSPM #>  tm             0.7-9      2022-10-19 [2] RSPM #>  topicmodels    0.2-12     2021-01-29 [2] RSPM #>  tzdb           0.3.0      2022-03-28 [2] RSPM #>  utf8           1.2.2      2021-07-24 [2] RSPM #>  V8             4.2.1      2022-08-07 [2] RSPM #>  vctrs          0.5.0      2022-10-22 [2] RSPM #>  vroom          1.6.0      2022-09-30 [2] RSPM #>  withr          2.5.0      2022-03-03 [2] RSPM #>  xfun           0.34       2022-10-18 [2] RSPM #>  xml2           1.3.3      2021-11-30 [2] RSPM #>  yaml           2.3.6      2022-10-18 [2] RSPM #>  #>  [1] /tmp/RtmpdyYsXI/temp_libpath60bc3270ceec #>  [2] /home/runner/work/_temp/Library #>  [3] /opt/R/4.2.1/lib/R/site-library #>  [4] /opt/R/4.2.1/lib/R/library #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"はじめに","dir":"Articles","previous_headings":"","what":"はじめに","title":"Text Mining with quanteda and gibasa","text":"quantedaとgibasaを用いたテキストマイニングの例です。 なお、以下のパッケージについては、ここではGitHubからインストールできるものを使っています。 paithiov909/audubon paithiov909/ldccr これらは、次のような感じでインストールできます。 この記事は、こういう使い方ができるというメモのようなもので、やっていることの意味についての説明はしていません。また、quantedaはtokenizersをラップした関数によって日本語の文書でも分かち書きできるので、手元の辞書に収録されている表現どおりに分かち書きしたい場合や、品詞情報が欲しい場合でないかぎりは、形態素解析器を使うメリットはあまりないかもしれません。tokenizers（内部的にはstringi）が利用しているICUのBoundary Analysisの仕様については、UAX#29などを参照してください。","code":"# Enable repository from paithiov909 options(repos = c(   ropensci = \"https://paithiov909.r-universe.dev\",   CRAN = \"https://cloud.r-project.org\"))  # Download and install ldccr in R install.packages(\"ldccr\")"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"データの準備","dir":"Articles","previous_headings":"","what":"データの準備","title":"Text Mining with quanteda and gibasa","text":"テキストデータとしてlivedoorニュースコーパスを使います。以下の9カテゴリです。 トピックニュース Sports Watch ITライフハック 家電チャンネル MOVIE ENTER 独女通信 エスマックス livedoor HOMME Peachy ldccrでデータフレームにします。 このうち一部だけをquantedaのコーパスオブジェクトとして格納し、いろいろ試していきます。このとき、あらかじめ「■」という文字だけ取り除いておきます。","code":"if (requireNamespace(\"ldccr\", quietly = TRUE)) {   data <- ldccr::read_ldnws() } #> Parsing dokujo-tsushin... #> Parsing it-life-hack... #> Parsing kaden-channel... #> Parsing livedoor-homme... #> Parsing movie-enter... #> Parsing peachy... #> Parsing smax... #> Parsing sports-watch... #> Parsing topic-news... #> Done. corp <- data |>   dplyr::select(category, body) |>   dplyr::mutate(doc_id = as.factor((dplyr::row_number()))) |>   dplyr::slice_sample(prop = .2)  corp <- corp |>   dplyr::mutate(     body = stringr::str_remove_all(body, \"[\\u25a0]+\"),     body = audubon::strj_normalize(body)   ) |>   gibasa::tokenize(body) |>   gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |>   dplyr::filter(!POS1 %in% c(\"助詞\", \"助動詞\")) |>   tidyr::drop_na() |>   gibasa::pack(Original) |>   dplyr::left_join(corp, by = \"doc_id\") |>   quanteda::corpus()"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"ワードクラウド","dir":"Articles","previous_headings":"","what":"ワードクラウド","title":"Text Mining with quanteda and gibasa","text":"","code":"corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_group(groups = category) |>   quanteda::dfm_trim(min_termfreq = 10L) |>   quanteda.textplots::textplot_wordcloud(color = viridisLite::cividis(8L))"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"出現頻度の集計","dir":"Articles","previous_headings":"","what":"出現頻度の集計","title":"Text Mining with quanteda and gibasa","text":"","code":"corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_weight(\"prop\") |>   quanteda.textstats::textstat_frequency(groups = category) |>   dplyr::top_n(-30L, rank) |>   ggpubr::ggdotchart(     x = \"feature\",     y = \"frequency\",     group = \"group\",     color = \"group\",     rotate = TRUE   ) +   ggplot2::theme_bw()"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"keyness","dir":"Articles","previous_headings":"","what":"Keyness","title":"Text Mining with quanteda and gibasa","text":"ITライフハック（-life-hack）グループの文書とその他の対照を見ています。","code":"corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_group(groups = category) |>   quanteda.textstats::textstat_keyness(target = \"it-life-hack\") |>   quanteda.textplots::textplot_keyness()"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"対応分析","dir":"Articles","previous_headings":"","what":"対応分析","title":"Text Mining with quanteda and gibasa","text":"全部をプロットすると潰れて見えないので一部だけを抽出しています。 ちなみに、require(ca)としてからquatenda.textmodels::textmodel_caの結果をplotに渡すと、バイプロットを出力することもできます。","code":"corp_sample <- quanteda::corpus_sample(corp, size = 32L) corp_sample |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_weight(scheme = \"prop\") |>   quanteda.textmodels::textmodel_ca() |>   quanteda.textplots::textplot_scale1d(     margin = \"documents\",     groups = quanteda::docvars(corp_sample, \"category\")   )"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"共起ネットワーク","dir":"Articles","previous_headings":"","what":"共起ネットワーク","title":"Text Mining with quanteda and gibasa","text":"共起ネットワークもあまり大きな文書集合だと潰れて見えないので、対応分析と同じコーパスについて描画してみます。","code":"corp_sample |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_group(groups = category) |>   quanteda::dfm_trim(min_termfreq = 15L) |>   quanteda::fcm() |>   quanteda.textplots::textplot_network() #> Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider #> increasing max.overlaps"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"クラスタリング","dir":"Articles","previous_headings":"","what":"クラスタリング","title":"Text Mining with quanteda and gibasa","text":"マンハッタン距離、ward法（ward.D2）です。ここでも一部だけを抽出しています。","code":"d <- corp_sample |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_weight(scheme = \"prop\") |>   quanteda.textstats::textstat_dist(method = \"manhattan\") |>   as.dist() |>   hclust(method = \"ward.D2\") |>   ggdendro::dendro_data(type = \"rectangle\") |>   (function(x) {     purrr::list_modify(       x,       labels = dplyr::bind_cols(         x$labels,         names = names(corp_sample),         category = quanteda::docvars(corp_sample, \"category\")       )     )   })()  ggplot2::ggplot(ggdendro::segment(d)) +   ggplot2::geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +   ggplot2::geom_text(ggdendro::label(d), mapping = aes(x, y, label = names, colour = category, hjust = 0), size = 3) +   ggplot2::coord_flip() +   ggplot2::scale_y_reverse(expand = c(.2, 0)) +   ggdendro::theme_dendro()"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"ldalatent-dirichlet-allocation","dir":"Articles","previous_headings":"","what":"LDA（Latent Dirichlet Allocation）","title":"Text Mining with quanteda and gibasa","text":"LDAについてはquanteda::convertでdfmを変換してtopicmodels::LDAに直接渡すこともできます。公式のクイックスタートガイドも参考にしてください。weighted LDAなどの実装を含むkeyATMといった選択肢もあります。 なお、トピック数は9に決め打ちしています。トピック数含めパラメタの探索をしたい場合には、ldatuningやstmなどを利用したほうがよいです。 LDAvisで可視化してみます（デフォルトエンコーディングがCP932であるWindows環境の場合、LDAvis::createJSONで書き出されるラベル（vocab）のエンコーディングがそっちに引きずられてCP932になってしまうため、ブラウザで表示したときにラベルが文字化けします。書き出されたlda.jsonをUTF-8に変換すれば文字化けは解消されるので、その場合は、あとから変換して上書きするとよいです）。 LDAvis","code":"dtm <- corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::dfm_tfidf()  features <- corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   quanteda::dfm() |>   quanteda::ntoken()  m <- dtm |>   as(\"dgCMatrix\") |>   textmineR::FitLdaModel(k = 9, iterations = 200, burnin = 175)  m$phi |>   textmineR::GetTopTerms(15L) |>   reactable::reactable(compact = TRUE) suppressWarnings({   LDAvis::createJSON(     phi = m$phi,     theta = m$theta,     doc.length = features,     vocab = stringi::stri_enc_toutf8(dtm@Dimnames$features),     term.frequency = quanteda::colSums(dtm)   ) |>     LDAvis::serVis(open.browser = FALSE, out.dir = file.path(getwd(), \"cache/ldavis\")) })  if (getRversion() < \"4.2.0\") {   readr::read_lines_raw(file.path(getwd(), \"cache/ldavis\", \"lda.json\")) %>%     iconv(from = \"CP932\", to = \"UTF-8\") %>%     jsonlite::parse_json(simplifyVector = TRUE) %>%     jsonlite::write_json(file.path(getwd(), \"cache/ldavis\", \"lda.json\"), dataframe = \"columns\", auto_unbox = TRUE) }"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"glove","dir":"Articles","previous_headings":"","what":"GloVe","title":"Text Mining with quanteda and gibasa","text":"ここでは50次元の埋め込みを得ます。 uwotで次元を減らして可視化します。色はstats::kmeansでクラスタリング（コサイン距離）して付けています。","code":"toks <- corp |>   quanteda::tokens(what = \"fastestword\", remove_punct = TRUE) |>   as.list() |>   text2vec::itoken()  vocab <- toks |>   text2vec::create_vocabulary() |>   text2vec::prune_vocabulary(term_count_min = 10L)  vectorize <- text2vec::vocab_vectorizer(vocab)  tcm <- text2vec::create_tcm(   it = toks,   vectorizer = vectorize,   skip_grams_window = 5L )  glove <- text2vec::GlobalVectors$new(   rank = 50,   x_max = 15L ) wv_main <- glove$fit_transform(   x = tcm,   n_iter = 10L ) #> INFO  [03:40:03.965] epoch 1, loss 0.1360 #> INFO  [03:40:04.428] epoch 2, loss 0.0844 #> INFO  [03:40:04.874] epoch 3, loss 0.0719 #> INFO  [03:40:05.316] epoch 4, loss 0.0646 #> INFO  [03:40:05.764] epoch 5, loss 0.0596 #> INFO  [03:40:06.207] epoch 6, loss 0.0560 #> INFO  [03:40:06.650] epoch 7, loss 0.0532 #> INFO  [03:40:07.092] epoch 8, loss 0.0510 #> INFO  [03:40:07.537] epoch 9, loss 0.0491 #> INFO  [03:40:07.979] epoch 10, loss 0.0476  wv <- (wv_main + t(glove$components)) |>   as.data.frame(stringsAsFactors = FALSE) |>   tibble::as_tibble(.name_repair = \"minimal\", rownames = NA) vec <- vocab |>   dplyr::arrange(desc(term_count)) |>   dplyr::slice_head(n = 100L) |>   dplyr::left_join(tibble::rownames_to_column(wv), by = c(\"term\" = \"rowname\")) |>   dplyr::select(!c(\"term_count\", \"doc_count\"))  labs <- vec$term vec <- dplyr::select(vec, !term)  dist <- 1 - proxyC::simil(as(as.matrix(vec), \"dgCMatrix\"), method = \"cosine\") clust <- kmeans(x = dist, centers = 9)  vec <- uwot::umap(vec) |>   as.data.frame() |>   dplyr::mutate(rowname = labs, cluster = as.factor(clust$cluster))  vec |>   ggplot2::ggplot(aes(x = V1, y = V2, colour = cluster)) +   ggplot2::geom_point() +   ggrepel::geom_text_repel(aes(label = rowname)) +   ggplot2::theme_light()"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"セッション情報","dir":"Articles","previous_headings":"","what":"セッション情報","title":"Text Mining with quanteda and gibasa","text":"","code":"sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.2.1 (2022-06-23) #>  os       Ubuntu 20.04.5 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  C.UTF-8 #>  ctype    C.UTF-8 #>  tz       UTC #>  date     2022-10-29 #>  pandoc   2.19.2 @ /usr/bin/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  package             * version         date (UTC) lib source #>  abind                 1.4-5           2016-07-21 [2] RSPM #>  audubon               0.3.0           2022-07-22 [2] RSPM #>  backports             1.4.1           2021-12-13 [2] RSPM #>  bit                   4.0.4           2020-08-04 [2] RSPM #>  bit64                 4.0.5           2020-08-30 [2] RSPM #>  broom                 1.0.1           2022-08-29 [2] RSPM #>  bslib                 0.4.0           2022-07-16 [2] RSPM #>  cachem                1.0.6           2021-08-19 [2] RSPM #>  car                   3.1-1           2022-10-19 [2] RSPM #>  carData               3.0-5           2022-01-06 [2] RSPM #>  cli                   3.4.1           2022-09-23 [2] RSPM #>  coda                  0.19-4          2020-09-30 [2] RSPM #>  codetools             0.2-18          2020-11-04 [4] CRAN (R 4.2.1) #>  colorspace            2.0-3           2022-02-21 [2] RSPM #>  crayon                1.5.2           2022-09-29 [2] RSPM #>  curl                  4.3.3           2022-10-06 [2] RSPM #>  data.table            1.14.4          2022-10-17 [2] RSPM #>  desc                  1.4.2           2022-09-08 [2] RSPM #>  digest                0.6.30          2022-10-18 [2] RSPM #>  dplyr                 1.0.10          2022-09-01 [2] RSPM #>  ellipsis              0.3.2           2021-04-29 [2] RSPM #>  evaluate              0.17            2022-10-07 [2] RSPM #>  fansi                 1.0.3           2022-03-24 [2] RSPM #>  farver                2.1.1           2022-07-06 [2] RSPM #>  fastmap               1.1.0           2021-01-25 [2] RSPM #>  fastmatch             1.1-3           2021-07-23 [2] RSPM #>  float                 0.3-0           2022-04-07 [2] RSPM #>  FNN                   1.1.3.1         2022-05-23 [2] RSPM #>  foreach               1.5.2           2022-02-02 [2] RSPM #>  fs                    1.5.2           2021-12-08 [2] RSPM #>  generics              0.1.3           2022-07-05 [2] RSPM #>  ggdendro              0.1.23          2022-02-16 [2] RSPM #>  ggplot2             * 3.3.6           2022-05-03 [2] RSPM #>  ggpubr                0.4.0           2020-06-27 [2] RSPM #>  ggrepel               0.9.1           2021-01-15 [2] RSPM #>  ggsignif              0.6.4           2022-10-13 [2] RSPM #>  gibasa                0.5.0.9004      2022-10-29 [1] local #>  glmnet                4.1-4           2022-04-15 [2] RSPM #>  glue                  1.6.2           2022-02-24 [2] RSPM #>  gtable                0.3.1           2022-09-01 [2] RSPM #>  highr                 0.9             2021-04-16 [2] RSPM #>  hms                   1.1.2           2022-08-19 [2] RSPM #>  htmltools             0.5.3           2022-07-18 [2] RSPM #>  htmlwidgets           1.5.4           2021-09-08 [2] RSPM #>  iterators             1.0.14          2022-02-05 [2] RSPM #>  jquerylib             0.1.4           2021-04-26 [2] RSPM #>  jsonlite              1.8.3           2022-10-21 [2] RSPM #>  knitr                 1.40            2022-08-24 [2] RSPM #>  labeling              0.4.2           2020-10-20 [2] RSPM #>  lattice               0.20-45         2021-09-22 [4] CRAN (R 4.2.1) #>  ldccr                 0.0.11.20220927 2022-10-29 [2] Github (paithiov909/ldccr@f2b4044) #>  lgr                   0.4.4           2022-09-05 [2] RSPM #>  LiblineaR             2.10-12         2021-03-02 [2] RSPM #>  lifecycle             1.0.3           2022-10-07 [2] RSPM #>  magrittr              2.0.3           2022-03-30 [2] RSPM #>  MASS                  7.3-57          2022-04-22 [4] CRAN (R 4.2.1) #>  Matrix                1.5-1           2022-09-13 [2] RSPM #>  memoise               2.0.1           2021-11-26 [2] RSPM #>  mlapi                 0.1.1           2022-04-24 [2] RSPM #>  munsell               0.5.0           2018-06-12 [2] RSPM #>  network               1.18.0          2022-10-06 [2] RSPM #>  nsyllable             1.0.1           2022-02-28 [2] RSPM #>  pillar                1.8.1           2022-08-19 [2] RSPM #>  pkgconfig             2.0.3           2019-09-22 [2] RSPM #>  pkgdown               2.0.6           2022-07-16 [2] any (@2.0.6) #>  proxyC                0.3.3           2022-10-06 [2] RSPM #>  purrr                 0.3.5           2022-10-06 [2] RSPM #>  quanteda              3.2.3           2022-08-29 [2] RSPM #>  quanteda.textmodels   0.9.5-1         2022-10-04 [2] RSPM #>  quanteda.textplots    0.94.2          2022-09-01 [2] RSPM #>  quanteda.textstats    0.96            2022-09-19 [2] RSPM #>  R.cache               0.16.0          2022-07-21 [2] RSPM #>  R.methodsS3           1.8.2           2022-06-13 [2] RSPM #>  R.oo                  1.25.0          2022-06-12 [2] RSPM #>  R.utils               2.12.0          2022-06-28 [2] RSPM #>  R6                    2.5.1           2021-08-19 [2] RSPM #>  ragg                  1.2.4           2022-10-24 [2] RSPM #>  Rcpp                  1.0.9           2022-07-08 [2] RSPM #>  RcppParallel          5.1.5           2022-01-05 [2] RSPM #>  RcppProgress          0.4.2           2020-02-06 [2] RSPM #>  reactable             0.3.0           2022-05-26 [2] RSPM #>  reactR                0.4.4           2021-02-22 [2] RSPM #>  readr                 2.1.3           2022-10-01 [2] RSPM #>  RhpcBLASctl           0.21-247.1      2021-11-05 [2] RSPM #>  rlang                 1.0.6           2022-09-24 [2] RSPM #>  rmarkdown             2.17            2022-10-07 [2] RSPM #>  rprojroot             2.0.3           2022-04-02 [2] RSPM #>  rsparse               0.5.1           2022-09-11 [2] RSPM #>  RSpectra              0.16-1          2022-04-24 [2] RSPM #>  rstatix               0.7.0           2021-02-13 [2] RSPM #>  sass                  0.4.2           2022-07-16 [2] RSPM #>  scales                1.2.1           2022-08-20 [2] RSPM #>  sessioninfo           1.2.2           2021-12-06 [2] any (@1.2.2) #>  shape                 1.4.6           2021-05-19 [2] RSPM #>  sna                   2.7             2022-06-01 [2] RSPM #>  SparseM               1.81            2021-02-18 [2] RSPM #>  statnet.common        4.7.0           2022-09-08 [2] RSPM #>  stopwords             2.3             2021-10-28 [2] RSPM #>  stringi               1.7.8           2022-07-11 [2] RSPM #>  stringr               1.4.1           2022-08-20 [2] RSPM #>  styler                1.8.0           2022-10-22 [2] any (@1.8.0) #>  survival              3.3-1           2022-03-03 [4] CRAN (R 4.2.1) #>  systemfonts           1.0.4           2022-02-11 [2] RSPM #>  text2vec              0.6.2           2022-09-11 [2] RSPM #>  textmineR             3.0.5           2021-06-28 [2] RSPM #>  textshaping           0.3.6           2021-10-13 [2] RSPM #>  tibble                3.1.8           2022-07-22 [2] RSPM #>  tidyr                 1.2.1           2022-09-08 [2] RSPM #>  tidyselect            1.2.0           2022-10-10 [2] RSPM #>  tzdb                  0.3.0           2022-03-28 [2] RSPM #>  utf8                  1.2.2           2021-07-24 [2] RSPM #>  uwot                  0.1.14          2022-08-22 [2] RSPM #>  V8                    4.2.1           2022-08-07 [2] RSPM #>  vctrs                 0.5.0           2022-10-22 [2] RSPM #>  viridisLite           0.4.1           2022-08-22 [2] RSPM #>  vroom                 1.6.0           2022-09-30 [2] RSPM #>  withr                 2.5.0           2022-03-03 [2] RSPM #>  xfun                  0.34            2022-10-18 [2] RSPM #>  yaml                  2.3.6           2022-10-18 [2] RSPM #>  #>  [1] /tmp/RtmpdyYsXI/temp_libpath60bc3270ceec #>  [2] /home/runner/work/_temp/Library #>  [3] /opt/R/4.2.1/lib/R/site-library #>  [4] /opt/R/4.2.1/lib/R/library #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"データの準備","dir":"Articles","previous_headings":"","what":"データの準備","title":"Supervised Learning Using tidymodels and gibasa","text":"livedoorニュースコーパスを使います。このコーパスのカテゴリ分類はかなり易しいタスクであることが知られている（というか、一部のカテゴリではそのカテゴリを同定できる単語が本文に含まれてしまっている）ので、機械学習を手軽に試すのに便利です。テキストの特徴量をもとに以下の9カテゴリの分類をします。 トピックニュース Sports Watch ITライフハック 家電チャンネル MOVIE ENTER 独女通信 エスマックス livedoor HOMME Peachy ldccrでデータフレームにします。 ここでは、未知語でない語で、かつ、名詞・形容詞・動詞である語についてのみ抽出し、IPA辞書に収録されている原形の分かち書きにします。","code":"if (requireNamespace(\"ldccr\", quietly = TRUE)) {   tbl <- ldccr::read_ldnws() |>     dplyr::mutate(doc_id = as.character(dplyr::row_number())) } #> Parsing dokujo-tsushin... #> Parsing it-life-hack... #> Parsing kaden-channel... #> Parsing livedoor-homme... #> Parsing movie-enter... #> Parsing peachy... #> Parsing smax... #> Parsing sports-watch... #> Parsing topic-news... #> Done. corpus <- tbl |>   dplyr::mutate(     text = audubon::strj_normalize(body),     chunk = dplyr::ntile(doc_id, 10)   ) |>   dplyr::group_by(chunk) |>   dplyr::group_map(function(df, idx) {     data.frame(       doc_id = df$doc_id,       text = df$text     ) |>       gibasa::tokenize(split = TRUE) |>       gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |>       dplyr::filter(         POS1 %in% c(\"名詞\", \"形容詞\", \"動詞\"),         !is.na(Original)       ) |>       gibasa::pack(Original)   }) |>   purrr::map_dfr(~.) |>   dplyr::left_join(dplyr::select(tbl, doc_id, category), by = \"doc_id\")"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"モデルの学習","dir":"Articles","previous_headings":"","what":"モデルの学習","title":"Supervised Learning Using tidymodels and gibasa","text":"データを分割します。 以下のレシピとモデルで学習します。ここでは、ハッシュトリックを使っています。デフォルトだとパラメータはここに書かれている感じになります。 なお、tidymodelsの枠組みの外であらかじめ分かち書きを済ませましたが、textrecipes::step_tokenizeのcustom_token引数に独自にトークナイザを指定することで、一つのstepとして分かち書きすることもできます。 F値をメトリクスにして学習します。3分割CVで、簡単にですが、ハイパーパラメータ探索をします。 ハイパラ探索の要約を確認します。  fitします。 学習したモデルの精度を見てみます。","code":"corpus_split <- rsample::initial_split(corpus, prop = .8, strata = \"category\") corpus_train <- rsample::training(corpus_split) corpus_test <- rsample::testing(corpus_split) corpus_spec <-   parsnip::boost_tree(     sample_size = tune::tune(),     loss_reduction = tune::tune(),     tree_depth = tune::tune()   ) |>   parsnip::set_engine(\"xgboost\") |>   parsnip::set_mode(\"classification\")  space_tokenizer <- function(x) {   strsplit(x, \" +\") }  corpus_rec <-   recipes::recipe(     category ~ text,     data = corpus_train   ) |>   textrecipes::step_tokenize(text, custom_token = space_tokenizer) |>   textrecipes::step_tokenfilter(text, min_times = 30L, max_tokens = 200L) |>   textrecipes::step_texthash(text, num_terms = 200L) corpus_wflow <-   workflows::workflow() |>   workflows::add_model(corpus_spec) |>   workflows::add_recipe(corpus_rec) # doParallel::registerDoParallel(cores = parallel::detectCores() - 1)  corpus_tune_res <-   corpus_wflow |>   tune::tune_grid(     resamples = rsample::vfold_cv(corpus_train, strata = category, v = 3L),     grid = dials::grid_latin_hypercube(       dials::sample_prop(),       dials::loss_reduction(),       dials::tree_depth(),       size = 5L     ),     metrics = yardstick::metric_set(yardstick::f_meas),     control = tune::control_grid(save_pred = TRUE)   ) #> as(<dgTMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead ggplot2::autoplot(corpus_tune_res) corpus_wflow <-   tune::finalize_workflow(corpus_wflow, tune::select_best(corpus_tune_res, metric = \"f_meas\"))  corpus_fit <- tune::last_fit(corpus_wflow, corpus_split)  # doParallel::stopImplicitCluster() corpus_fit |>   tune::collect_predictions() |>   yardstick::f_meas(truth = category, estimate = .pred_class) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 f_meas  macro          0.863"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"セッション情報","dir":"Articles","previous_headings":"","what":"セッション情報","title":"Supervised Learning Using tidymodels and gibasa","text":"","code":"sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.2.1 (2022-06-23) #>  os       Ubuntu 20.04.5 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  C.UTF-8 #>  ctype    C.UTF-8 #>  tz       UTC #>  date     2022-10-29 #>  pandoc   2.19.2 @ /usr/bin/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  package      * version         date (UTC) lib source #>  audubon        0.3.0           2022-07-22 [2] RSPM #>  backports      1.4.1           2021-12-13 [2] RSPM #>  bit            4.0.4           2020-08-04 [2] RSPM #>  bit64          4.0.5           2020-08-30 [2] RSPM #>  broom        * 1.0.1           2022-08-29 [2] RSPM #>  bslib          0.4.0           2022-07-16 [2] RSPM #>  cachem         1.0.6           2021-08-19 [2] RSPM #>  class          7.3-20          2022-01-16 [4] CRAN (R 4.2.1) #>  cli            3.4.1           2022-09-23 [2] RSPM #>  codetools      0.2-18          2020-11-04 [4] CRAN (R 4.2.1) #>  colorspace     2.0-3           2022-02-21 [2] RSPM #>  conflicted     1.1.0           2021-11-26 [2] RSPM #>  crayon         1.5.2           2022-09-29 [2] RSPM #>  curl           4.3.3           2022-10-06 [2] RSPM #>  data.table     1.14.4          2022-10-17 [2] RSPM #>  desc           1.4.2           2022-09-08 [2] RSPM #>  dials        * 1.0.0           2022-06-14 [2] RSPM #>  DiceDesign     1.9             2021-02-13 [2] RSPM #>  digest         0.6.30          2022-10-18 [2] RSPM #>  dplyr        * 1.0.10          2022-09-01 [2] RSPM #>  ellipsis       0.3.2           2021-04-29 [2] RSPM #>  evaluate       0.17            2022-10-07 [2] RSPM #>  fansi          1.0.3           2022-03-24 [2] RSPM #>  farver         2.1.1           2022-07-06 [2] RSPM #>  fastmap        1.1.0           2021-01-25 [2] RSPM #>  float          0.3-0           2022-04-07 [2] RSPM #>  foreach        1.5.2           2022-02-02 [2] RSPM #>  fs             1.5.2           2021-12-08 [2] RSPM #>  furrr          0.3.1           2022-08-15 [2] RSPM #>  future         1.28.0          2022-09-02 [2] RSPM #>  future.apply   1.9.1           2022-09-07 [2] RSPM #>  generics       0.1.3           2022-07-05 [2] RSPM #>  ggplot2      * 3.3.6           2022-05-03 [2] RSPM #>  gibasa         0.5.0.9004      2022-10-29 [1] local #>  globals        0.16.1          2022-08-28 [2] RSPM #>  glue           1.6.2           2022-02-24 [2] RSPM #>  gower          1.0.0           2022-02-03 [2] RSPM #>  GPfit          1.0-8           2019-02-08 [2] RSPM #>  gtable         0.3.1           2022-09-01 [2] RSPM #>  hardhat        1.2.0           2022-06-30 [2] RSPM #>  highr          0.9             2021-04-16 [2] RSPM #>  hms            1.1.2           2022-08-19 [2] RSPM #>  htmltools      0.5.3           2022-07-18 [2] RSPM #>  infer        * 1.0.3           2022-08-22 [2] RSPM #>  ipred          0.9-13          2022-06-02 [2] RSPM #>  iterators      1.0.14          2022-02-05 [2] RSPM #>  jquerylib      0.1.4           2021-04-26 [2] RSPM #>  jsonlite       1.8.3           2022-10-21 [2] RSPM #>  knitr          1.40            2022-08-24 [2] RSPM #>  labeling       0.4.2           2020-10-20 [2] RSPM #>  lattice        0.20-45         2021-09-22 [4] CRAN (R 4.2.1) #>  lava           1.7.0           2022-10-25 [2] RSPM #>  ldccr          0.0.11.20220927 2022-10-29 [2] Github (paithiov909/ldccr@f2b4044) #>  lgr            0.4.4           2022-09-05 [2] RSPM #>  lhs            1.1.5           2022-03-22 [2] RSPM #>  lifecycle      1.0.3           2022-10-07 [2] RSPM #>  listenv        0.8.0           2019-12-05 [2] RSPM #>  lubridate      1.8.0           2021-10-07 [2] RSPM #>  magrittr       2.0.3           2022-03-30 [2] RSPM #>  MASS           7.3-57          2022-04-22 [4] CRAN (R 4.2.1) #>  Matrix         1.5-1           2022-09-13 [2] RSPM #>  memoise        2.0.1           2021-11-26 [2] RSPM #>  mlapi          0.1.1           2022-04-24 [2] RSPM #>  modeldata    * 1.0.1           2022-09-06 [2] RSPM #>  munsell        0.5.0           2018-06-12 [2] RSPM #>  nnet           7.3-17          2022-01-16 [4] CRAN (R 4.2.1) #>  parallelly     1.32.1          2022-07-21 [2] RSPM #>  parsnip      * 1.0.2           2022-10-01 [2] RSPM #>  pillar         1.8.1           2022-08-19 [2] RSPM #>  pkgconfig      2.0.3           2019-09-22 [2] RSPM #>  pkgdown        2.0.6           2022-07-16 [2] any (@2.0.6) #>  prodlim        2019.11.13      2019-11-17 [2] RSPM #>  purrr        * 0.3.5           2022-10-06 [2] RSPM #>  R.cache        0.16.0          2022-07-21 [2] RSPM #>  R.methodsS3    1.8.2           2022-06-13 [2] RSPM #>  R.oo           1.25.0          2022-06-12 [2] RSPM #>  R.utils        2.12.0          2022-06-28 [2] RSPM #>  R6             2.5.1           2021-08-19 [2] RSPM #>  ragg           1.2.4           2022-10-24 [2] RSPM #>  Rcpp           1.0.9           2022-07-08 [2] RSPM #>  RcppParallel   5.1.5           2022-01-05 [2] RSPM #>  readr          2.1.3           2022-10-01 [2] RSPM #>  recipes      * 1.0.2           2022-10-16 [2] RSPM #>  RhpcBLASctl    0.21-247.1      2021-11-05 [2] RSPM #>  rlang          1.0.6           2022-09-24 [2] RSPM #>  rmarkdown      2.17            2022-10-07 [2] RSPM #>  rpart          4.1.16          2022-01-24 [4] CRAN (R 4.2.1) #>  rprojroot      2.0.3           2022-04-02 [2] RSPM #>  rsample      * 1.1.0           2022-08-08 [2] RSPM #>  rsparse        0.5.1           2022-09-11 [2] RSPM #>  rstudioapi     0.14            2022-08-22 [2] RSPM #>  sass           0.4.2           2022-07-16 [2] RSPM #>  scales       * 1.2.1           2022-08-20 [2] RSPM #>  sessioninfo    1.2.2           2021-12-06 [2] any (@1.2.2) #>  stringi        1.7.8           2022-07-11 [2] RSPM #>  stringr        1.4.1           2022-08-20 [2] RSPM #>  styler         1.8.0           2022-10-22 [2] any (@1.8.0) #>  survival       3.3-1           2022-03-03 [4] CRAN (R 4.2.1) #>  systemfonts    1.0.4           2022-02-11 [2] RSPM #>  text2vec     * 0.6.2           2022-09-11 [2] RSPM #>  textrecipes  * 1.0.1           2022-10-06 [2] RSPM #>  textshaping    0.3.6           2021-10-13 [2] RSPM #>  tibble       * 3.1.8           2022-07-22 [2] RSPM #>  tidymodels   * 1.0.0           2022-07-13 [2] RSPM #>  tidyr        * 1.2.1           2022-09-08 [2] RSPM #>  tidyselect     1.2.0           2022-10-10 [2] RSPM #>  timeDate       4021.106        2022-09-30 [2] RSPM #>  tune         * 1.0.1           2022-10-09 [2] RSPM #>  tzdb           0.3.0           2022-03-28 [2] RSPM #>  utf8           1.2.2           2021-07-24 [2] RSPM #>  V8             4.2.1           2022-08-07 [2] RSPM #>  vctrs          0.5.0           2022-10-22 [2] RSPM #>  vroom          1.6.0           2022-09-30 [2] RSPM #>  withr          2.5.0           2022-03-03 [2] RSPM #>  workflows    * 1.1.0           2022-09-26 [2] RSPM #>  workflowsets * 1.0.0           2022-07-12 [2] RSPM #>  xfun           0.34            2022-10-18 [2] RSPM #>  xgboost      * 1.6.0.1         2022-04-16 [2] RSPM #>  yaml           2.3.6           2022-10-18 [2] RSPM #>  yardstick    * 1.1.0           2022-09-07 [2] RSPM #>  #>  [1] /tmp/RtmpdyYsXI/temp_libpath60bc3270ceec #>  [2] /home/runner/work/_temp/Library #>  [3] /opt/R/4.2.1/lib/R/site-library #>  [4] /opt/R/4.2.1/lib/R/library #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"https://paithiov909.github.io/gibasa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Akiru Kato. Author, maintainer. Shogo Ichinose. Author. Taku Kudo. Author. Nippon Telegraph Telephone Corporation. Copyright holder.","code":""},{"path":"https://paithiov909.github.io/gibasa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kato , Ichinose S, Kudo T (2022). gibasa: Alternate 'Rcpp' Interface 'MeCab'. R package version 0.5.0.9004, https://paithiov909.github.io/gibasa/.","code":"@Manual{,   title = {gibasa: An Alternate 'Rcpp' Interface to 'MeCab'},   author = {Akiru Kato and Shogo Ichinose and Taku Kudo},   year = {2022},   note = {R package version 0.5.0.9004},   url = {https://paithiov909.github.io/gibasa/}, }"},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"An Alternate Rcpp Interface to MeCab","text":"Gibasa plain ‘Rcpp’ interface ‘MeCab’, CJK tokenizer morphological analysis tool. main goal gibasa package provide alternative tidytext::unnest_tokens CJK text data. analyzing CJK text data, usually requires part--speech tagging, separated spaces tokenizers::tokenize_words sometimes splits erroneous tokens. Gibasa provides 3 main functions: gibasa::tokenize, gibasa::prettify, gibasa::pack. image gibasa::tokenize retrieves TIF-compliant data.frame corpus, returning tokens format known ‘tidy text data’, users can replace tidytext::unnest_tokens tokenizing CJK text. gibasa::prettify turns tagged features columns. gibasa::pack retrieves ‘tidy text data’, typically returning space-separated corpus.","code":""},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An Alternate Rcpp Interface to MeCab","text":"can install binary package via r-universe. use gibasa package requires MeCab library dictionary installed available. case using Linux OSX, can install package managers, build install source . case using Windows, use installer built 32bit built 64bit. Note gibasa requires UTF-8 dictionary, Shift-JIS one.","code":"# Enable repository from paithiov909 options(repos = c(   paithiov909 = \"https://paithiov909.r-universe.dev\",   CRAN = \"https://cloud.r-project.org\"))  # Download and install gibasa in R install.packages(\"gibasa\")  # Or build from source package Sys.setenv(MECAB_DEFAULT_RC = \"/fullpath/to/your/mecabrc\") # if necessary remotes::install_github(\"paithiov909/gibasa\")"},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"tokenize-sentences","dir":"","previous_headings":"Usage","what":"Tokenize sentences","title":"An Alternate Rcpp Interface to MeCab","text":"","code":"res <- gibasa::tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) head(res) #>   doc_id sentence_id token_id    token #> 1      1           1        1     その #> 2      1           1        2     ころ #> 3      1           1        3 わたくし #> 4      1           1        4       は #> 5      1           1        5       、 #> 6      1           1        6 モリーオ #>                                             feature #> 1                   連体詞,*,*,*,*,*,その,ソノ,ソノ #> 2         名詞,非自立,副詞可能,*,*,*,ころ,コロ,コロ #> 3 名詞,代名詞,一般,*,*,*,わたくし,ワタクシ,ワタクシ #> 4                      助詞,係助詞,*,*,*,*,は,ハ,ワ #> 5                        記号,読点,*,*,*,*,、,、,、 #> 6                     名詞,固有名詞,地域,一般,*,*,*"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"prettify-output","dir":"","previous_headings":"Usage","what":"Prettify output","title":"An Alternate Rcpp Interface to MeCab","text":"","code":"head(gibasa::prettify(res)) #>   doc_id sentence_id token_id    token   POS1     POS2     POS3 POS4 #> 1      1           1        1     その 連体詞     <NA>     <NA> <NA> #> 2      1           1        2     ころ   名詞   非自立 副詞可能 <NA> #> 3      1           1        3 わたくし   名詞   代名詞     一般 <NA> #> 4      1           1        4       は   助詞   係助詞     <NA> <NA> #> 5      1           1        5       、   記号     読点     <NA> <NA> #> 6      1           1        6 モリーオ   名詞 固有名詞     地域 一般 #>   X5StageUse1 X5StageUse2 Original    Yomi1    Yomi2 #> 1        <NA>        <NA>     その     ソノ     ソノ #> 2        <NA>        <NA>     ころ     コロ     コロ #> 3        <NA>        <NA> わたくし ワタクシ ワタクシ #> 4        <NA>        <NA>       は       ハ       ワ #> 5        <NA>        <NA>       、       、       、 #> 6        <NA>        <NA>     <NA>     <NA>     <NA> head(gibasa::prettify(res, col_select = 1:3)) #>   doc_id sentence_id token_id    token   POS1     POS2     POS3 #> 1      1           1        1     その 連体詞     <NA>     <NA> #> 2      1           1        2     ころ   名詞   非自立 副詞可能 #> 3      1           1        3 わたくし   名詞   代名詞     一般 #> 4      1           1        4       は   助詞   係助詞     <NA> #> 5      1           1        5       、   記号     読点     <NA> #> 6      1           1        6 モリーオ   名詞 固有名詞     地域 head(gibasa::prettify(res, col_select = c(1,3,5))) #>   doc_id sentence_id token_id    token   POS1     POS3 X5StageUse1 #> 1      1           1        1     その 連体詞     <NA>        <NA> #> 2      1           1        2     ころ   名詞 副詞可能        <NA> #> 3      1           1        3 わたくし   名詞     一般        <NA> #> 4      1           1        4       は   助詞     <NA>        <NA> #> 5      1           1        5       、   記号     <NA>        <NA> #> 6      1           1        6 モリーオ   名詞     地域        <NA> head(gibasa::prettify(res, col_select = c(\"POS1\", \"Original\"))) #>   doc_id sentence_id token_id    token   POS1 Original #> 1      1           1        1     その 連体詞     その #> 2      1           1        2     ころ   名詞     ころ #> 3      1           1        3 わたくし   名詞 わたくし #> 4      1           1        4       は   助詞       は #> 5      1           1        5       、   記号       、 #> 6      1           1        6 モリーオ   名詞     <NA>"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"pack-output","dir":"","previous_headings":"Usage","what":"Pack output","title":"An Alternate Rcpp Interface to MeCab","text":"","code":"res <- gibasa::prettify(res) gibasa::pack(res) #>   doc_id #> 1      1 #> 2      2 #> 3      3 #> 4      4 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text #> 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     その ころ わたくし は 、 モリーオ 市 の 博物 局 に 勤め て 居り まし た 。 #> 2 十 八 等 官 でし た から 役所 の なか でも 、 ず うっ と 下 の 方 でし た し 俸給 も ほんの わずか でし た が 、 受持ち が 標本 の 採集 や 整理 で 生れ 付き 好き な こと でし た から 、 わたくし は 毎日 ずいぶん 愉快 に はたらき まし た 。 殊に その ころ 、 モリーオ 市 で は 競馬 場 を 植物 園 に 拵え 直す と いう ので 、 その 景色 の いい まわり に アカシヤ を 植え込ん だ 広い 地面 が 、 切符 売場 や 信号 所 の 建物 の つい た まま 、 わたくし ども の 役所 の 方 へ まわっ て 来 た もの です から 、 わたくし は すぐ 宿直 という 名前 で 月賦 で 買っ た 小さな 蓄音器 と 二 十 枚 ばかり の レコード を もっ て 、 その 番小屋 に ひとり 住む こと に なり まし た 。 わたくし は そこ の 馬 を 置く 場所 に 板 で 小さな し きい を つけ て 一疋 の 山羊 を 飼い まし た 。 毎朝 その 乳 を しぼっ て つめたい パン を ひたし て た べ 、 それ から 黒い 革 の かばん へ すこし の 書類 や 雑誌 を 入れ 、 靴 も きれい に みがき 、 並木 の ポプラ の 影法師 を 大股 にわたって 市 の 役所 へ 出 て 行く の でし た 。 #> 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          あの イーハトーヴォ の すきとおっ た 風 、 夏 で も 底 に 冷た さ を もつ 青い そら 、 うつくしい 森 で 飾ら れ た モリーオ 市 、 郊外 の ぎらぎら ひかる 草 の 波 。 #> 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           また その なか で いっしょ に なっ た たくさん の ひと たち 、 ファゼーロ と ロザーロ 、 羊 飼 の ミーロ や 、 顔 の 赤い こども たち 、 地主 の テーモ 、 山猫 博士 の ボーガント・デストゥパーゴ など 、 いま この 暗い 巨 き な 石 の 建物 の なか で 考え て いる と 、 みんな むかし 風 の なつかしい 青い 幻 燈 の よう に 思わ れ ます 。 で は 、 わたくし は いつか の 小さな み だし を つけ ながら 、 しずか に あの 年 の イーハトーヴォ の 五月 から 十月 まで を 書きつけ ましょ う 。 gibasa::pack(res, POS1) #>   doc_id #> 1      1 #> 2      2 #> 3      3 #> 4      4 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text #> 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 連体詞 名詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 名詞 助詞 動詞 助詞 動詞 助動詞 助動詞 記号 #> 2 名詞 名詞 名詞 名詞 助動詞 助動詞 助詞 名詞 助詞 名詞 助詞 記号 助動詞 動詞 助詞 名詞 助詞 名詞 助動詞 助動詞 助詞 名詞 助詞 連体詞 副詞 助動詞 助動詞 助詞 記号 動詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 名詞 名詞 助動詞 名詞 助動詞 助動詞 助詞 記号 名詞 助詞 名詞 副詞 名詞 助詞 動詞 助動詞 助動詞 記号 副詞 連体詞 名詞 記号 名詞 名詞 助詞 助詞 名詞 名詞 助詞 名詞 名詞 助詞 動詞 動詞 助詞 動詞 助詞 記号 連体詞 名詞 助詞 形容詞 名詞 助詞 名詞 助詞 動詞 助動詞 形容詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 名詞 助詞 名詞 助詞 動詞 助動詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 助動詞 名詞 助動詞 助詞 記号 名詞 助詞 副詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助動詞 連体詞 名詞 助詞 名詞 名詞 名詞 助詞 助詞 名詞 助詞 動詞 助詞 記号 連体詞 名詞 助詞 副詞 動詞 名詞 助詞 動詞 助動詞 助動詞 記号 名詞 助詞 名詞 助詞 名詞 助詞 動詞 名詞 助詞 名詞 助詞 連体詞 助動詞 名詞 助詞 動詞 助詞 名詞 助詞 名詞 助詞 動詞 助動詞 助動詞 記号 名詞 連体詞 名詞 助詞 動詞 助詞 形容詞 名詞 助詞 動詞 動詞 助動詞 助詞 記号 名詞 助詞 形容詞 名詞 助詞 名詞 助詞 副詞 助詞 名詞 助詞 名詞 助詞 動詞 記号 名詞 助詞 名詞 助詞 動詞 記号 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 名詞 助動詞 助動詞 記号 #> 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          連体詞 名詞 助詞 動詞 助動詞 名詞 記号 名詞 助詞 助詞 名詞 助詞 形容詞 名詞 助詞 動詞 形容詞 感動詞 記号 形容詞 名詞 助詞 動詞 動詞 助動詞 名詞 名詞 記号 名詞 助詞 副詞 動詞 名詞 助詞 名詞 記号 #> 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   接続詞 連体詞 名詞 助詞 名詞 助詞 動詞 助動詞 名詞 助詞 名詞 名詞 記号 名詞 助詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 記号 名詞 助詞 形容詞 名詞 名詞 記号 名詞 助詞 名詞 記号 名詞 名詞 助詞 名詞 助詞 記号 名詞 連体詞 形容詞 名詞 助動詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 動詞 助詞 動詞 助詞 記号 名詞 名詞 名詞 助詞 形容詞 形容詞 名詞 名詞 助詞 名詞 助詞 動詞 動詞 助動詞 記号 助動詞 助詞 記号 名詞 助詞 名詞 助詞 連体詞 接頭詞 名詞 助詞 動詞 助詞 記号 名詞 助詞 連体詞 名詞 助詞 名詞 助詞 名詞 助詞 名詞 助詞 助詞 動詞 助動詞 助動詞 記号"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"change-dictionary","dir":"","previous_headings":"Usage","what":"Change dictionary","title":"An Alternate Rcpp Interface to MeCab","text":"IPA, UniDic, CC-CEDICT-MeCab, mecab-ko-dic schemes supported.","code":"## UniDic 2.1.2 gibasa::gbs_tokenize(\"あのイーハトーヴォのすきとおった風\", sys_dic = \"/mecab/unidic-lite\") |>      gibasa::prettify(into = gibasa::get_dict_features(\"unidic26\")) |>      head() #>   doc_id sentence_id token_id          token   POS1     POS2 POS3 POS4 #> 1      1           1        1           あの 感動詞 フィラー <NA> <NA> #> 2      1           1        2 イーハトーヴォ   名詞 普通名詞 一般 <NA> #> 3      1           1        3             の   助詞   格助詞 <NA> <NA> #> 4      1           1        4     すきとおっ   動詞     一般 <NA> <NA> #> 5      1           1        5             た 助動詞     <NA> <NA> <NA> #> 6      1           1        6             風   名詞 普通名詞 一般 <NA> #>       cType         cForm      lForm    lemma       orth       pron   orthBase #> 1      <NA>          <NA>       アノ     あの       あの       アノ       あの #> 2      <NA>          <NA>                                                      #> 3      <NA>          <NA>         ノ       の         の         ノ         の #> 4 五段-ラ行 連用形-促音便 スキトオル 透き通る すきとおっ スキトーッ すきとおる #> 5 助動詞-タ   連体形-一般         タ       た         た         タ         た #> 6      <NA>          <NA>       カゼ       風         風       カゼ         風 #>     pronBase goshu iType iForm fType fForm       kana   kanaBase       form #> 1       アノ    和  <NA>  <NA>  <NA>  <NA>       アノ       アノ       アノ #> 2                                                                           #> 3         ノ    和  <NA>  <NA>  <NA>  <NA>         ノ         ノ         ノ #> 4 スキトール    和  <NA>  <NA>  <NA>  <NA> スキトオッ スキトオル スキトオッ #> 5         タ    和  <NA>  <NA>  <NA>  <NA>         タ         タ         タ #> 6       カゼ    和  <NA>  <NA>  <NA>  <NA>       カゼ       カゼ       カゼ #>     formBase iConType fConType aType               aConType aModeType #> 1       アノ     <NA>     <NA>     0                   <NA>      <NA> #> 2                                                                     #> 3         ノ     <NA>     <NA>  <NA>                名詞%F1      <NA> #> 4 スキトオル     <NA>     <NA>     3                     C1      <NA> #> 5         タ     <NA>     <NA>  <NA> 動詞%F2@1,形容詞%F4@-2      <NA> #> 6       カゼ     <NA>     <NA>     0                     C4      <NA>  ## CC-CEDICT gibasa::gbs_tokenize(\"它可以进行日语和汉语的语态分析\", sys_dic = \"/mecab/cc-cedict\") |>      gibasa::prettify(into = gibasa::get_dict_features(\"cc-cedict\")) #>   doc_id sentence_id token_id            token POS1 POS2 POS3 POS4 pinyin_pron #> 1      1           1        1               它 <NA> <NA> <NA> <NA>         ta1 #> 2      1           1        2             可以 <NA> <NA> <NA> <NA>     ke3 yi3 #> 3      1           1        3       <U+8FDB>行 <NA> <NA> <NA> <NA>  jin4 xing2 #> 4      1           1        4       日<U+8BED> <NA> <NA> <NA> <NA>     Ri4 yu3 #> 5      1           1        5               和 <NA> <NA> <NA> <NA>         he2 #> 6      1           1        6 <U+6C49><U+8BED> <NA> <NA> <NA> <NA>    Han4 yu3 #> 7      1           1        7               的 <NA> <NA> <NA> <NA>         di4 #> 8      1           1        8 <U+8BED><U+6001> <NA> <NA> <NA> <NA>    yu3 tai4 #> 9      1           1        9             分析 <NA> <NA> <NA> <NA>    fen1 xi1 #>   traditional_char_form simplified_char_form #> 1                    它                   它 #> 2                  可以                 可以 #> 3                  進行           <U+8FDB>行 #> 4                  日語           日<U+8BED> #> 5              <U+9FA2>                   和 #> 6                  漢語     <U+6C49><U+8BED> #> 7                    的                   的 #> 8                  語態     <U+8BED><U+6001> #> 9                  分析                 分析 #>                                                                              definition #> 1                                                                                   it/ #> 2                                         can/may/possible/able to/not bad/pretty good/ #> 3 to advance/to conduct/underway/in progress/to do/to carry out/to carry on/to execute/ #> 4                                                                    Japanese language/ #> 5                                                    old variant of 和[he2]/harmonious/ #> 6                                                Chinese language/CL:門|<U+95E8>[men2]/ #> 7                                                                            aim/clear/ #> 8                                                                      voice (grammar)/ #> 9                                                    to analyze/analysis/CL:個|个[ge4]/  ## mecan-ko-dic gibasa::gbs_tokenize(\"하네다공항한정토트백\", sys_dic = \"/mecab/mecab-ko-dic\") |>      gibasa::prettify(into = gibasa::get_dict_features(\"ko-dic\")) #>   doc_id sentence_id token_id                    token POS          meaning #> 1      1           1        1 <U+D558><U+B124><U+B2E4> NNP <U+C778><U+BA85> #> 2      1           1        2         <U+ACF5><U+D56D> NNG <U+C7A5><U+C18C> #> 3      1           1        3         <U+D55C><U+C815> NNG             <NA> #> 4      1           1        4 <U+D1A0><U+D2B8><U+BC31> NNG             <NA> #>   presence                  reading     type first_pos last_pos #> 1        F <U+D558><U+B124><U+B2E4>     <NA>      <NA>     <NA> #> 2        T         <U+ACF5><U+D56D>     <NA>      <NA>     <NA> #> 3        T         <U+D55C><U+C815>     <NA>      <NA>     <NA> #> 4        T <U+D1A0><U+D2B8><U+BC31> Compound      <NA>     <NA> #>                                             expression #> 1                                                 <NA> #> 2                                                 <NA> #> 3                                                 <NA> #> 4 <U+D1A0><U+D2B8>/NNP/<U+C778><U+BA85>+<U+BC31>/NNG/*"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"An Alternate Rcpp Interface to MeCab","text":"GPL (>=3).","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind the term frequency and inverse document frequency — bind_tf_idf2","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"Calculates bind term frequency, inverse document frequency, tf-idf dataset. function experimentally supports 3 types term frequencies 4 types inverse document frequencies, implemented RMeCab package features.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"bind_tf_idf2(   tbl,   term = \"token\",   document = \"doc_id\",   n = \"n\",   tf = c(\"tf\", \"tf2\", \"tf3\"),   idf = c(\"idf\", \"idf2\", \"idf3\", \"idf4\"),   norm = FALSE,   rmecab_compat = TRUE )"},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"tbl tidy text dataset one-row-per-term-per-document. term Column containing terms string symbol. document Column containing document IDs string symbol. n Column containing document-term counts string symbol. tf Method computing term frequency. idf Method computing inverse document frequency. norm Logical; supplied TRUE, raw term counts normalized divided L2 norms computing IDF values. rmecab_compat Logical; supplied TRUE, computes values taking care compatibility RMeCab. Note RMeCab always computes IDF values using term frequency rather raw term counts, thus TF-IDF values may doubly affected term frequency.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"Types term frequency can switched tf argument: tf term frequency (raw count terms). tf2 logarithmic term frequency base 10. tf3 binary-weighted term frequency. Types inverse document frequencies can switched idf argument: idf inverse document frequency base 2, smoothed. 'smoothed' means just adding 1 raw counts logarithmizing. idf2 global frequency IDF. idf3 probabilistic IDF base 2. idf4 global entropy, IDF actual.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) df <- dplyr::group_by(df, doc_id) |>   dplyr::count(token) |>   dplyr::ungroup() bind_tf_idf2(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize sentence for character vector — gbs_tokenize","title":"Tokenize sentence for character vector — gbs_tokenize","text":"Tokenize sentence character vector","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize sentence for character vector — gbs_tokenize","text":"","code":"gbs_tokenize(   sentence,   sys_dic = \"\",   user_dic = \"\",   split = FALSE,   mode = c(\"parse\", \"wakati\") )"},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize sentence for character vector — gbs_tokenize","text":"sentence Character vector tokenized. sys_dic Character scalar; path system dictionary mecab. Note system dictionary expected compiled UTF-8, Shift-JIS encodings. user_dic Character scalar; path user dictionary mecab. split Logical. supplied TRUE, function internally splits sentence sub-sentences using stringi::stri_split_boudaries(type = \"sentence\"). mode Character scalar switch output format.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize sentence for character vector — gbs_tokenize","text":"data.frame named list.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize sentence for character vector — gbs_tokenize","text":"","code":"if (FALSE) { df <- gbs_tokenize(\"\\u3053\\u3093\\u306b\\u3061\\u306f\") head(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dictionary's features — get_dict_features","title":"Get dictionary's features — get_dict_features","text":"Returns dictionary's features. Currently supports \"unidic17\" (2.1.2 src schema), \"unidic26\" (2.1.2 bin schema), \"unidic29\" (schema used 2.2.0, 2.3.0), \"cc-cedict\", \"ko-dic\" (mecab-ko-dic), \"naist11\", \"sudachi\", \"ipa\".","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dictionary's features — get_dict_features","text":"","code":"get_dict_features(   dict = c(\"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\",     \"sudachi\") )"},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dictionary's features — get_dict_features","text":"dict Character scalar; one \"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\", \"sudachi\".","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dictionary's features — get_dict_features","text":"character vector.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/gibasa-package.html","id":null,"dir":"Reference","previous_headings":"","what":"gibasa: An Alternate 'Rcpp' Interface to 'MeCab' — gibasa-package","title":"gibasa: An Alternate 'Rcpp' Interface to 'MeCab' — gibasa-package","text":"Gibasa plain 'Rcpp' interface 'MeCab', CJK tokenizer morphological analysis tool. main goal gibasa package provide alternative 'tidytext' CJK text data.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/gibasa-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"gibasa: An Alternate 'Rcpp' Interface to 'MeCab' — gibasa-package","text":"Maintainer: Akiru Kato paithiov909@gmail.com Authors: Shogo Ichinose Taku Kudo contributors: Nippon Telegraph Telephone Corporation [copyright holder]","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if scalars are blank — is_blank","title":"Check if scalars are blank — is_blank","text":"Check scalars blank","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if scalars are blank — is_blank","text":"","code":"is_blank(x, trim = TRUE, ...)"},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if scalars are blank — is_blank","text":"x Object check emptiness. trim Logical. ... Additional arguments base::sapply().","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if scalars are blank — is_blank","text":"Logical.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if scalars are blank — is_blank","text":"","code":"is_blank(list(c(a = \"\", b = NA_character_), NULL)) #> [[1]] #>    a    b  #> TRUE TRUE  #>  #> [[2]] #> [1] TRUE #>"},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate lexical density — lex_density","title":"Calculate lexical density — lex_density","text":"lexical density proportion content words (lexical items) documents. function simple helper calculating lexical density given datasets.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate lexical density — lex_density","text":"","code":"lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))"},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate lexical density — lex_density","text":"vec character vector. contents_words character vector containing values counted contents words. targets character vector denominator lexical density filtered computing values. negate logical vector length 2. supplied TRUE, respectively negates predicate functions counting contents words targets.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate lexical density — lex_density","text":"numeric vector.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate lexical density — lex_density","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) df |>   prettify(col_select = \"POS1\") |>   dplyr::group_by(doc_id) |>   dplyr::summarise(     noun_ratio = lex_density(POS1,       \"\\u540d\\u8a5e\",       c(\"\\u52a9\\u8a5e\", \"\\u52a9\\u52d5\\u8a5e\"),       negate = c(FALSE, TRUE)     ),     mvr = lex_density(       POS1,       c(\"\\u5f62\\u5bb9\\u8a5e\", \"\\u526f\\u8a5e\", \"\\u9023\\u4f53\\u8a5e\"),       \"\\u52d5\\u8a5e\"     ),     vnr = lex_density(POS1, \"\\u52d5\\u8a5e\", \"\\u540d\\u8a5e\")   ) }"},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":null,"dir":"Reference","previous_headings":"","what":"Pack prettified data.frame of tokens — pack","title":"Pack prettified data.frame of tokens — pack","text":"Packs prettified data.frame tokens new data.frame corpus, compatible Text Interchange Formats.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pack prettified data.frame of tokens — pack","text":"","code":"pack(tbl, pull = \"token\", n = 1L, sep = \"-\", .collapse = \" \")"},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pack prettified data.frame of tokens — pack","text":"tbl prettified data.frame tokens. pull Column packed text ngrams body. Default value token. n Integer internally passed ngrams tokenizer function created audubon::ngram_tokenizer() sep Character scalar internally used concatenator ngrams. .collapse argument passed stringi::stri_join().","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pack prettified data.frame of tokens — pack","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"text-interchange-formats-tif-","dir":"Reference","previous_headings":"","what":"Text Interchange Formats (TIF)","title":"Pack prettified data.frame of tokens — pack","text":"Text Interchange Formats (TIF) set standards allows R text analysis packages target defined inputs outputs corpora, tokens, document-term matrices.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"valid-data-frame-of-tokens","dir":"Reference","previous_headings":"","what":"Valid data.frame of tokens","title":"Pack prettified data.frame of tokens — pack","text":"prettified data.frame tokens data.frame object compatible TIF. TIF valid data.frame tokens expected one unique key column (named doc_id) text several feature columns tokens. feature columns must contain least token .","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pack prettified data.frame of tokens — pack","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) pack(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Call tagger inside 'tbb::parallel_for' and return a data.frame. — posParallelRcpp","title":"Call tagger inside 'tbb::parallel_for' and return a data.frame. — posParallelRcpp","text":"Call tagger inside 'tbb::parallel_for' return data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call tagger inside 'tbb::parallel_for' and return a data.frame. — posParallelRcpp","text":"text Character vector. sys_dic String scalar. user_dic String scalar.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call tagger inside 'tbb::parallel_for' and return a data.frame. — posParallelRcpp","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":null,"dir":"Reference","previous_headings":"","what":"Prettify tokenized output — prettify","title":"Prettify tokenized output — prettify","text":"Turns single character column features separating delimiter.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prettify tokenized output — prettify","text":"","code":"prettify(df, into = get_dict_features(\"ipa\"), col_select = seq_along(into))"},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prettify tokenized output — prettify","text":"df data.frame feature column prettified. Character vector used column names features. col_select Character integer vector kept prettified features.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prettify tokenized output — prettify","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prettify tokenized output — prettify","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) prettify(df, col_select = 1:3) prettify(df, col_select = c(1, 3, 6)) prettify(df, col_select = c(\"POS1\", \"Original\")) }"},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize sentence for data.frame — tokenize","title":"Tokenize sentence for data.frame — tokenize","text":"Tokenize sentence data.frame","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize sentence for data.frame — tokenize","text":"","code":"tokenize(   tbl,   text_field = \"text\",   docid_field = \"doc_id\",   sys_dic = \"\",   user_dic = \"\",   split = FALSE )"},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize sentence for data.frame — tokenize","text":"tbl data.frame. text_field String symbol; column name get texts tokenized. docid_field String symbol; column name get identifiers texts. sys_dic Character scalar; path system dictionary mecab. Note system dictionary expected compiled UTF-8, Shift-JIS encodings. user_dic Character scalar; path user dictionary mecab. split Logical. supplied TRUE, function internally splits sentence sub-sentences using stringi::stri_split_boudaries(type = \"sentence\").","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize sentence for data.frame — tokenize","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize sentence for data.frame — tokenize","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(audubon::polano[5:8]),     text = audubon::polano[5:8]   ) ) head(df) }"},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-development-version","dir":"Changelog","previous_headings":"","what":"gibasa (development version)","title":"gibasa (development version)","text":"Added new function lex_density can calculate lexical density dataset. Bug fix bind_tf_idf2 function.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-050","dir":"Changelog","previous_headings":"","what":"gibasa 0.5.0","title":"gibasa 0.5.0","text":"gibasa now includes MeCab source, users need pre-install MeCab library building installing package (use tokenize without specifying dictionaries, still requires MeCab dictionaries installed available).","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-041","dir":"Changelog","previous_headings":"","what":"gibasa 0.4.1","title":"gibasa 0.4.1","text":"tokenize now preserves original order docid_field.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-040","dir":"Changelog","previous_headings":"","what":"gibasa 0.4.0","title":"gibasa 0.4.0","text":"Added bind_tf_idf2 function is_blank function.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-031","dir":"Changelog","previous_headings":"","what":"gibasa 0.3.1","title":"gibasa 0.3.1","text":"Updated dependencies.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-030","dir":"Changelog","previous_headings":"","what":"gibasa 0.3.0","title":"gibasa 0.3.0","text":"Changed build process Windows. Added vignette.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-021","dir":"Changelog","previous_headings":"","what":"gibasa 0.2.1","title":"gibasa 0.2.1","text":"prettify now can extract columns specified col_select.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-020","dir":"Changelog","previous_headings":"","what":"gibasa 0.2.0","title":"gibasa 0.2.0","text":"Added NEWS.md file track changes package. tokenize now gets data.frame first argument, returns data.frame . former function gets character vector returns data.frame named list renamed gbs_tokenize.","code":""}]
