[{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://paithiov909.github.io/gibasa/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"gibasaパッケージについて","dir":"Articles","previous_headings":"","what":"gibasaパッケージについて","title":"Introduction to gibasa","text":"gibasaは、RからMeCabを利用して形態素解析をおこなうためのパッケージです。 モチベーションとしては、tidytext::unnest_tokensを意識した処理をMeCabを利用しつつできるようにしたいということで開発しています。","code":""},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"インストールの仕方","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"インストールの仕方","title":"Introduction to gibasa","text":"CRANのほか、r-universeからもインストールできます。 バイナリパッケージが用意されていない環境では、ソースパッケージをビルドしてインストールします。ビルド時にMECAB_DEFAULT_RCという環境変数を内部的に指定するため、正しく動作させるにはmecabrcとMeCabの辞書があらかじめ適切な位置に配置されている必要があります。使っているOSのパッケージマネージャなどからインストールしておいてください。 Windowsの場合、ソースビルドにはRtoolsが必要です。また、インストールの方法によらず、gibasaを使用するにはMeCabの辞書が必要なため、これなどを使ってMeCabをインストールしておいてください。","code":"# Install gibasa from r-universe repository install.packages(\"gibasa\", repos = c(\"https://paithiov909.r-universe.dev\", \"https://cloud.r-project.org\")) # Sys.setenv(MECAB_DEFAULT_RC = \"/fullpath/to/your/mecabrc\") # if necessary remotes::install_github(\"paithiov909/gibasa\")"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"基本的な使い方","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"基本的な使い方","title":"Introduction to gibasa","text":"gibasaは、次にあげる関数を使って、CJKテキストの分かち書きをすることができるというパッケージです。 gibasa::tokenize gibasa::prettify gibasa::pack まず、doc_id列とtext列をもつデータフレームについて、gibasa::tokenizeでtidy textのかたちにできます（以下の例ではIPA辞書を使っています）。ちなみに、元のデータフレームのdoc_id列とtext列以外の列は戻り値にも保持されます。 gibasa::prettifyでfeature列の素性情報をパースして分割できます。このとき、col_select引数でパースしたい列を指定すると、それらの列だけをパースすることができます。 gibasa::packを使うと、pull引数で指定した列について、いわゆる「分かち書き」にすることができます。デフォルトではtoken列について分かち書きにします。","code":"gibasa::ginga[5] #> [1] \"　カムパネルラが手をあげました。それから四、五人手をあげました。ジョバンニも手をあげようとして、急いでそのままやめました。たしかにあれがみんな星だと、いつか雑誌で読んだのでしたが、このごろはジョバンニはまるで毎日教室でもねむく、本を読むひまも読む本もないので、なんだかどんなこともよくわからないという気持ちがするのでした。\"  dat <- data.frame(   doc_id = seq_along(gibasa::ginga[5:8]),   text = gibasa::ginga[5:8],   meta = c(\"aaa\", \"bbb\", \"ccc\", \"ddd\") )  res <- gibasa::tokenize(dat, text, doc_id)  head(res) #> # A tibble: 6 × 6 #>   doc_id meta  sentence_id token_id token        feature                         #>   <fct>  <chr>       <int>    <int> <chr>        <chr>                           #> 1 1      aaa             1        1 　           記号,空白,*,*,*,*,　,　,　      #> 2 1      aaa             1        2 カムパネルラ 名詞,一般,*,*,*,*,*             #> 3 1      aaa             1        3 が           助詞,格助詞,一般,*,*,*,が,ガ,ガ #> 4 1      aaa             1        4 手           名詞,一般,*,*,*,*,手,テ,テ      #> 5 1      aaa             1        5 を           助詞,格助詞,一般,*,*,*,を,ヲ,ヲ #> 6 1      aaa             1        6 あげ         動詞,自立,*,*,一段,連用形,あげ… head(gibasa::prettify(res)) #> # A tibble: 6 × 14 #>   doc_id meta  sentence_id token_id token   POS1  POS2   POS3  POS4  X5StageUse1 #>   <fct>  <chr>       <int>    <int> <chr>   <chr> <chr>  <chr> <chr> <chr>       #> 1 1      aaa             1        1 　      記号  空白   NA    NA    NA          #> 2 1      aaa             1        2 カムパ… 名詞  一般   NA    NA    NA          #> 3 1      aaa             1        3 が      助詞  格助詞 一般  NA    NA          #> 4 1      aaa             1        4 手      名詞  一般   NA    NA    NA          #> 5 1      aaa             1        5 を      助詞  格助詞 一般  NA    NA          #> 6 1      aaa             1        6 あげ    動詞  自立   NA    NA    一段        #> # ℹ 4 more variables: X5StageUse2 <chr>, Original <chr>, Yomi1 <chr>, #> #   Yomi2 <chr> head(gibasa::prettify(res, col_select = 1:3)) #> # A tibble: 6 × 8 #>   doc_id meta  sentence_id token_id token        POS1  POS2   POS3  #>   <fct>  <chr>       <int>    <int> <chr>        <chr> <chr>  <chr> #> 1 1      aaa             1        1 　           記号  空白   NA    #> 2 1      aaa             1        2 カムパネルラ 名詞  一般   NA    #> 3 1      aaa             1        3 が           助詞  格助詞 一般  #> 4 1      aaa             1        4 手           名詞  一般   NA    #> 5 1      aaa             1        5 を           助詞  格助詞 一般  #> 6 1      aaa             1        6 あげ         動詞  自立   NA head(gibasa::prettify(res, col_select = c(1, 3, 5))) #> # A tibble: 6 × 8 #>   doc_id meta  sentence_id token_id token        POS1  POS3  X5StageUse1 #>   <fct>  <chr>       <int>    <int> <chr>        <chr> <chr> <chr>       #> 1 1      aaa             1        1 　           記号  NA    NA          #> 2 1      aaa             1        2 カムパネルラ 名詞  NA    NA          #> 3 1      aaa             1        3 が           助詞  一般  NA          #> 4 1      aaa             1        4 手           名詞  NA    NA          #> 5 1      aaa             1        5 を           助詞  一般  NA          #> 6 1      aaa             1        6 あげ         動詞  NA    一段 head(gibasa::prettify(res, col_select = c(\"POS1\", \"Original\"))) #> # A tibble: 6 × 7 #>   doc_id meta  sentence_id token_id token        POS1  Original #>   <fct>  <chr>       <int>    <int> <chr>        <chr> <chr>    #> 1 1      aaa             1        1 　           記号  　       #> 2 1      aaa             1        2 カムパネルラ 名詞  NA       #> 3 1      aaa             1        3 が           助詞  が       #> 4 1      aaa             1        4 手           名詞  手       #> 5 1      aaa             1        5 を           助詞  を       #> 6 1      aaa             1        6 あげ         動詞  あげる res <- gibasa::prettify(res) gibasa::pack(res) #> # A tibble: 4 × 2 #>   doc_id text                                                                    #>   <fct>  <chr>                                                                   #> 1 1      　 カムパネルラ が 手 を あげ まし た 。 それ から 四 、 五 人 手 を …  #> 2 2      　 ところが 先生 は 早く も それ を 見つけ た の でし た 。             #> 3 3      「 ジョバンニ さん 。 あなた は わかっ て いる の でしょ う 」          #> 4 4      　 ジョバンニ は 勢い よく 立ちあがり まし た が 、 立っ て みる と も… gibasa::pack(res, POS1) #> # A tibble: 4 × 2 #>   doc_id text                                                                    #>   <fct>  <chr>                                                                   #> 1 1      記号 名詞 助詞 名詞 助詞 動詞 助動詞 助動詞 記号 名詞 助詞 名詞 記号 …  #> 2 2      記号 接続詞 名詞 助詞 形容詞 助詞 名詞 助詞 動詞 助動詞 名詞 助動詞 助… #> 3 3      記号 名詞 名詞 記号 名詞 助詞 動詞 助詞 動詞 名詞 助動詞 助動詞 記号    #> 4 4      記号 名詞 助詞 副詞 副詞 動詞 助動詞 助動詞 助詞 記号 動詞 助詞 動詞 …"},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"詳しい使い方","dir":"Articles","previous_headings":"gibasaパッケージについて","what":"詳しい使い方","title":"Introduction to gibasa","text":"より詳しい使い方については、次のサイトを参照してください。 RとMeCabによる日本語テキストマイニングの前処理","code":""},{"path":"https://paithiov909.github.io/gibasa/articles/gibasa.html","id":"ベンチマーク","dir":"Articles","previous_headings":"","what":"ベンチマーク","title":"Introduction to gibasa","text":"TokyoR #98でのLTのスライドでも紹介しましたが、ある程度の分量がある文字列ベクトルに対して、ごくふつうに形態素解析だけをやるかぎりにおいては、RMeCabよりもgibasaのほうが解析速度が速いと思います（というより、RMeCabでもMeCabを呼んでいる部分はおそらく十分速いのですが、欲しいかたちに加工するためのRの処理に時間がかかることが多いです）。","code":"dat <- data.frame(     doc_id = seq_along(gibasa::ginga),     text = gibasa::ginga )  gibasa <- function() {     gibasa::tokenize(dat) |>         gibasa::prettify(col_select = \"POS1\") |>         dplyr::mutate(doc_id = as.integer(doc_id)) |>         dplyr::select(doc_id, token, POS1) |>         as.data.frame() }  rmecab <- function() {     purrr::imap_dfr(         RMeCab::RMeCabDF(dat, 2),         ~ data.frame(doc_id = .y, token = unname(.x), POS1 = names(.x))     ) }  bench <- microbenchmark::microbenchmark(gibasa(),                                         rmecab(),                                         times = 20L,                                         check = \"equal\") ggplot2::autoplot(bench)"},{"path":"https://paithiov909.github.io/gibasa/articles/partial.html","id":"制約付き解析部分解析について","dir":"Articles","previous_headings":"","what":"制約付き解析（部分解析）について","title":"How to Use Partial Parsing Mode","text":"MeCabのやや発展的な使い方のひとつとして、制約付き解析があります。 入力文の一部の形態素情報が既知である、あるいは境界がわかっているときに、 それを満たすように解析する機能です。 たとえば、「にわにはにわにわとりがいる。」という文に対して、 「はにわ」の部分が名詞であるとか、「にわとり」の部分が一つの形態素 であるというように指定した上で解析することができます。このとき、 制約に反する4文字目の「は」が単独で形態素となったり、「にわとり」が「にわ」と「とり」 に分割されるような解析候補は排除されます。 制約付き解析を利用するには、解析させたい文を「文断片」や「形態素断片」に分けながら与えます。 それぞれの文断片や形態素断片は、改行（\\n）によって分けられます。文断片については通常と同じように形態素解析がおこなわれますが、文断片や形態素断片の間をまたぐような解析結果は抑制されます。また、形態素断片は表層形\\t素性パターンのようなかたちで与えることができます。形態素断片についてはそれ以上分割されず、与えられた断片がそのままのかたちで出力されます。","code":""},{"path":"https://paithiov909.github.io/gibasa/articles/partial.html","id":"ipa辞書による素の解析の場合","dir":"Articles","previous_headings":"","what":"IPA辞書による素の解析の場合","title":"How to Use Partial Parsing Mode","text":"一例として、ここではIPA辞書を使って、「月ノ美兎」という語彙を含む文を解析してみます。posDebugRcppは、与えられた文字列について、MeCabの-aオプションに相当する解析結果（解析結果になりえるすべての形態素の組み合わせ）を出力する関数です。ここでの最適解（is_best == \"01\"）である結果について確認すると、「月ノ美兎」という語彙は次のように複数の形態素に分割されてしまっていることがわかります。","code":"gibasa::posDebugRcpp(\"月ノ美兎は箱の中\") |>   dplyr::filter(is_best == \"01\") #>    doc_id pos_id surface                                 feature stat lcAttr #> 1       1      0                         BOS/EOS,*,*,*,*,*,*,*,*   02      0 #> 2       1     38      月          名詞,一般,*,*,*,*,月,ツキ,ツキ   00   1285 #> 3       1      4      ノ              記号,一般,*,*,*,*,ノ,ノ,ノ   00      5 #> 4       1     44      美  名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ   00   1291 #> 5       1     38      兎      名詞,一般,*,*,*,*,兎,ウサギ,ウサギ   00   1285 #> 6       1     16      は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261 #> 7       1     38      箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285 #> 8       1     24      の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368 #> 9       1     66      中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313 #> 10      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   03      0 #>    rcAttr     alpha      beta is_best prob wcost  cost #> 1       0      0.00 -22144.50      01    0     0     0 #> 2    1285  -6190.50 -15954.00      01    1  8537  8254 #> 3       5  -8874.75 -13269.75      01    1  4929 11833 #> 4    1291 -13974.00  -8170.50      01    1  7885 18632 #> 5    1285 -18080.25  -4064.25      01    1  5290 24107 #> 6     261 -18095.25  -4049.25      01    1  3865 24127 #> 7    1285 -22729.50    585.00      01    1  6142 30306 #> 8     368 -23010.00    865.50      01    1  4816 30680 #> 9    1313 -24007.50   1863.00      01    1  6528 32010 #> 10      0 -22144.50      0.00      01    1     0 29526"},{"path":"https://paithiov909.github.io/gibasa/articles/partial.html","id":"制約付き解析部分解析の場合","dir":"Articles","previous_headings":"","what":"制約付き解析（部分解析）の場合","title":"How to Use Partial Parsing Mode","text":"そこで、「月ノ美兎」という語彙を形態素断片として与えてみます。なお、posDebugRcppでは、partial=TRUEにすると-aに相当するオプションは無効になり、最適解のみが出力されます。 これでひとまず期待どおりに解析することができました。こうした使い方のほかに、形態素断片の素性パターンにワイルドカード（*）を使うと、その単語を切り出しながら品詞については適当なものを付与させることができます。以下では「月ノ美兎」については常に単語として切り出しながら、品詞は適当なものを付与させています。","code":"gibasa::posDebugRcpp(\"月ノ美兎\\t名詞,固有名詞,人名,*,*,*\\nは箱の中\", partial = TRUE) #>   doc_id pos_id  surface                                 feature stat lcAttr #> 1      1      0                          BOS/EOS,*,*,*,*,*,*,*,*   02      0 #> 2      1     42 月ノ美兎           名詞,固有名詞,人名,一般,*,*,*   01   1289 #> 3      1     16       は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261 #> 4      1     38       箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285 #> 5      1     24       の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368 #> 6      1     66       中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313 #> 7      1      0                          BOS/EOS,*,*,*,*,*,*,*,*   03      0 #>   rcAttr     alpha      beta is_best prob wcost  cost #> 1      0      0.00 -16275.00      01    0     0     0 #> 2   1289 -12134.25  -4140.75      01    1 17340 16179 #> 3    261 -12225.75  -4049.25      01    1  3865 16301 #> 4   1285 -16860.00    585.00      01    1  6142 22480 #> 5    368 -17140.50    865.50      01    1  4816 22854 #> 6   1313 -18138.00   1863.00      01    1  6528 24184 #> 7      0 -16275.00      0.00      01    1     0 21700 gibasa::posDebugRcpp(\"月ノ美兎\\t*\\nは箱の中\", partial = TRUE) #>   doc_id pos_id  surface                                 feature stat lcAttr #> 1      1      0                          BOS/EOS,*,*,*,*,*,*,*,*   02      0 #> 2      1     38 月ノ美兎                     名詞,一般,*,*,*,*,*   01   1285 #> 3      1     16       は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261 #> 4      1     38       箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285 #> 5      1     24       の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368 #> 6      1     66       中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313 #> 7      1      0                          BOS/EOS,*,*,*,*,*,*,*,*   03      0 #>   rcAttr     alpha      beta is_best prob wcost  cost #> 1      0      0.00 -12421.50      01    0     0     0 #> 2   1285  -8357.25  -4064.25      01    1 11426 11143 #> 3    261  -8372.25  -4049.25      01    1  3865 11163 #> 4   1285 -13006.50    585.00      01    1  6142 17342 #> 5    368 -13287.00    865.50      01    1  4816 17716 #> 6   1313 -14284.50   1863.00      01    1  6528 19046 #> 7      0 -12421.50      0.00      01    1     0 16562"},{"path":"https://paithiov909.github.io/gibasa/articles/partial.html","id":"具体的な使用例","dir":"Articles","previous_headings":"","what":"具体的な使用例","title":"How to Use Partial Parsing Mode","text":"partial引数はtokenizeにも実装されています。実用的には、たとえば次のように使えるかもしれません。 なお、この機能を利用時に、文断片や形態素断片の先頭や末尾に半角スペースが含まれていると、MeCab側でエラーが発生し、Rセッションごと落ちる可能性がありますので、注意してください。","code":"sentences <- c(   \"証券コードは４桁の銘柄識別コードです。\",   \"たとえば、7777です。あるいは7777 JPや7777.Tというのもあります。\",   \"また「７７７７ＪＰ」のような全角文字を使う表し方もあるかもしれません。\" )  sentences |>   stringi::stri_replace_all_regex(\"(?<codes>[0-9\\uFF10-\\uFF19]{4}((\\\\s|\\\\.)+[a-zA-Z]{1,2}|[\\uFF21-\\uFF3A]{2}))\", \"\\n${codes}\\t*\\n\") |>   gibasa::tokenize(partial = TRUE, mode = \"wakati\") #> $`1` #>  [1] \"証券\"   \"コード\" \"は\"     \"４\"     \"桁\"     \"の\"     \"銘柄\"   \"識別\"   #>  [9] \"コード\" \"です\"   \"。\"     #>  #> $`2` #>  [1] \"たとえば\" \"、\"       \"7777\"     \"です\"     \"。\"       \"あるいは\" #>  [7] \"7777 JP\"  \"や\"       \"7777.T\"   \"という\"   \"の\"       \"も\"       #> [13] \"あり\"     \"ます\"     \"。\"       #>  #> $`3` #>  [1] \"また\"         \"「\"           \"７７７７ＪＰ\" \"」\"           \"の\"           #>  [6] \"よう\"         \"な\"           \"全角\"         \"文字\"         \"を\"           #> [11] \"使う\"         \"表し\"         \"方\"           \"も\"           \"ある\"         #> [16] \"かも\"         \"しれ\"         \"ませ\"         \"ん\"           \"。\""},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"はじめに","dir":"Articles","previous_headings":"","what":"はじめに","title":"Text Mining with quanteda and gibasa","text":"このページでは、quantedaとgibasaを用いた簡単なテキストマイニングを例に、quantedaやその他のRパッケージとのあいだでテキストデータをやりとりする方法を紹介します。ここでおこなっているようなテキスト分析の例としては、次のサイトも参考にしてください。 Cookbook Draw KHCoder-like Visualizations Using R ちなみに、quantedaはstringiをラップした関数によって日本語の文書でも分かち書きできるので、手元の辞書に収録されている表現どおりに分かち書きしたい場合や、品詞情報が欲しい場合でないかぎりは、形態素解析器を使うメリットはあまりないかもしれません。stringiが利用しているICUのBoundary Analysisの仕様については、UAX#29などを参照してください。","code":""},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"text-interchange-formatstif","dir":"Articles","previous_headings":"","what":"Text Interchange Formats（TIF）","title":"Text Mining with quanteda and gibasa","text":"gibasaによる形態素解析の結果をquantedaなどと組み合わせて使ううえでは、Text Interchange Formats（TIF）という仕様に沿ったかたちのオブジェクトとしてテキストデータをもつことを意識すると便利です。 TIFというのは、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。TIFでは、コーパス（corpus）、トークン（token）、文書単語行列（dtm）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。 このうち「コーパス」は、文書の集合をデータフレームあるいは名前付きベクトルの形式で保持したものです。 ldccrでは、livedoorニュースコーパスをデータフレームの形式のコーパスとして読み込むことができます。より厳密に言うと、データフレーム形式のコーパスは、少なくともdoc_idとtextという列を含むデータフレームなので、doc_id列は自分でつくる必要があります。コーパスにおけるdoc_id列は、文書によって一意なID列（character型である必要がある）で、text列は文書本体になります。「少なくとも」なので、このほかの列にここでのcategory列のような文書のメタ情報などが含まれる場合があります。 livedoorニュースコーパスは文書分類をおこなうことを主な目的につくられたデータセットで、以下の9カテゴリのブログ記事からなっています。 トピックニュース Sports Watch ITライフハック 家電チャンネル MOVIE ENTER 独女通信 エスマックス livedoor HOMME Peachy このうち一部だけをgibasaで形態素解析して、データフレームの形式の「トークン」としてもっておきます。トークンは、コーパスを文書ごとに適当な単位にまとめあげながら格納したものです。それぞれのトークンは単語だったり、単語のN-gramだったりします。 ここではKH Coderで採用されている品詞体系を参考に、形態素解析された語の品詞情報を適当な値に置き換えています。なお、こうしたデータフレームの形式のトークンは、しばしばtidy textとも呼ばれます。","code":"tbl <- ldccr::read_ldnws() |>   dplyr::mutate(doc_id = as.character(dplyr::row_number())) |>   dplyr::rename(text = body) #> Parsing dokujo-tsushin... #> Parsing it-life-hack... #> Parsing kaden-channel... #> Parsing livedoor-homme... #> Parsing movie-enter... #> Parsing peachy... #> Parsing smax... #> Parsing sports-watch... #> Parsing topic-news... #> Done.  tbl #> # A tibble: 7,376 × 6 #>    category       file_path                       source time_stamp text  doc_id #>    <fct>          <chr>                           <chr>  <chr>      <chr> <chr>  #>  1 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"友…  1      #>  2 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"ネ…  2      #>  3 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"相…  3      #>  4 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"ム…  4      #>  5 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"税…  5      #>  6 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-3… \"読…  6      #>  7 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"大…  7      #>  8 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"結…  8      #>  9 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-2… \"お…  9      #> 10 dokujo-tsushin /tmp/Rtmp7VBWin/text/dokujo-ts… http:… 2010-05-3… \"初…  10     #> # ℹ 7,366 more rows toks <- tbl |>   dplyr::select(doc_id, category, text) |>   dplyr::slice_sample(prop = .8) |>   dplyr::mutate(     text = stringi::stri_trans_nfkc(text) |>       stringi::stri_replace_all_regex(\"(https?\\\\://[[:alnum:]\\\\.\\\\-_/]+)\", \"\\nURL\\tタグ\\n\") |>       stringi::stri_replace_all_regex(\"[\\\\s]{2,}\", \"\\n\") |>       stringi::stri_trim_both()   ) |>   gibasa::tokenize(text, partial = TRUE) |>   gibasa::prettify(     col_select = c(\"POS1\", \"POS2\", \"POS3\", \"Original\")   ) |>   dplyr::mutate(     pos = dplyr::case_when(       (POS1 == \"タグ\") ~ \"タグ\",       (is.na(Original) & stringr::str_detect(token, \"^[[:alpha:]]+$\")) ~ \"未知語\",       (POS1 == \"感動詞\") ~ \"感動詞\",       (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Han}]{1}$\")) ~ \"名詞C\",       (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"名詞B\",       (POS1 == \"名詞\" & POS2 == \"一般\") ~ \"名詞\",       (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"地域\") ~ \"地名\",       (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"人名\") ~ \"人名\",       (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"組織\") ~ \"組織名\",       (POS1 == \"名詞\" & POS2 == \"形容動詞語幹\") ~ \"形容動詞\",       (POS1 == \"名詞\" & POS2 == \"ナイ形容詞語幹\") ~ \"ナイ形容詞\",       (POS1 == \"名詞\" & POS2 == \"固有名詞\") ~ \"固有名詞\",       (POS1 == \"名詞\" & POS2 == \"サ変接続\") ~ \"サ変名詞\",       (POS1 == \"名詞\" & POS2 == \"副詞可能\") ~ \"副詞可能\",       (POS1 == \"動詞\" & POS2 == \"自立\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"動詞B\",       (POS1 == \"動詞\" & POS2 == \"自立\") ~ \"動詞\",       (POS1 == \"形容詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"形容詞B\",       (POS1 == \"形容詞\" & POS2 == \"非自立\") ~ \"形容詞（非自立）\",       (POS1 == \"形容詞\") ~ \"形容詞\",       (POS1 == \"副詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"副詞B\",       (POS1 == \"副詞\") ~ \"副詞\",       (POS1 == \"助動詞\" & Original %in% c(\"ない\", \"まい\", \"ぬ\", \"ん\")) ~ \"否定助動詞\",       .default = \"その他\"     )   ) |>   dplyr::select(doc_id, category, token_id, token, pos, Original) |>   dplyr::rename(original = Original)  toks #> # A tibble: 3,915,857 × 6 #>    doc_id category       token_id token  pos      original #>    <fct>  <fct>             <int> <chr>  <chr>    <chr>    #>  1 2736   livedoor-homme        1 <      サ変名詞 NA       #>  2 2736   livedoor-homme        2 コラム 名詞     コラム   #>  3 2736   livedoor-homme        3 >      サ変名詞 NA       #>  4 2736   livedoor-homme        4 名手   名詞     名手     #>  5 2736   livedoor-homme        5 が     その他   が       #>  6 2736   livedoor-homme        6 愛し   動詞     愛す     #>  7 2736   livedoor-homme        7 た     その他   た       #>  8 2736   livedoor-homme        8 パター 名詞     パター   #>  9 2736   livedoor-homme        9 【     その他   【       #> 10 2736   livedoor-homme       10 ゴルフ 名詞     ゴルフ   #> # ℹ 3,915,847 more rows"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"tidy-textからdfmへの変換","dir":"Articles","previous_headings":"","what":"Tidy textからdfmへの変換","title":"Text Mining with quanteda and gibasa","text":"さて、実際にこういったトークンからさまざまな分析をするには、トークンを品詞などによって取捨選択しながら、分析したい単位ごとにグルーピングして集計する必要があります。 簡単に集計するだけであれば、次のようにdplyrの関数を使って集計することができます。こうした文書ID、単語と、単語の文書内頻度の3つ組のかたちをしたデータフレームは、summarized textという呼び方をすることがあるようです。 一方で、たとえば、特定のトークンの連なりは連語と見なして一つのトークンとして集計したいといった場合には、dplyrだけで集計するのはなかなか大変です。そういったより複雑なケースでは、quantedaの枠組みと組み合わせて使ったほうが便利なことがあります。 たとえば、IPA辞書では正しく解析されない「スマートフォン」といった語について、形態素解析した結果を確認した後に再度まとめあげて集計したい場合、quantedaを使うと次のように書くことができます。また、ここでは、うまく「記号」として解析されなかった記号類を除外するために、正規表現にマッチするトークンだけに絞り込んで集計しています。 quanteda::dfm()の戻り値は「文書単語行列（dtm）」を疎行列オブジェクト（dgCMatrix ‘Matrix’ package）として保持したものです（厳密には、dgCMatrixをスロットにもっているS4オブジェクト）。dfmオブジェクトは、qunateda.textstats::textstat_frequency()などを使って、さらに集計することができます。","code":"toks |>   dplyr::filter(!pos %in% c(\"その他\", \"タグ\")) |>   dplyr::count(doc_id, token) |>   dplyr::arrange(dplyr::desc(n)) #> # A tibble: 1,035,532 × 3 #>    doc_id token            n #>    <fct>  <chr>        <int> #>  1 5681   /              607 #>  2 5681   ソフトバンク   508 #>  3 5196   /              204 #>  4 5081   ,              126 #>  5 5544   )              120 #>  6 5544   (              119 #>  7 1461   し             111 #>  8 5699   :              101 #>  9 5699   .               97 #> 10 5134   HTC             94 #> # ℹ 1,035,522 more rows dfm <- toks |>   dplyr::filter(!pos %in% c(\"その他\", \"タグ\")) |>   gibasa::pack() |>   quanteda::corpus() |>   quanteda::tokens(what = \"fastestword\", remove_url = TRUE) |>   quanteda::tokens_compound(     quanteda::phrase(c(\"スマート フォン\", \"s - max\"))   ) |>   quanteda::tokens_keep(     \"^[[:alnum:]]{2,}$\",     valuetype = \"regex\"   ) |>   quanteda::dfm()  dfm #> Document-feature matrix of: 5,900 documents, 57,383 features (99.74% sparse) and 0 docvars. #>       features #> docs   コラム 名手 愛し パター ゴルフ 特集 パッティング 持つ 不思議 難し #>   2736      1    8    1     18     12    2            9    2      2    1 #>   2838      0    0    0      0      0    0            0    1      0    0 #>   3570      0    0    0      0      0    1            0    0      0    0 #>   1524      0    0    0      0      0    0            0    0      0    0 #>   3643      0    0    0      0      0    0            0    0      0    0 #>   7150      0    0    1      0      0    0            0    0      0    0 #> [ reached max_ndoc ... 5,894 more documents, reached max_nfeat ... 57,373 more features ] dat <- dfm |>   quanteda.textstats::textstat_frequency() |>   dplyr::as_tibble()  head(dat) #> # A tibble: 6 × 5 #>   feature frequency  rank docfreq group #>   <chr>       <dbl> <dbl>   <dbl> <chr> #> 1 する        21936     1    5200 all   #> 2 ない        20669     2    4681 all   #> 3 なっ         8733     3    3936 all   #> 4 ある         7426     4    3317 all   #> 5 なる         6648     5    3285 all   #> 6 できる       6344     6    2610 all  dat |>   ggplot(aes(x = frequency, y = docfreq)) +   geom_jitter() +   gghighlight::gghighlight(     frequency > 2500 & docfreq < 2000   ) +   ggrepel::geom_text_repel(     aes(label = feature),     max.overlaps = 50   ) +   scale_x_log10() +   scale_y_sqrt() +   theme_bw() +   labs(x = \"出現回数\", y = \"文書数\")"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"summarized-textからdfmへの変換","dir":"Articles","previous_headings":"","what":"Summarized textからdfmへの変換","title":"Text Mining with quanteda and gibasa","text":"「文書単語行列（dtm）」は、トークンを集計した結果を縦長のデータフレームとしてもっているsummarized textを横に展開したものだと理解できます。両者は表現が異なるだけで、もっているデータとしては同じものであるため、quantedaにおけるdfmオブジェクトはtidytext::cast_dfm()を使ってsummarized textを変換することによって得ることもできます。 dfmオブジェクトはquanteda::dfm_*という名前の関数を使って語彙を減らしたり、単語頻度に重みづけをしたりすることができます。 ここではquanteda.textstats::textstat_simil()で単語間の類似度を得て、階層的クラスタリングをしてみます。","code":"dfm <- toks |>   dplyr::filter(     pos %in% c(       \"名詞\",       \"地名\", \"人名\", \"組織名\", \"固有名詞\",       \"動詞\", \"未知語\"     ),     stringr::str_detect(token, \"^[[:alnum:]]{2,}$\")   ) |>   dplyr::mutate(     token = dplyr::if_else(is.na(original), tolower(token), original),     token = paste(token, pos, sep = \"/\")   ) |>   dplyr::count(doc_id, token) |>   tidytext::cast_dfm(doc_id, token, n) clusters <- dfm |>   quanteda::dfm_trim(min_termfreq = 40, termfreq_type = \"rank\") |>   quanteda::dfm_weight(scheme = \"boolean\") |>   quanteda.textstats::textstat_simil(margin = \"features\", method = \"dice\") |>   rlang::as_function(~ 1 - .)() |>   as.dist() |>   hclust(method = \"ward.D2\")  dfm |>   quanteda::dfm_trim(min_termfreq = 40, termfreq_type = \"rank\") |>   quanteda::colSums() |>   tibble::enframe() |>   dplyr::mutate(     clust = (clusters |> cutree(k = 5))[name]   ) |>   ggplot(aes(x = value, y = name, fill = factor(clust))) +   geom_bar(stat = \"identity\", show.legend = FALSE) +   scale_x_sqrt() +   ggh4x::scale_y_dendrogram(hclust = clusters) +   labs(x = \"出現回数\", y = element_blank()) +   theme_bw() #> Warning: The S3 guide system was deprecated in ggplot2 3.5.0. #> ℹ It has been replaced by a ggproto system that can be extended. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated."},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"dfmからsummarized-textへの変換","dir":"Articles","previous_headings":"","what":"dfmからSummarized textへの変換","title":"Text Mining with quanteda and gibasa","text":"dfmオブジェクトはtidytext::tidy()でsummarized textのかたちに変換することができます。tidytext::tidy()とtidytext::cast_dfm()をあわせて使うことで、dfmオブジェクトとsummarized textのあいだで自由に変換しあうことができます。 ここでは、quanteda.textmodels::textmodel_ca()を使ってdfmオブジェクトを対応分析にかけます。この関数の戻り値はcaパッケージのオブジェクトと互換性があるため、library(ca)としてからplot()に渡すことでバイプロットを描画することができます。  より見やすい表現としては、次のようにしてggplot2でバイプロットを描画することもできます。","code":"dfm <- dfm |>   quanteda::dfm_trim(     min_termfreq = 40,     termfreq_type = \"rank\"   ) |>   tidytext::tidy() |>   dplyr::left_join(     tbl |>       dplyr::mutate(doc_id = factor(dplyr::row_number())) |>       dplyr::select(doc_id, category),     by = dplyr::join_by(document == doc_id)   ) |>   tidytext::cast_dfm(category, term, count) ca_fit <- dfm |>   quanteda.textmodels::textmodel_ca(nd = 2, sparse = TRUE)  library(ca) dat <- plot(ca_fit) tf <- quanteda::colSums(dfm)  make_ca_plot_df <- function(ca.plot.obj, row.lab = \"Rows\", col.lab = \"Columns\") {   tibble::tibble(     Label = c(       rownames(ca.plot.obj$rows),       rownames(ca.plot.obj$cols)     ),     Dim1 = c(       ca.plot.obj$rows[, 1],       ca.plot.obj$cols[, 1]     ),     Dim2 = c(       ca.plot.obj$rows[, 2],       ca.plot.obj$cols[, 2]     ),     Variable = c(       rep(row.lab, nrow(ca.plot.obj$rows)),       rep(col.lab, nrow(ca.plot.obj$cols))     )   ) } dat <- dat |>   make_ca_plot_df(row.lab = \"Construction\", col.lab = \"Medium\") |>   dplyr::mutate(     Size = dplyr::if_else(Variable == \"Construction\", mean(tf), tf[Label])   ) # 非ASCII文字のラベルに対してwarningを出さないようにする suppressWarnings({   ca_sum <- summary(ca_fit)   dim_var_percs <- ca_sum$scree[, \"values2\"] })  dat |>   ggplot(aes(x = Dim1, y = Dim2, col = Variable, label = Label)) +   geom_vline(xintercept = 0, lty = \"dashed\", alpha = .5) +   geom_hline(yintercept = 0, lty = \"dashed\", alpha = .5) +   geom_jitter(aes(size = Size), alpha = .2, show.legend = FALSE) +   ggrepel::geom_label_repel(     data = \\(x) dplyr::filter(x, Variable == \"Construction\"),     show.legend = FALSE   ) +   ggrepel::geom_text_repel(     data = \\(x) dplyr::filter(x, Variable == \"Medium\", sqrt(Dim1^2 + Dim2^2) > 0.25),     show.legend = FALSE   ) +   scale_x_continuous(     limits = range(dat$Dim1) +       c(diff(range(dat$Dim1)) * -0.2, diff(range(dat$Dim1)) * 0.2)   ) +   scale_y_continuous(     limits = range(dat$Dim2) +       c(diff(range(dat$Dim2)) * -0.2, diff(range(dat$Dim2)) * 0.2)   ) +   scale_size_area(max_size = 12) +   labs(     x = paste0(\"Dimension 1 (\", signif(dim_var_percs[1], 3), \"%)\"),     y = paste0(\"Dimension 2 (\", signif(dim_var_percs[2], 3), \"%)\")   ) +   theme_classic() #> Warning: ggrepel: 15 unlabeled data points (too many overlaps). Consider #> increasing max.overlaps"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"まとめ","dir":"Articles","previous_headings":"","what":"まとめ","title":"Text Mining with quanteda and gibasa","text":"ここで紹介したような、quantedaが実装しているオブジェクトとTIFにおけるコーパス（corpus）、トークン（token）、文書単語行列（dtm）とのあいだで変換する操作は、だいたい次の図のようにして実現することができます。 この図では「Text Data」として括られているのがTIFにおけるコーパスで、最下段の「Corpus Object」と「Document-Term Matrix」として図示されているのがquantedaにおけるオブジェクトになります。ちなみに、この図中にはありませんが、tidy textからコーパスのかたちを経ずにquantedaのtokensオブジェクトに変換するには、たとえば次のようにします。 一度にぜんぶ覚えるのはむずかしいでしょうが、こうした変換の方法を一通り覚えておくと、Rでテキストデータを扱うためのさまざまなバッケージについて、臨機応変に使いこなせるようになるはずです。少しずつでも、ぜひマスターしてみてください。","code":"toks |>   dplyr::filter(     pos %in% c(       \"名詞\",       \"地名\", \"人名\", \"組織名\", \"固有名詞\",       \"動詞\", \"未知語\"     ),     stringr::str_detect(token, \"^[[:alnum:]]{2,}$\")   ) |>   dplyr::reframe(token = list(token), .by = doc_id) |>   tibble::deframe() |>   quanteda::as.tokens() #> Tokens consisting of 5,900 documents. #> 2736 : #>  [1] \"コラム\"       \"名手\"         \"愛し\"         \"パター\"       \"ゴルフ\"       #>  [6] \"パッティング\" \"持つ\"         \"ゴルフ\"       \"スポーツ\"     \"魅力\"         #> [11] \"一つ\"         \"ドライバー\"   #> [ ... and 231 more ] #>  #> 2838 : #>  [1] \"プリウス\"               \"最大\"                   \"ライバル\"               #>  [4] \"エクステンデッドレンジ\" \"EV\"                     \"パシフィコ横浜\"         #>  [7] \"行わ\"                   \"クルマ\"                 \"テクノロジー\"           #> [10] \"クルマ\"                 \"シボレー\"               \"ボルト\"                 #> [ ... and 139 more ] #>  #> 3570 : #>  [1] \"東日本\"     \"大震災\"     \"マスコミ\"   \"災害\"       \"語る\"       #>  [6] \"映画\"       \"東日本\"     \"大震災\"     \"経と\"       \"NHK\"        #> [11] \"音楽\"       \"メッセージ\" #> [ ... and 149 more ] #>  #> 1524 : #>  [1] \"ターガス\"           \"USB\"                \"ポートリプリケータ\" #>  [4] \"ビューアー\"         \"無償\"               \"ゲット\"             #>  [7] \"ターガス\"           \"言え\"               \"PC\"                 #> [10] \"ノート\"             \"PC\"                 \"富む\"               #> [ ... and 317 more ] #>  #> 3643 : #>  [1] \"DAIGO\"              \"ウルトラマン\"       \"ウルトラマン\"       #>  [4] \"シリーズ\"           \"最新\"               \"ウルトラマンサーガ\" #>  [7] \"全国\"               \"ウルトラマン\"       \"DAIGO\"              #> [10] \"剛士\"               \"杉浦\"               \"太陽\"               #> [ ... and 143 more ] #>  #> 7150 : #>  [1] \"DV\"     \"早乙女\" \"太一\"   \"西山\"   \"茉希\"   \"驚き\"   \"俳優\"   \"早乙女\" #>  [9] \"太一\"   \"東京\"   \"明治\"   \"舞台\"   #> [ ... and 105 more ] #>  #> [ reached max_ndoc ... 5,894 more documents ]"},{"path":"https://paithiov909.github.io/gibasa/articles/quanteda.html","id":"セッション情報","dir":"Articles","previous_headings":"","what":"セッション情報","title":"Text Mining with quanteda and gibasa","text":"","code":"sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.3.3 (2024-02-29) #>  os       Ubuntu 22.04.4 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  C.UTF-8 #>  ctype    C.UTF-8 #>  tz       UTC #>  date     2024-03-23 #>  pandoc   3.1.11 @ /opt/hostedtoolcache/pandoc/3.1.11/x64/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  package             * version    date (UTC) lib source #>  bit                   4.0.5      2022-11-15 [2] RSPM #>  bit64                 4.0.5      2020-08-30 [2] RSPM #>  bslib                 0.6.1      2023-11-28 [2] RSPM #>  ca                  * 0.71.1     2020-01-24 [2] RSPM #>  cachem                1.0.8      2023-05-01 [2] RSPM #>  cli                   3.6.2      2023-12-11 [2] RSPM #>  codetools             0.2-19     2023-02-01 [4] CRAN (R 4.3.3) #>  colorspace            2.1-0      2023-01-23 [2] RSPM #>  crayon                1.5.2      2022-09-29 [2] RSPM #>  desc                  1.4.3      2023-12-10 [2] RSPM #>  digest                0.6.35     2024-03-11 [2] RSPM #>  dplyr                 1.1.4      2023-11-17 [2] RSPM #>  evaluate              0.23       2023-11-01 [2] RSPM #>  fansi                 1.0.6      2023-12-08 [2] RSPM #>  farver                2.1.1      2022-07-06 [2] RSPM #>  fastmap               1.1.1      2023-02-24 [2] RSPM #>  fastmatch             1.1-4      2023-08-18 [2] RSPM #>  foreach               1.5.2      2022-02-02 [2] RSPM #>  fs                    1.6.3      2023-07-20 [2] RSPM #>  generics              0.1.3      2022-07-05 [2] RSPM #>  ggdendro              0.2.0      2024-02-23 [2] RSPM #>  ggh4x                 0.2.8      2024-01-23 [2] RSPM #>  gghighlight           0.4.1      2023-12-16 [2] RSPM #>  ggplot2             * 3.5.0      2024-02-23 [2] RSPM #>  ggrepel               0.9.5      2024-01-10 [2] RSPM #>  gibasa                1.1.0      2024-03-23 [1] local #>  glmnet                4.1-8      2023-08-22 [2] RSPM #>  glue                  1.7.0      2024-01-09 [2] RSPM #>  gtable                0.3.4      2023-08-21 [2] RSPM #>  highr                 0.10       2022-12-22 [2] RSPM #>  hms                   1.1.3      2023-03-21 [2] RSPM #>  htmltools             0.5.7      2023-11-03 [2] RSPM #>  iterators             1.0.14     2022-02-05 [2] RSPM #>  janeaustenr           1.0.0      2022-08-26 [2] RSPM #>  jquerylib             0.1.4      2021-04-26 [2] RSPM #>  jsonlite              1.8.8      2023-12-04 [2] RSPM #>  knitr                 1.45       2023-10-30 [2] RSPM #>  labeling              0.4.3      2023-08-29 [2] RSPM #>  lattice               0.22-5     2023-10-24 [4] CRAN (R 4.3.3) #>  ldccr                 2024.02.04 2024-03-22 [2] Github (paithiov909/ldccr@0f566b0) #>  LiblineaR             2.10-23    2023-12-11 [2] RSPM #>  lifecycle             1.0.4      2023-11-07 [2] RSPM #>  magrittr              2.0.3      2022-03-30 [2] RSPM #>  MASS                  7.3-60.0.1 2024-01-13 [4] CRAN (R 4.3.3) #>  Matrix                1.6-5      2024-01-11 [4] CRAN (R 4.3.3) #>  memoise               2.0.1      2021-11-26 [2] RSPM #>  munsell               0.5.0      2018-06-12 [2] RSPM #>  nsyllable             1.0.1      2022-02-28 [2] RSPM #>  pillar                1.9.0      2023-03-22 [2] RSPM #>  pkgconfig             2.0.3      2019-09-22 [2] RSPM #>  pkgdown               2.0.7      2022-12-14 [2] any (@2.0.7) #>  proxyC                0.3.4      2023-10-25 [2] RSPM #>  purrr                 1.0.2      2023-08-10 [2] RSPM #>  quanteda              3.3.1      2023-05-18 [2] RSPM #>  quanteda.textmodels   0.9.6      2023-03-22 [2] RSPM #>  quanteda.textstats    0.96.4     2023-11-02 [2] RSPM #>  R.cache               0.16.0     2022-07-21 [2] RSPM #>  R.methodsS3           1.8.2      2022-06-13 [2] RSPM #>  R.oo                  1.26.0     2024-01-24 [2] RSPM #>  R.utils               2.12.3     2023-11-18 [2] RSPM #>  R6                    2.5.1      2021-08-19 [2] RSPM #>  ragg                  1.3.0      2024-03-13 [2] RSPM #>  Rcpp                  1.0.12     2024-01-09 [2] RSPM #>  RcppParallel          5.1.7      2023-02-27 [2] RSPM #>  readr                 2.1.5      2024-01-10 [2] RSPM #>  rlang                 1.1.3      2024-01-10 [2] RSPM #>  rmarkdown             2.26       2024-03-05 [2] RSPM #>  RSpectra              0.16-1     2022-04-24 [2] RSPM #>  sass                  0.4.9      2024-03-15 [2] RSPM #>  scales                1.3.0      2023-11-28 [2] RSPM #>  sessioninfo           1.2.2      2021-12-06 [2] any (@1.2.2) #>  shape                 1.4.6.1    2024-02-23 [2] RSPM #>  SnowballC             0.7.1      2023-04-25 [2] RSPM #>  SparseM               1.81       2021-02-18 [2] RSPM #>  stopwords             2.3        2021-10-28 [2] RSPM #>  stringi               1.8.3      2023-12-11 [2] RSPM #>  stringr               1.5.1      2023-11-14 [2] RSPM #>  styler                1.10.2     2023-08-29 [2] any (@1.10.2) #>  survival              3.5-8      2024-02-14 [4] CRAN (R 4.3.3) #>  systemfonts           1.0.6      2024-03-07 [2] RSPM #>  textshaping           0.3.7      2023-10-09 [2] RSPM #>  tibble                3.2.1      2023-03-20 [2] RSPM #>  tidyselect            1.2.1      2024-03-11 [2] RSPM #>  tidytext              0.4.1      2023-01-07 [2] RSPM #>  tokenizers            0.3.0      2022-12-22 [2] RSPM #>  tzdb                  0.4.0      2023-05-12 [2] RSPM #>  utf8                  1.2.4      2023-10-22 [2] RSPM #>  vctrs                 0.6.5      2023-12-01 [2] RSPM #>  vroom                 1.6.5      2023-12-05 [2] RSPM #>  withr                 3.0.0      2024-01-16 [2] RSPM #>  xfun                  0.42       2024-02-08 [2] RSPM #>  yaml                  2.3.8      2023-12-11 [2] RSPM #>  #>  [1] /tmp/RtmpheSglV/temp_libpath36517838a50d #>  [2] /home/runner/work/_temp/Library #>  [3] /opt/R/4.3.3/lib/R/site-library #>  [4] /opt/R/4.3.3/lib/R/library #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"データの準備","dir":"Articles","previous_headings":"","what":"データの準備","title":"Supervised Learning Using tidymodels and gibasa","text":"livedoorニュースコーパスを使います。このコーパスのカテゴリ分類はかなり易しいタスクであることが知られている（というか、一部のカテゴリではそのカテゴリを同定できる単語が本文に含まれてしまっている）ので、機械学習を手軽に試すのに便利です。テキストの特徴量をもとに以下の9カテゴリの分類をします。 トピックニュース Sports Watch ITライフハック 家電チャンネル MOVIE ENTER 独女通信 エスマックス livedoor HOMME Peachy ldccrでデータフレームにします。 ここでは、KH Coderの品詞体系における名詞・地名・人名・組織名・固有名詞・動詞・未知語を抽出し、IPA辞書に収録されている語については原形にしながら分かち書きにします。","code":"tbl <- ldccr::read_ldnws() |>   dplyr::mutate(doc_id = as.character(dplyr::row_number())) #> Parsing dokujo-tsushin... #> Parsing it-life-hack... #> Parsing kaden-channel... #> Parsing livedoor-homme... #> Parsing movie-enter... #> Parsing peachy... #> Parsing smax... #> Parsing sports-watch... #> Parsing topic-news... #> Done. corp <- tbl |>   dplyr::mutate(     text = stringi::stri_trans_nfkc(body) |>       stringi::stri_replace_all_regex(\"(https?\\\\://[[:alnum:]\\\\.\\\\-_/]+)\", \"\\nURL\\tタグ\\n\") |>       stringi::stri_replace_all_regex(\"[\\\\s]{2,}\", \"\\n\") |>       stringi::stri_trim_both(),     chunk = dplyr::ntile(dplyr::row_number(), 10)   ) |>   dplyr::group_by(chunk) |>   dplyr::group_modify(\\(df, idx) {     data.frame(       doc_id = df$doc_id,       text = df$text     ) |>       gibasa::tokenize(text, partial = TRUE) |>       gibasa::prettify(         col_select = c(\"POS1\", \"POS2\", \"POS3\", \"Original\")       ) |>       dplyr::mutate(         pos = dplyr::case_when(           (POS1 == \"タグ\") ~ \"タグ\",           (is.na(Original) & stringr::str_detect(token, \"^[[:alpha:]]+$\")) ~ \"未知語\",           (POS1 == \"感動詞\") ~ \"感動詞\",           (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Han}]{1}$\")) ~ \"名詞C\",           (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"名詞B\",           (POS1 == \"名詞\" & POS2 == \"一般\") ~ \"名詞\",           (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"地域\") ~ \"地名\",           (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"人名\") ~ \"人名\",           (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"組織\") ~ \"組織名\",           (POS1 == \"名詞\" & POS2 == \"形容動詞語幹\") ~ \"形容動詞\",           (POS1 == \"名詞\" & POS2 == \"ナイ形容詞語幹\") ~ \"ナイ形容詞\",           (POS1 == \"名詞\" & POS2 == \"固有名詞\") ~ \"固有名詞\",           (POS1 == \"名詞\" & POS2 == \"サ変接続\") ~ \"サ変名詞\",           (POS1 == \"名詞\" & POS2 == \"副詞可能\") ~ \"副詞可能\",           (POS1 == \"動詞\" & POS2 == \"自立\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"動詞B\",           (POS1 == \"動詞\" & POS2 == \"自立\") ~ \"動詞\",           (POS1 == \"形容詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"形容詞B\",           (POS1 == \"形容詞\" & POS2 == \"非自立\") ~ \"形容詞（非自立）\",           (POS1 == \"形容詞\") ~ \"形容詞\",           (POS1 == \"副詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"副詞B\",           (POS1 == \"副詞\") ~ \"副詞\",           (POS1 == \"助動詞\" & Original %in% c(\"ない\", \"まい\", \"ぬ\", \"ん\")) ~ \"否定助動詞\",           .default = \"その他\"         )       ) |>       dplyr::filter(         pos %in% c(           \"名詞\",           \"地名\", \"人名\", \"組織名\", \"固有名詞\",           \"動詞\", \"未知語\"         )       ) |>       dplyr::mutate(         doc_id = droplevels(doc_id),         token = dplyr::if_else(is.na(Original), token, Original),         token = paste(token, pos, sep = \"/\")       ) |>       gibasa::pack()   }) |>   dplyr::ungroup() |>   dplyr::left_join(dplyr::select(tbl, doc_id, category), by = \"doc_id\")"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"モデルの学習","dir":"Articles","previous_headings":"","what":"モデルの学習","title":"Supervised Learning Using tidymodels and gibasa","text":"データを分割します。 以下のレシピとモデルで学習します。ここでは、ハッシュトリックを使っています。 なお、tidymodelsの枠組みの外であらかじめ分かち書きを済ませましたが、textrecipes::step_tokenizeのcustom_token引数に独自にトークナイザを指定することで、一つのstepとして分かち書きすることもできます。 F値をメトリクスにして学習します。5分割CVで、簡単にですが、ハイパーパラメータ探索をします。 ハイパラ探索の要約を確認します。  fitします。 学習したモデルの精度を見てみます。","code":"corp_split <- rsample::initial_split(corp, prop = .8, strata = \"category\") corp_train <- rsample::training(corp_split) corp_test <- rsample::testing(corp_split) NUM_TERMS <- 100L  corp_spec <-   parsnip::boost_tree(     trees = !!NUM_TERMS, # model_specに外にある変数を与える場合には、このようにinjectionします     tree_depth = tune::tune(),     mtry = tune::tune(),     min_n = 5,     learn_rate = .3,     stop_iter = 5 # 例なので小さな値にしています   ) |>   parsnip::set_engine(     \"xgboost\",     nthread = !!max(1, parallel::detectCores() - 1, na.rm = TRUE)   ) |>   parsnip::set_mode(\"classification\")  corp_rec <-   recipes::recipe(     category ~ text,     data = corp_train   ) |>   textrecipes::step_tokenize(     text,     custom_token = \\(x) strsplit(x, \" +\")   ) |>   textrecipes::step_tokenfilter(     text,     max_times = nrow(corp_train),     max_tokens = NUM_TERMS * 5   ) |>   textrecipes::step_texthash(text, num_terms = NUM_TERMS) corp_wflow <-   workflows::workflow() |>   workflows::add_model(corp_spec) |>   workflows::add_recipe(corp_rec) corp_tune_res <-   corp_wflow |>   tune::tune_grid(     resamples = rsample::vfold_cv(corp_train, strata = category, v = 5L),     grid = dials::grid_latin_hypercube(       dials::tree_depth(),       dials::mtry(range = c(30L, NUM_TERMS)),       size = 10L     ),     metrics = yardstick::metric_set(yardstick::f_meas),     control = tune::control_grid(save_pred = TRUE)   ) ggplot2::autoplot(corp_tune_res) corp_wflow <-   tune::finalize_workflow(corp_wflow, tune::select_best(corp_tune_res, metric = \"f_meas\"))  corp_fit <- tune::last_fit(corp_wflow, corp_split) corp_fit |>   tune::collect_predictions() |>   yardstick::f_meas(truth = category, estimate = .pred_class) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 f_meas  macro          0.844"},{"path":"https://paithiov909.github.io/gibasa/articles/textrecipes.html","id":"セッション情報","dir":"Articles","previous_headings":"","what":"セッション情報","title":"Supervised Learning Using tidymodels and gibasa","text":"","code":"sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.3.3 (2024-02-29) #>  os       Ubuntu 22.04.4 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  C.UTF-8 #>  ctype    C.UTF-8 #>  tz       UTC #>  date     2024-03-23 #>  pandoc   3.1.11 @ /opt/hostedtoolcache/pandoc/3.1.11/x64/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  package      * version    date (UTC) lib source #>  backports      1.4.1      2021-12-13 [2] RSPM #>  bit            4.0.5      2022-11-15 [2] RSPM #>  bit64          4.0.5      2020-08-30 [2] RSPM #>  broom        * 1.0.5      2023-06-09 [2] RSPM #>  bslib          0.6.1      2023-11-28 [2] RSPM #>  cachem         1.0.8      2023-05-01 [2] RSPM #>  class          7.3-22     2023-05-03 [4] CRAN (R 4.3.3) #>  cli            3.6.2      2023-12-11 [2] RSPM #>  codetools      0.2-19     2023-02-01 [4] CRAN (R 4.3.3) #>  colorspace     2.1-0      2023-01-23 [2] RSPM #>  conflicted     1.2.0      2023-02-01 [2] RSPM #>  crayon         1.5.2      2022-09-29 [2] RSPM #>  data.table     1.15.2     2024-02-29 [2] RSPM #>  desc           1.4.3      2023-12-10 [2] RSPM #>  dials        * 1.2.1      2024-02-22 [2] RSPM #>  DiceDesign     1.10       2023-12-07 [2] RSPM #>  digest         0.6.35     2024-03-11 [2] RSPM #>  dplyr        * 1.1.4      2023-11-17 [2] RSPM #>  ellipsis       0.3.2      2021-04-29 [2] RSPM #>  evaluate       0.23       2023-11-01 [2] RSPM #>  fansi          1.0.6      2023-12-08 [2] RSPM #>  farver         2.1.1      2022-07-06 [2] RSPM #>  fastmap        1.1.1      2023-02-24 [2] RSPM #>  float          0.3-2      2023-12-10 [2] RSPM #>  foreach        1.5.2      2022-02-02 [2] RSPM #>  fs             1.6.3      2023-07-20 [2] RSPM #>  furrr          0.3.1      2022-08-15 [2] RSPM #>  future         1.33.1     2023-12-22 [2] RSPM #>  future.apply   1.11.1     2023-12-21 [2] RSPM #>  generics       0.1.3      2022-07-05 [2] RSPM #>  ggplot2      * 3.5.0      2024-02-23 [2] RSPM #>  gibasa         1.1.0      2024-03-23 [1] local #>  globals        0.16.3     2024-03-08 [2] RSPM #>  glue           1.7.0      2024-01-09 [2] RSPM #>  gower          1.0.1      2022-12-22 [2] RSPM #>  GPfit          1.0-8      2019-02-08 [2] RSPM #>  gtable         0.3.4      2023-08-21 [2] RSPM #>  hardhat        1.3.1      2024-02-02 [2] RSPM #>  highr          0.10       2022-12-22 [2] RSPM #>  hms            1.1.3      2023-03-21 [2] RSPM #>  htmltools      0.5.7      2023-11-03 [2] RSPM #>  infer        * 1.0.6      2024-01-31 [2] RSPM #>  ipred          0.9-14     2023-03-09 [2] RSPM #>  iterators      1.0.14     2022-02-05 [2] RSPM #>  jquerylib      0.1.4      2021-04-26 [2] RSPM #>  jsonlite       1.8.8      2023-12-04 [2] RSPM #>  knitr          1.45       2023-10-30 [2] RSPM #>  labeling       0.4.3      2023-08-29 [2] RSPM #>  lattice        0.22-5     2023-10-24 [4] CRAN (R 4.3.3) #>  lava           1.8.0      2024-03-05 [2] RSPM #>  ldccr          2024.02.04 2024-03-22 [2] Github (paithiov909/ldccr@0f566b0) #>  lgr            0.4.4      2022-09-05 [2] RSPM #>  lhs            1.1.6      2022-12-17 [2] RSPM #>  lifecycle      1.0.4      2023-11-07 [2] RSPM #>  listenv        0.9.1      2024-01-29 [2] RSPM #>  lubridate      1.9.3      2023-09-27 [2] RSPM #>  magrittr       2.0.3      2022-03-30 [2] RSPM #>  MASS           7.3-60.0.1 2024-01-13 [4] CRAN (R 4.3.3) #>  Matrix         1.6-5      2024-01-11 [4] CRAN (R 4.3.3) #>  memoise        2.0.1      2021-11-26 [2] RSPM #>  mlapi          0.1.1      2022-04-24 [2] RSPM #>  modeldata    * 1.3.0      2024-01-21 [2] RSPM #>  munsell        0.5.0      2018-06-12 [2] RSPM #>  nnet           7.3-19     2023-05-03 [4] CRAN (R 4.3.3) #>  parallelly     1.37.1     2024-02-29 [2] RSPM #>  parsnip      * 1.2.0      2024-02-16 [2] RSPM #>  pillar         1.9.0      2023-03-22 [2] RSPM #>  pkgconfig      2.0.3      2019-09-22 [2] RSPM #>  pkgdown        2.0.7      2022-12-14 [2] any (@2.0.7) #>  prodlim        2023.08.28 2023-08-28 [2] RSPM #>  purrr        * 1.0.2      2023-08-10 [2] RSPM #>  R.cache        0.16.0     2022-07-21 [2] RSPM #>  R.methodsS3    1.8.2      2022-06-13 [2] RSPM #>  R.oo           1.26.0     2024-01-24 [2] RSPM #>  R.utils        2.12.3     2023-11-18 [2] RSPM #>  R6             2.5.1      2021-08-19 [2] RSPM #>  ragg           1.3.0      2024-03-13 [2] RSPM #>  Rcpp           1.0.12     2024-01-09 [2] RSPM #>  RcppParallel   5.1.7      2023-02-27 [2] RSPM #>  readr          2.1.5      2024-01-10 [2] RSPM #>  recipes      * 1.0.10     2024-02-18 [2] RSPM #>  RhpcBLASctl    0.23-42    2023-02-11 [2] RSPM #>  rlang          1.1.3      2024-01-10 [2] RSPM #>  rmarkdown      2.26       2024-03-05 [2] RSPM #>  rpart          4.1.23     2023-12-05 [4] CRAN (R 4.3.3) #>  rsample      * 1.2.0      2023-08-23 [2] RSPM #>  rsparse        0.5.1      2022-09-11 [2] RSPM #>  rstudioapi     0.15.0     2023-07-07 [2] RSPM #>  sass           0.4.9      2024-03-15 [2] RSPM #>  scales       * 1.3.0      2023-11-28 [2] RSPM #>  sessioninfo    1.2.2      2021-12-06 [2] any (@1.2.2) #>  stringi        1.8.3      2023-12-11 [2] RSPM #>  stringr        1.5.1      2023-11-14 [2] RSPM #>  styler         1.10.2     2023-08-29 [2] any (@1.10.2) #>  survival       3.5-8      2024-02-14 [4] CRAN (R 4.3.3) #>  systemfonts    1.0.6      2024-03-07 [2] RSPM #>  text2vec     * 0.6.4      2023-11-09 [2] RSPM #>  textrecipes  * 1.0.6      2023-11-15 [2] RSPM #>  textshaping    0.3.7      2023-10-09 [2] RSPM #>  tibble       * 3.2.1      2023-03-20 [2] RSPM #>  tidymodels   * 1.1.1      2023-08-24 [2] RSPM #>  tidyr        * 1.3.1      2024-01-24 [2] RSPM #>  tidyselect     1.2.1      2024-03-11 [2] RSPM #>  timechange     0.3.0      2024-01-18 [2] RSPM #>  timeDate       4032.109   2023-12-14 [2] RSPM #>  tune         * 1.2.0      2024-03-20 [2] RSPM #>  tzdb           0.4.0      2023-05-12 [2] RSPM #>  utf8           1.2.4      2023-10-22 [2] RSPM #>  vctrs          0.6.5      2023-12-01 [2] RSPM #>  vroom          1.6.5      2023-12-05 [2] RSPM #>  withr          3.0.0      2024-01-16 [2] RSPM #>  workflows    * 1.1.4      2024-02-19 [2] RSPM #>  workflowsets * 1.1.0      2024-03-21 [2] RSPM #>  xfun           0.42       2024-02-08 [2] RSPM #>  xgboost      * 1.7.7.1    2024-01-25 [2] RSPM #>  yaml           2.3.8      2023-12-11 [2] RSPM #>  yardstick    * 1.3.1      2024-03-21 [2] RSPM #>  #>  [1] /tmp/RtmpheSglV/temp_libpath36517838a50d #>  [2] /home/runner/work/_temp/Library #>  [3] /opt/R/4.3.3/lib/R/site-library #>  [4] /opt/R/4.3.3/lib/R/library #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"https://paithiov909.github.io/gibasa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Akiru Kato. Author, maintainer. Shogo Ichinose. Author. Taku Kudo. Author. Jorge Nocedal. Contributor. Nippon Telegraph Telephone Corporation. Copyright holder.","code":""},{"path":"https://paithiov909.github.io/gibasa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kato , Ichinose S, Kudo T (2024). gibasa: Alternative 'Rcpp' Wrapper 'MeCab'. R package version 1.1.0, https://paithiov909.github.io/gibasa/.","code":"@Manual{,   title = {gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab'},   author = {Akiru Kato and Shogo Ichinose and Taku Kudo},   year = {2024},   note = {R package version 1.1.0},   url = {https://paithiov909.github.io/gibasa/}, }"},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"An Alternative Rcpp Wrapper of MeCab","text":"Gibasa plain ‘Rcpp’ wrapper ‘MeCab’, morphological analyzer CJK text. Part--speech tagging morphological analyzers useful processing CJK text data. words CJK text separated whitespaces tokenizers::tokenize_words may split wrong tokens. main goal gibasa package provide alternative tidytext::unnest_tokens CJK text data. goal, gibasa provides three main functions: gibasa::tokenize, gibasa::prettify, gibasa::pack. flowchart text analysis combines gibasa packages gibasa::tokenize takes TIF-compliant data.frame corpus, returning tokens format known ‘tidy text data’, users can replace tidytext::unnest_tokens tokenizing CJK text. gibasa::prettify turns tagged features columns. gibasa::pack takes ‘tidy text data’, typically returning space-separated corpus.","code":""},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An Alternative Rcpp Wrapper of MeCab","text":"can install binary package via CRAN r-universe. use gibasa package requires MeCab library dictionary installed available. case using Linux OSX, can install package managers, build install source . case using Windows, use installer built 32bit built 64bit. Note gibasa requires UTF-8 dictionary, Shift-JIS one. v0.9.4, gibasa looks file specified environment variable MECABRC file located ~/.mecabrc. MeCab dictionary different location default, create mecabrc file specify dictionary located. example, install use ipadic PyPI, run:","code":"## Install gibasa from r-universe repository install.packages(\"gibasa\", repos = c(\"https://paithiov909.r-universe.dev\", \"https://cloud.r-project.org\"))  ## Or build from source package Sys.setenv(MECAB_DEFAULT_RC = \"/fullpath/to/your/mecabrc\") # if necessary remotes::install_github(\"paithiov909/gibasa\") $ python3 -m pip install ipadic $ python3 -c \"import ipadic; print('dicdir=' + ipadic.DICDIR);\" > ~/.mecabrc"},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"tokenize-sentences","dir":"","previous_headings":"Usage","what":"Tokenize sentences","title":"An Alternative Rcpp Wrapper of MeCab","text":"","code":"res <- gibasa::tokenize(   data.frame(     doc_id = seq_along(gibasa::ginga[5:8]),     text = gibasa::ginga[5:8]   ),   text,   doc_id ) res #> # A tibble: 187 × 5 #>    doc_id sentence_id token_id token        feature                              #>    <fct>        <int>    <int> <chr>        <chr>                                #>  1 1                1        1 　           記号,空白,*,*,*,*,　,　,　           #>  2 1                1        2 カムパネルラ 名詞,一般,*,*,*,*,*                  #>  3 1                1        3 が           助詞,格助詞,一般,*,*,*,が,ガ,ガ      #>  4 1                1        4 手           名詞,一般,*,*,*,*,手,テ,テ           #>  5 1                1        5 を           助詞,格助詞,一般,*,*,*,を,ヲ,ヲ      #>  6 1                1        6 あげ         動詞,自立,*,*,一段,連用形,あげる,ア… #>  7 1                1        7 まし         助動詞,*,*,*,特殊・マス,連用形,ます… #>  8 1                1        8 た           助動詞,*,*,*,特殊・タ,基本形,た,タ,… #>  9 1                1        9 。           記号,句点,*,*,*,*,。,。,。           #> 10 1                1       10 それ         名詞,代名詞,一般,*,*,*,それ,ソレ,ソ… #> # ℹ 177 more rows"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"prettify-output","dir":"","previous_headings":"Usage","what":"Prettify output","title":"An Alternative Rcpp Wrapper of MeCab","text":"","code":"gibasa::prettify(res) #> # A tibble: 187 × 13 #>    doc_id sentence_id token_id token        POS1   POS2  POS3  POS4  X5StageUse1 #>    <fct>        <int>    <int> <chr>        <chr>  <chr> <chr> <chr> <chr>       #>  1 1                1        1 　           記号   空白  <NA>  <NA>  <NA>        #>  2 1                1        2 カムパネルラ 名詞   一般  <NA>  <NA>  <NA>        #>  3 1                1        3 が           助詞   格助… 一般  <NA>  <NA>        #>  4 1                1        4 手           名詞   一般  <NA>  <NA>  <NA>        #>  5 1                1        5 を           助詞   格助… 一般  <NA>  <NA>        #>  6 1                1        6 あげ         動詞   自立  <NA>  <NA>  一段        #>  7 1                1        7 まし         助動詞 <NA>  <NA>  <NA>  特殊・マス  #>  8 1                1        8 た           助動詞 <NA>  <NA>  <NA>  特殊・タ    #>  9 1                1        9 。           記号   句点  <NA>  <NA>  <NA>        #> 10 1                1       10 それ         名詞   代名… 一般  <NA>  <NA>        #> # ℹ 177 more rows #> # ℹ 4 more variables: X5StageUse2 <chr>, Original <chr>, Yomi1 <chr>, #> #   Yomi2 <chr> gibasa::prettify(res, col_select = 1:3) #> # A tibble: 187 × 7 #>    doc_id sentence_id token_id token        POS1   POS2   POS3  #>    <fct>        <int>    <int> <chr>        <chr>  <chr>  <chr> #>  1 1                1        1 　           記号   空白   <NA>  #>  2 1                1        2 カムパネルラ 名詞   一般   <NA>  #>  3 1                1        3 が           助詞   格助詞 一般  #>  4 1                1        4 手           名詞   一般   <NA>  #>  5 1                1        5 を           助詞   格助詞 一般  #>  6 1                1        6 あげ         動詞   自立   <NA>  #>  7 1                1        7 まし         助動詞 <NA>   <NA>  #>  8 1                1        8 た           助動詞 <NA>   <NA>  #>  9 1                1        9 。           記号   句点   <NA>  #> 10 1                1       10 それ         名詞   代名詞 一般  #> # ℹ 177 more rows gibasa::prettify(res, col_select = c(1, 3, 5)) #> # A tibble: 187 × 7 #>    doc_id sentence_id token_id token        POS1   POS3  X5StageUse1 #>    <fct>        <int>    <int> <chr>        <chr>  <chr> <chr>       #>  1 1                1        1 　           記号   <NA>  <NA>        #>  2 1                1        2 カムパネルラ 名詞   <NA>  <NA>        #>  3 1                1        3 が           助詞   一般  <NA>        #>  4 1                1        4 手           名詞   <NA>  <NA>        #>  5 1                1        5 を           助詞   一般  <NA>        #>  6 1                1        6 あげ         動詞   <NA>  一段        #>  7 1                1        7 まし         助動詞 <NA>  特殊・マス  #>  8 1                1        8 た           助動詞 <NA>  特殊・タ    #>  9 1                1        9 。           記号   <NA>  <NA>        #> 10 1                1       10 それ         名詞   一般  <NA>        #> # ℹ 177 more rows gibasa::prettify(res, col_select = c(\"POS1\", \"Original\")) #> # A tibble: 187 × 6 #>    doc_id sentence_id token_id token        POS1   Original #>    <fct>        <int>    <int> <chr>        <chr>  <chr>    #>  1 1                1        1 　           記号   　       #>  2 1                1        2 カムパネルラ 名詞   <NA>     #>  3 1                1        3 が           助詞   が       #>  4 1                1        4 手           名詞   手       #>  5 1                1        5 を           助詞   を       #>  6 1                1        6 あげ         動詞   あげる   #>  7 1                1        7 まし         助動詞 ます     #>  8 1                1        8 た           助動詞 た       #>  9 1                1        9 。           記号   。       #> 10 1                1       10 それ         名詞   それ     #> # ℹ 177 more rows"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"pack-output","dir":"","previous_headings":"Usage","what":"Pack output","title":"An Alternative Rcpp Wrapper of MeCab","text":"","code":"res <- gibasa::prettify(res) gibasa::pack(res) #> # A tibble: 4 × 2 #>   doc_id text                                                                    #>   <fct>  <chr>                                                                   #> 1 1      　 カムパネルラ が 手 を あげ まし た 。 それ から 四 、 五 人 手 を …  #> 2 2      　 ところが 先生 は 早く も それ を 見つけ た の でし た 。             #> 3 3      「 ジョバンニ さん 。 あなた は わかっ て いる の でしょ う 」          #> 4 4      　 ジョバンニ は 勢い よく 立ちあがり まし た が 、 立っ て みる と も…  dplyr::mutate(   res,   token = dplyr::if_else(is.na(Original), token, Original),   token = paste(token, POS1, sep = \"/\") ) |>   gibasa::pack() |>   head(1L) #> # A tibble: 1 × 2 #>   doc_id text                                                                   #>   <fct>  <chr>                                                                  #> 1 1      　/記号 カムパネルラ/名詞 が/助詞 手/名詞 を/助詞 あげる/動詞 ます/助…"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"change-dictionary","dir":"","previous_headings":"Usage","what":"Change dictionary","title":"An Alternative Rcpp Wrapper of MeCab","text":"IPA, UniDic, CC-CEDICT-MeCab, mecab-ko-dic schemes supported.","code":"## UniDic 2.1.2 gibasa::tokenize(\"あのイーハトーヴォのすきとおった風\", sys_dic = file.path(\"mecab/unidic-lite\")) |>   gibasa::prettify(into = gibasa::get_dict_features(\"unidic26\")) #> # A tibble: 6 × 30 #>   doc_id sentence_id token_id token   POS1   POS2  POS3  POS4  cType cForm lForm #>   <fct>        <int>    <int> <chr>   <chr>  <chr> <chr> <chr> <chr> <chr> <chr> #> 1 1                1        1 あの    感動詞 フィ… <NA>  <NA>  <NA>  <NA>  アノ  #> 2 1                1        2 イーハ… 名詞   普通… 一般  <NA>  <NA>  <NA>  <NA>  #> 3 1                1        3 の      助詞   格助… <NA>  <NA>  <NA>  <NA>  ノ    #> 4 1                1        4 すきと… 動詞   一般  <NA>  <NA>  五段… 連用… スキ… #> 5 1                1        5 た      助動詞 <NA>  <NA>  <NA>  助動… 連体… タ    #> 6 1                1        6 風      名詞   普通… 一般  <NA>  <NA>  <NA>  カゼ  #> # ℹ 19 more variables: lemma <chr>, orth <chr>, pron <chr>, orthBase <chr>, #> #   pronBase <chr>, goshu <chr>, iType <chr>, iForm <chr>, fType <chr>, #> #   fForm <chr>, kana <chr>, kanaBase <chr>, form <chr>, formBase <chr>, #> #   iConType <chr>, fConType <chr>, aType <chr>, aConType <chr>, #> #   aModeType <chr>   ## CC-CEDICT gibasa::tokenize(\"它可以进行日语和汉语的语态分析\", sys_dic = file.path(\"mecab/cc-cedict\")) |>   gibasa::prettify(into = gibasa::get_dict_features(\"cc-cedict\")) #> # A tibble: 9 × 12 #>   doc_id sentence_id token_id token POS1  POS2  POS3  POS4  pinyin_pron #>   <fct>        <int>    <int> <chr> <chr> <chr> <chr> <chr> <chr>       #> 1 1                1        1 它    <NA>  <NA>  <NA>  <NA>  ta1         #> 2 1                1        2 可以  <NA>  <NA>  <NA>  <NA>  ke3 yi3     #> 3 1                1        3 进行  <NA>  <NA>  <NA>  <NA>  jin4 xing2  #> 4 1                1        4 日语  <NA>  <NA>  <NA>  <NA>  Ri4 yu3     #> 5 1                1        5 和    <NA>  <NA>  <NA>  <NA>  he2         #> 6 1                1        6 汉语  <NA>  <NA>  <NA>  <NA>  Han4 yu3    #> 7 1                1        7 的    <NA>  <NA>  <NA>  <NA>  di4         #> 8 1                1        8 语态  <NA>  <NA>  <NA>  <NA>  yu3 tai4    #> 9 1                1        9 分析  <NA>  <NA>  <NA>  <NA>  fen1 xi1    #> # ℹ 3 more variables: traditional_char_form <chr>, simplified_char_form <chr>, #> #   definition <chr>   ## mecab-ko-dic gibasa::tokenize(\"하네다공항한정토트백\", sys_dic = file.path(\"mecab/mecab-ko-dic\")) |>   gibasa::prettify(into = gibasa::get_dict_features(\"ko-dic\")) #> # A tibble: 4 × 12 #>   doc_id sentence_id token_id token  POS   meaning presence reading type     #>   <fct>        <int>    <int> <chr>  <chr> <chr>   <chr>    <chr>   <chr>    #> 1 1                1        1 하네다 NNP   인명    F        하네다  <NA>     #> 2 1                1        2 공항   NNG   장소    T        공항    <NA>     #> 3 1                1        3 한정   NNG   <NA>    T        한정    <NA>     #> 4 1                1        4 토트백 NNG   <NA>    T        토트백  Compound #> # ℹ 3 more variables: first_pos <chr>, last_pos <chr>, expression <chr>"},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"build-a-system-dictionary","dir":"","previous_headings":"Build dictionaries","what":"Build a system dictionary","title":"An Alternative Rcpp Wrapper of MeCab","text":"","code":"## build a new ipadic in temporary directory build_sys_dic(   dic_dir = file.path(\"mecab/ipadic-eucjp\"), # replace here with path to your source dictionary   out_dir = tempdir(),   encoding = \"euc-jp\" # encoding of source csv files ) #> reading mecab/ipadic-eucjp/unk.def ... 40 #> reading mecab/ipadic-eucjp/Adverb.csv ... 3032 #> reading mecab/ipadic-eucjp/Conjunction.csv ... 171 #> reading mecab/ipadic-eucjp/Suffix.csv ... 1393 #> reading mecab/ipadic-eucjp/Noun.adverbal.csv ... 795 #> reading mecab/ipadic-eucjp/Noun.others.csv ... 151 #> reading mecab/ipadic-eucjp/Noun.org.csv ... 16668 #> reading mecab/ipadic-eucjp/Verb.csv ... 130750 #> reading mecab/ipadic-eucjp/Noun.place.csv ... 72999 #> reading mecab/ipadic-eucjp/Noun.csv ... 60477 #> reading mecab/ipadic-eucjp/Adnominal.csv ... 135 #> reading mecab/ipadic-eucjp/Noun.number.csv ... 42 #> reading mecab/ipadic-eucjp/Noun.verbal.csv ... 12146 #> reading mecab/ipadic-eucjp/Filler.csv ... 19 #> reading mecab/ipadic-eucjp/Others.csv ... 2 #> reading mecab/ipadic-eucjp/Noun.adjv.csv ... 3328 #> reading mecab/ipadic-eucjp/Interjection.csv ... 252 #> reading mecab/ipadic-eucjp/Postp-col.csv ... 91 #> reading mecab/ipadic-eucjp/Noun.nai.csv ... 42 #> reading mecab/ipadic-eucjp/Prefix.csv ... 221 #> reading mecab/ipadic-eucjp/Noun.name.csv ... 34202 #> reading mecab/ipadic-eucjp/Symbol.csv ... 208 #> reading mecab/ipadic-eucjp/Adj.csv ... 27210 #> reading mecab/ipadic-eucjp/Noun.demonst.csv ... 120 #> reading mecab/ipadic-eucjp/Noun.proper.csv ... 27327 #> reading mecab/ipadic-eucjp/Postp.csv ... 146 #> reading mecab/ipadic-eucjp/Auxil.csv ... 199 #> reading mecab/ipadic-eucjp/matrix.def ... 1316x1316 #>  #> done!  ## copy the 'dicrc' file file.copy(file.path(\"mecab/ipadic-eucjp/dicrc\"), tempdir()) #> [1] TRUE  dictionary_info(sys_dic = tempdir()) #>                 file_path charset lsize rsize   size type version #> 1 /tmp/RtmpqB7E2w/sys.dic    utf8  1316  1316 392126    0     102"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"build-a-user-dictionary","dir":"","previous_headings":"Build dictionaries","what":"Build a user dictionary","title":"An Alternative Rcpp Wrapper of MeCab","text":"","code":"## write a csv file and compile it into a user dictionary writeLines(   c(     \"月ノ,1290,1290,4579,名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ\",     \"美兎,1291,1291,8561,名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト\"   ),   con = (csv_file <- tempfile(fileext = \".csv\")) ) build_user_dic(   dic_dir = file.path(\"mecab/ipadic-eucjp\"),   file = (user_dic <- tempfile(fileext = \".dic\")),   csv_file = csv_file,   encoding = \"utf8\" ) #> reading /tmp/RtmpqB7E2w/file7f01ffa2fde.csv ... 2 #>  #> done!  tokenize(\"月ノ美兎は箱の中\", sys_dic = tempdir(), user_dic = user_dic) #> # A tibble: 6 × 5 #>   doc_id sentence_id token_id token feature                                      #>   <fct>        <int>    <int> <chr> <chr>                                        #> 1 1                1        1 月ノ  名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ #> 2 1                1        2 美兎  名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト     #> 3 1                1        3 は    助詞,係助詞,*,*,*,*,は,ハ,ワ                 #> 4 1                1        4 箱    名詞,一般,*,*,*,*,箱,ハコ,ハコ               #> 5 1                1        5 の    助詞,連体化,*,*,*,*,の,ノ,ノ                 #> 6 1                1        6 中    名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ"},{"path":"https://paithiov909.github.io/gibasa/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"An Alternative Rcpp Wrapper of MeCab","text":"GPL (>=3).","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/as_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a list of tokens — as_tokens","title":"Create a list of tokens — as_tokens","text":"Create list tokens","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/as_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a list of tokens — as_tokens","text":"","code":"as_tokens(   tbl,   token_field = \"token\",   pos_field = get_dict_features()[1],   nm = NULL )"},{"path":"https://paithiov909.github.io/gibasa/reference/as_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a list of tokens — as_tokens","text":"tbl tibble tokens tokenize(). token_field <data-masked> Column containing tokens. pos_field Column containing features kept names tokens. tokens. need , give NULL argument. nm Names returned list. left NULL, \"doc_id\" field tbl used instead.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/as_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a list of tokens — as_tokens","text":"named list tokens.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/as_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a list of tokens — as_tokens","text":"","code":"if (FALSE) { tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) |>   prettify(col_select = \"POS1\") |>   as_tokens() }"},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind importance of bigrams — bind_lr","title":"Bind importance of bigrams — bind_lr","text":"Calculates binds importance bigrams synergistic average.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind importance of bigrams — bind_lr","text":"","code":"bind_lr(tbl, term = \"token\", lr_mode = c(\"n\", \"dn\"), avg_rate = 1)"},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind importance of bigrams — bind_lr","text":"tbl tidy text dataset. term <data-masked> Column containing terms. lr_mode Method computing 'FL' 'FR' values. n equivalent 'LN' 'RN', dn equivalent 'LDN' 'RDN'. avg_rate Weight 'LR' value.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind importance of bigrams — bind_lr","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bind importance of bigrams — bind_lr","text":"'LR' value synergistic average bigram importance based words positions (left right side).","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/bind_lr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bind importance of bigrams — bind_lr","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) bind_lr(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind term frequency and inverse document frequency — bind_tf_idf2","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"Calculates binds term frequency, inverse document frequency, TF-IDF dataset. function experimentally supports 4 types term frequencies 5 types inverse document frequencies.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"bind_tf_idf2(   tbl,   term = \"token\",   document = \"doc_id\",   n = \"n\",   tf = c(\"tf\", \"tf2\", \"tf3\", \"itf\"),   idf = c(\"idf\", \"idf2\", \"idf3\", \"idf4\", \"df\"),   norm = FALSE,   rmecab_compat = TRUE )"},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"tbl tidy text dataset. term <data-masked> Column containing terms. document <data-masked> Column containing document IDs. n <data-masked> Column containing document-term counts. tf Method computing term frequency. idf Method computing inverse document frequency. norm Logical; passed TRUE, TF-IDF values normalized divided L2 norms. rmecab_compat Logical; passed TRUE, computes values taking care compatibility 'RMeCab'. Note 'RMeCab' always computes IDF values using term frequency rather raw term counts, thus TF-IDF values may doubly affected term frequency.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"Types term frequency can switched tf argument: tf term frequency (raw count terms). tf2 logarithmic term frequency base exp(1). tf3 binary-weighted term frequency. itf inverse term frequency. Use idf=\"df\". Types inverse document frequencies can switched idf argument: idf inverse document frequency base 2, smoothed. 'smoothed' means just adding 1 raw values logarithmizing. idf2 global frequency IDF. idf3 probabilistic IDF base 2. idf4 global entropy, IDF actual. df document frequency. Use tf=\"itf\".","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/bind_tf_idf2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bind term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) |>   dplyr::group_by(doc_id) |>   dplyr::count(token) |>   dplyr::ungroup() bind_tf_idf2(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":null,"dir":"Reference","previous_headings":"","what":"Build system dictionary — build_sys_dic","title":"Build system dictionary — build_sys_dic","text":"Builds UTF-8 system dictionary source dictionary files.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build system dictionary — build_sys_dic","text":"","code":"build_sys_dic(dic_dir, out_dir, encoding)"},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build system dictionary — build_sys_dic","text":"dic_dir Directory source dictionaries located. argument passed '-d' option argument. out_dir Directory binary dictionary written. argument passed '-o' option argument. encoding Encoding input csv files. argument passed '-f' option argument.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build system dictionary — build_sys_dic","text":"TRUE invisibly returned dictionary successfully built.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build system dictionary — build_sys_dic","text":"function wrapper around dictionary compiler 'MeCab'. Running function create 4 files: 'char.bin', 'matrix.bin', 'sys.dic', 'unk.dic' out_dir. use compiled dictionary, also need create dicrc file out_dir. dicrc file included source dictionaries, can just copy out_dir.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_sys_dic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build system dictionary — build_sys_dic","text":"","code":"# \\donttest{ if (requireNamespace(\"withr\")) {   # create a sample dictionary in temporary directory   build_sys_dic(     dic_dir = system.file(\"latin\", package = \"gibasa\"),     out_dir = tempdir(),     encoding = \"utf8\"   )   # copy the 'dicrc' file   file.copy(     system.file(\"latin/dicrc\", package = \"gibasa\"),     tempdir()   )   # mocking a 'mecabrc' file to temporarily use the dictionary   withr::with_envvar(     c(       \"MECABRC\" = if (.Platform$OS.type == \"windows\") {         \"nul\"       } else {         \"/dev/null\"       },       \"RCPP_PARALLEL_BACKEND\" = \"tinythread\"     ),     {       tokenize(\"katta-wokattauresikatta\", sys_dic = tempdir())     }   ) } #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/unk.def ... 2 #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/dic.csv ... 450 #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/matrix.def ... 1x1 #>  #> done! #> # A tibble: 11 × 5 #>    doc_id sentence_id token_id token feature #>    <fct>        <int>    <int> <chr> <chr>   #>  1 1                1        1 ka    か      #>  2 1                1        2 tta   った    #>  3 1                1        3 -     ー      #>  4 1                1        4 wo    を      #>  5 1                1        5 ka    か      #>  6 1                1        6 tta   った    #>  7 1                1        7 u     う      #>  8 1                1        8 re    れ      #>  9 1                1        9 si    し      #> 10 1                1       10 ka    か      #> 11 1                1       11 tta   った    # }"},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":null,"dir":"Reference","previous_headings":"","what":"Build user dictionary — build_user_dic","title":"Build user dictionary — build_user_dic","text":"Builds UTF-8 user dictionary csv file.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build user dictionary — build_user_dic","text":"","code":"build_user_dic(dic_dir, file, csv_file, encoding)"},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build user dictionary — build_user_dic","text":"dic_dir Directory source dictionaries located. argument passed '-d' option argument. file Path write user dictionary. argument passed '-u' option argument. csv_file Path input csv file. encoding Encoding input csv files. argument passed '-f' option argument.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build user dictionary — build_user_dic","text":"TRUE invisibly returned dictionary successfully built.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build user dictionary — build_user_dic","text":"function wrapper around dictionary compiler 'MeCab'. Note function support auto assignment word cost field. , leave word costs empty input csv file. estimate word costs, use posDebugRcpp() function.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/build_user_dic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build user dictionary — build_user_dic","text":"","code":"# \\donttest{ if (requireNamespace(\"withr\")) {   # create a sample dictionary in temporary directory   build_sys_dic(     dic_dir = system.file(\"latin\", package = \"gibasa\"),     out_dir = tempdir(),     encoding = \"utf8\"   )   # copy the 'dicrc' file   file.copy(     system.file(\"latin/dicrc\", package = \"gibasa\"),     tempdir()   )   # write a csv file and compile it into a user dictionary   csv_file <- tempfile(fileext = \".csv\")   writeLines(     c(       \"qa, 0, 0, 5, \\u304f\\u3041\",       \"qi, 0, 0, 5, \\u304f\\u3043\",       \"qu, 0, 0, 5, \\u304f\",       \"qe, 0, 0, 5, \\u304f\\u3047\",       \"qo, 0, 0, 5, \\u304f\\u3049\"     ),     csv_file   )   build_user_dic(     dic_dir = tempdir(),     file = (user_dic <- tempfile(fileext = \".dic\")),     csv_file = csv_file,     encoding = \"utf8\"   )   # mocking a 'mecabrc' file to temporarily use the dictionary   withr::with_envvar(     c(       \"MECABRC\" = if (.Platform$OS.type == \"windows\") {         \"nul\"       } else {         \"/dev/null\"       },       \"RCPP_PARALLEL_BACKEND\" = \"tinythread\"     ),     {       tokenize(\"quensan\", sys_dic = tempdir(), user_dic = user_dic)     }   ) } #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/unk.def ... 2 #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/dic.csv ... 450 #> reading /tmp/RtmpheSglV/temp_libpath36517838a50d/gibasa/latin/matrix.def ... 1x1 #>  #> done! #> reading /tmp/RtmpheSglV/file36514368ae5a.csv ... 5 #>  #> done! #> # A tibble: 5 × 5 #>   doc_id sentence_id token_id token feature #>   <fct>        <int>    <int> <chr> <chr>   #> 1 1                1        1 qu    く      #> 2 1                1        2 e     え      #> 3 1                1        3 n     ん      #> 4 1                1        4 sa    さ      #> 5 1                1        5 n     ん      # }"},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Collapse sequences of tokens by condition — collapse_tokens","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"Concatenates sequences tokens tidy text dataset, grouping expression.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"","code":"collapse_tokens(tbl, condition, .collapse = \"\")"},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"tbl tidy text dataset. condition <data-masked> logical expression. .collapse String tokens concatenated.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"Note function drops columns except 'token' columns grouping sequences. , returned data.frame 'doc_id', 'sentence_id', 'token_id', 'token' columns.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/collapse_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = \"odakyu-sen\",     text = \"\\u5c0f\\u7530\\u6025\\u7dda\"   ) ) |>   prettify(col_select = \"POS1\")  head(collapse_tokens(   df,   POS1 == \"\\u540d\\u8a5e\" & stringr::str_detect(token, \"^[\\\\p{Han}]+$\") )) }"},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_sys.html","id":null,"dir":"Reference","previous_headings":"","what":"Build system dictionary — dict_index_sys","title":"Build system dictionary — dict_index_sys","text":"Build system dictionary","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_sys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build system dictionary — dict_index_sys","text":"dic_dir Directory source dictionaries located. argument passed '-d' option argument. out_dir Directory binary dictionary written. argument passed '-o' option argument. encoding Encoding input csv files. argument passed '-f' option argument.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_sys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build system dictionary — dict_index_sys","text":"Logical.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Build user dictionary — dict_index_user","title":"Build user dictionary — dict_index_user","text":"Build user dictionary","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build user dictionary — dict_index_user","text":"dic_dir Directory source dictionaries located. argument passed '-d' option argument. file Path write user dictionary. argument passed '-u' option argument. csv_file Path input csv file. encoding Encoding input csv files. argument passed '-f' option argument.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dict_index_user.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build user dictionary — dict_index_user","text":"Logical.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dictionary_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dictionary information — dictionary_info","title":"Get dictionary information — dictionary_info","text":"Get dictionary information","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dictionary_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dictionary information — dictionary_info","text":"sys_dic String scalar. user_dic String scalar.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/dictionary_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dictionary information — dictionary_info","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize sentences using 'MeCab' — gbs_tokenize","title":"Tokenize sentences using 'MeCab' — gbs_tokenize","text":"Tokenize sentences using 'MeCab'","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize sentences using 'MeCab' — gbs_tokenize","text":"","code":"gbs_tokenize(   x,   sys_dic = \"\",   user_dic = \"\",   split = FALSE,   partial = FALSE,   mode = c(\"parse\", \"wakati\") )"},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize sentences using 'MeCab' — gbs_tokenize","text":"x data.frame like object character vector tokenized. sys_dic Character scalar; path system dictionary 'MeCab'. Note system dictionary expected compiled UTF-8, Shift-JIS encodings. user_dic Character scalar; path user dictionary 'MeCab'. split Logical. passed TRUE, function internally splits sentences sub-sentences using stringi::stri_split_boundaries(type = \"sentence\"). partial Logical. passed TRUE, activates partial parsing mode. activate feature, remember spaces start end input chunks already squashed. particular, trailing spaces chunks sometimes cause fatal errors. mode Character scalar switch output format.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gbs_tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize sentences using 'MeCab' — gbs_tokenize","text":"tibble named list tokens.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dictionary features — get_dict_features","title":"Get dictionary features — get_dict_features","text":"Returns names dictionary features. Currently supports \"unidic17\" (2.1.2 src schema), \"unidic26\" (2.1.2 bin schema), \"unidic29\" (schema used 2.2.0, 2.3.0), \"cc-cedict\", \"ko-dic\" (mecab-ko-dic), \"naist11\", \"sudachi\", \"ipa\".","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dictionary features — get_dict_features","text":"","code":"get_dict_features(   dict = c(\"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\",     \"sudachi\") )"},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dictionary features — get_dict_features","text":"dict Character scalar; one \"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\", \"sudachi\".","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dictionary features — get_dict_features","text":"character vector.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/get_dict_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get dictionary features — get_dict_features","text":"","code":"get_dict_features(\"ipa\") #> [1] \"POS1\"        \"POS2\"        \"POS3\"        \"POS4\"        \"X5StageUse1\" #> [6] \"X5StageUse2\" \"Original\"    \"Yomi1\"       \"Yomi2\""},{"path":"https://paithiov909.github.io/gibasa/reference/get_transition_cost.html","id":null,"dir":"Reference","previous_headings":"","what":"Get transition cost between pos attributes — get_transition_cost","title":"Get transition cost between pos attributes — get_transition_cost","text":"Get transition cost pos attributes","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_transition_cost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get transition cost between pos attributes — get_transition_cost","text":"","code":"get_transition_cost(rcAttr, lcAttr, sys_dic = \"\", user_dic = \"\")"},{"path":"https://paithiov909.github.io/gibasa/reference/get_transition_cost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get transition cost between pos attributes — get_transition_cost","text":"rcAttr Integer. lcAttr Integer. sys_dic String. user_dic String.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/get_transition_cost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get transition cost between pos attributes — get_transition_cost","text":"Numeric.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/gibasa-package.html","id":null,"dir":"Reference","previous_headings":"","what":"gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab' — gibasa-package","title":"gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab' — gibasa-package","text":"plain 'Rcpp' wrapper 'MeCab' can segment Chinese, Japanese, Korean text tokens. main goal package provide alternative 'tidytext' using morphological analysis.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/gibasa-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"gibasa: An Alternative 'Rcpp' Wrapper of 'MeCab' — gibasa-package","text":"Maintainer: Akiru Kato paithiov909@gmail.com Authors: Shogo Ichinose Taku Kudo contributors: Jorge Nocedal [contributor] Nippon Telegraph Telephone Corporation [copyright holder]","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":null,"dir":"Reference","previous_headings":"","what":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"Whole text 'Ginga Tetsudo Yoru' written Miyazawa Kenji Aozora Bunko","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"","code":"ginga"},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"object class character length 553.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"https://www.aozora.gr.jp/cards/000081/files/43737_ruby_19028.zip","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"dataset containing text Miyazawa Kenji's novel \"Ginga Tetsudo Yoru\" (English title: \"Night Galactic Railroad\") published 1934, year Kenji's death. Copyright work expired since 70 years passed author's death. UTF-8 plain text sourced https://www.aozora.gr.jp/cards/000081/card43737.html cleaned meta data.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ginga.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Whole text of 'Ginga Tetsudo no Yoru' written by Miyazawa Kenji\nfrom Aozora Bunko — ginga","text":"","code":"head(ginga) #> [1] \"銀河鉄道の夜\"                                                                                                                                                                                                                                                                                                                       #> [2] \"宮沢賢治\"                                                                                                                                                                                                                                                                                                                           #> [3] \"一　午後の授業\"                                                                                                                                                                                                                                                                                                                     #> [4] \"「ではみなさんは、そういうふうに川だと言われたり、乳の流れたあとだと言われたりしていた、このぼんやりと白いものがほんとうは何かご承知ですか」先生は、黒板につるした大きな黒い星座の図の、上から下へ白くけぶった銀河帯のようなところを指しながら、みんなに問いをかけました。\"                                                         #> [5] \"　カムパネルラが手をあげました。それから四、五人手をあげました。ジョバンニも手をあげようとして、急いでそのままやめました。たしかにあれがみんな星だと、いつか雑誌で読んだのでしたが、このごろはジョバンニはまるで毎日教室でもねむく、本を読むひまも読む本もないので、なんだかどんなこともよくわからないという気持ちがするのでした。\" #> [6] \"　ところが先生は早くもそれを見つけたのでした。\""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if scalars are blank — is_blank","title":"Check if scalars are blank — is_blank","text":"Check scalars blank","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if scalars are blank — is_blank","text":"","code":"is_blank(x, trim = TRUE, ...)"},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if scalars are blank — is_blank","text":"x Object check emptiness. trim Logical. ... Additional arguments base::sapply().","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if scalars are blank — is_blank","text":"Logicals.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/is_blank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if scalars are blank — is_blank","text":"","code":"is_blank(list(c(a = \"\", b = NA_character_), NULL)) #> [[1]] #> [1] TRUE TRUE #>  #> [[2]] #> [1] TRUE #>"},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate lexical density — lex_density","title":"Calculate lexical density — lex_density","text":"lexical density proportion content words (lexical items) documents. function simple helper calculating lexical density given datasets.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate lexical density — lex_density","text":"","code":"lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))"},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate lexical density — lex_density","text":"vec character vector. contents_words character vector containing values counted contents words. targets character vector denominator lexical density filtered computing values. negate logical vector length 2. passed TRUE, respectively negates predicate functions counting contents words targets.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate lexical density — lex_density","text":"numeric vector.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/lex_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate lexical density — lex_density","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) df |>   prettify(col_select = \"POS1\") |>   dplyr::group_by(doc_id) |>   dplyr::summarise(     noun_ratio = lex_density(POS1,       \"\\u540d\\u8a5e\",       c(\"\\u52a9\\u8a5e\", \"\\u52a9\\u52d5\\u8a5e\"),       negate = c(FALSE, TRUE)     ),     mvr = lex_density(       POS1,       c(\"\\u5f62\\u5bb9\\u8a5e\", \"\\u526f\\u8a5e\", \"\\u9023\\u4f53\\u8a5e\"),       \"\\u52d5\\u8a5e\"     ),     vnr = lex_density(POS1, \"\\u52d5\\u8a5e\", \"\\u540d\\u8a5e\")   ) }"},{"path":"https://paithiov909.github.io/gibasa/reference/mute_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Mute tokens by condition — mute_tokens","title":"Mute tokens by condition — mute_tokens","text":"Replaces tokens tidy text dataset string scalar matched expression.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/mute_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mute tokens by condition — mute_tokens","text":"","code":"mute_tokens(tbl, condition, .as = NA_character_)"},{"path":"https://paithiov909.github.io/gibasa/reference/mute_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mute tokens by condition — mute_tokens","text":"tbl tidy text dataset. condition <data-masked> logical expression. .String tokens replaced matched condition. default value NA_character.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/mute_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mute tokens by condition — mute_tokens","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/mute_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mute tokens by condition — mute_tokens","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) |>   prettify(col_select = \"POS1\")  head(mute_tokens(df, POS1 %in% c(\"\\u52a9\\u8a5e\", \"\\u52a9\\u52d5\\u8a5e\"))) }"},{"path":"https://paithiov909.github.io/gibasa/reference/ngram_tokenizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Ngrams tokenizer — ngram_tokenizer","title":"Ngrams tokenizer — ngram_tokenizer","text":"Makes ngram tokenizer function.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ngram_tokenizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ngrams tokenizer — ngram_tokenizer","text":"","code":"ngram_tokenizer(n = 1L)"},{"path":"https://paithiov909.github.io/gibasa/reference/ngram_tokenizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ngrams tokenizer — ngram_tokenizer","text":"n Integer.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ngram_tokenizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ngrams tokenizer — ngram_tokenizer","text":"ngram tokenizer function","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/ngram_tokenizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ngrams tokenizer — ngram_tokenizer","text":"","code":"bigram <- ngram_tokenizer(2) bigram(letters, sep = \"-\") #>  [1] \"a-b\" \"b-c\" \"c-d\" \"d-e\" \"e-f\" \"f-g\" \"g-h\" \"h-i\" \"i-j\" \"j-k\" \"k-l\" \"l-m\" #> [13] \"m-n\" \"n-o\" \"o-p\" \"p-q\" \"q-r\" \"r-s\" \"s-t\" \"t-u\" \"u-v\" \"v-w\" \"w-x\" \"x-y\" #> [25] \"y-z\""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":null,"dir":"Reference","previous_headings":"","what":"Pack a data.frame of tokens — pack","title":"Pack a data.frame of tokens — pack","text":"Packs data.frame tokens new data.frame corpus, compatible Text Interchange Formats.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pack a data.frame of tokens — pack","text":"","code":"pack(tbl, pull = \"token\", n = 1L, sep = \"-\", .collapse = \" \")"},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pack a data.frame of tokens — pack","text":"tbl data.frame tokens. pull <data-masked> Column packed text ngrams body. Default value token. n Integer internally passed ngrams tokenizer function created gibasa::ngram_tokenizer() sep Character scalar internally used concatenator ngrams. .collapse argument passed stringi::stri_c().","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pack a data.frame of tokens — pack","text":"tibble.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"text-interchange-formats-tif-","dir":"Reference","previous_headings":"","what":"Text Interchange Formats (TIF)","title":"Pack a data.frame of tokens — pack","text":"Text Interchange Formats (TIF) set standards allows R text analysis packages target defined inputs outputs corpora, tokens, document-term matrices.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"valid-data-frame-of-tokens","dir":"Reference","previous_headings":"","what":"Valid data.frame of tokens","title":"Pack a data.frame of tokens — pack","text":"data.frame tokens data.frame object compatible TIF. TIF valid data.frame tokens expected one unique key column (named doc_id) text several feature columns tokens. feature columns must contain least token .","code":""},{"path":[]},{"path":"https://paithiov909.github.io/gibasa/reference/pack.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pack a data.frame of tokens — pack","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) pack(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/posDebugRcpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenizer for debug use — posDebugRcpp","title":"Tokenizer for debug use — posDebugRcpp","text":"Tokenizes character vector returns possible results tokenization process. returned data.frame contains additional attributes debug usage.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posDebugRcpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenizer for debug use — posDebugRcpp","text":"text String. sys_dic String. user_dic String. partial Logical.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posDebugRcpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenizer for debug use — posDebugRcpp","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Call tagger inside 'RcppParallel::parallelFor' and return a data.frame. — posParallelRcpp","title":"Call tagger inside 'RcppParallel::parallelFor' and return a data.frame. — posParallelRcpp","text":"Call tagger inside 'RcppParallel::parallelFor' return data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call tagger inside 'RcppParallel::parallelFor' and return a data.frame. — posParallelRcpp","text":"text Character vector. sys_dic String scalar. user_dic String scalar. partial Logical. grain_size Integer (larger 1).","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/posParallelRcpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call tagger inside 'RcppParallel::parallelFor' and return a data.frame. — posParallelRcpp","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":null,"dir":"Reference","previous_headings":"","what":"Prettify tokenized output — prettify","title":"Prettify tokenized output — prettify","text":"Turns single character column features separating delimiter.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prettify tokenized output — prettify","text":"","code":"prettify(   tbl,   col = \"feature\",   into = get_dict_features(\"ipa\"),   col_select = seq_along(into),   delim = \",\" )"},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prettify tokenized output — prettify","text":"tbl data.frame feature column prettified. col <data-masked> Column containing features prettified. Character vector used column names features. col_select Character integer vector kept prettified features. delim Character scalar used separate fields within feature.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prettify tokenized output — prettify","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/prettify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prettify tokenized output — prettify","text":"","code":"prettify(   data.frame(x = c(\"x,y\", \"y,z\", \"z,x\")),   col = \"x\",   into = c(\"a\", \"b\"),   col_select = \"b\" ) #>   b #> 1 y #> 2 z #> 3 x  if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) prettify(df, col_select = 1:3) prettify(df, col_select = c(1, 3, 6)) prettify(df, col_select = c(\"POS1\", \"Original\")) }"},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize sentences using 'MeCab' — tokenize","title":"Tokenize sentences using 'MeCab' — tokenize","text":"Tokenize sentences using 'MeCab'","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize sentences using 'MeCab' — tokenize","text":"","code":"tokenize(   x,   text_field = \"text\",   docid_field = \"doc_id\",   sys_dic = \"\",   user_dic = \"\",   split = FALSE,   partial = FALSE,   grain_size = 1L,   mode = c(\"parse\", \"wakati\") )"},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize sentences using 'MeCab' — tokenize","text":"x data.frame like object character vector tokenized. text_field <data-masked> String symbol; column containing texts tokenized. docid_field <data-masked> String symbol; column containing document IDs. sys_dic Character scalar; path system dictionary 'MeCab'. Note system dictionary expected compiled UTF-8, Shift-JIS encodings. user_dic Character scalar; path user dictionary 'MeCab'. split Logical. passed TRUE, function internally splits sentences sub-sentences using stringi::stri_split_boundaries(type = \"sentence\"). partial Logical. passed TRUE, activates partial parsing mode. activate feature, remember spaces start end input chunks already squashed. particular, trailing spaces chunks sometimes cause fatal errors. grain_size Integer value larger 1. argument internally passed RcppParallel::parallelFor function. Setting larger chunk size improve performance cases. mode Character scalar switch output format.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize sentences using 'MeCab' — tokenize","text":"tibble named list tokens.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize sentences using 'MeCab' — tokenize","text":"","code":"if (FALSE) { df <- tokenize(   data.frame(     doc_id = seq_along(ginga[5:8]),     text = ginga[5:8]   ) ) head(df) }"},{"path":"https://paithiov909.github.io/gibasa/reference/transition_cost.html","id":null,"dir":"Reference","previous_headings":"","what":"Get transition cost between pos attributes — transition_cost","title":"Get transition cost between pos attributes — transition_cost","text":"Get transition cost pos attributes","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/transition_cost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get transition cost between pos attributes — transition_cost","text":"rcAttr Integer. lcAttr Integer. sys_dic String. user_dic String.","code":""},{"path":"https://paithiov909.github.io/gibasa/reference/transition_cost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get transition cost between pos attributes — transition_cost","text":"Numeric.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-110","dir":"Changelog","previous_headings":"","what":"gibasa 1.1.0","title":"gibasa 1.1.0","text":"CRAN release: 2024-02-17 Corrected probabilistic IDF calculation global_idf3. Breaking Change: Changed behavior norm=TRUE. Cosine nomalization now performed tf_idf values RMeCab package. Added tf=\"itf\" idf=\"df\" options.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-101","dir":"Changelog","previous_headings":"","what":"gibasa 1.0.1","title":"gibasa 1.0.1","text":"CRAN release: 2023-12-02 Added wrappers around dictionary compiler MeCab.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-095","dir":"Changelog","previous_headings":"","what":"gibasa 0.9.5","title":"gibasa 0.9.5","text":"CRAN release: 2023-07-09 Removed audubon dependency maintainability. pack now preserves doc_id type ’s factor.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-094","dir":"Changelog","previous_headings":"","what":"gibasa 0.9.4","title":"gibasa 0.9.4","text":"CRAN release: 2023-06-03 Updated Makevars Unix alikes. Users can now use file specified MECABRC environment variable ~/.mecabrc set dictionaries.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-093","dir":"Changelog","previous_headings":"","what":"gibasa 0.9.3","title":"gibasa 0.9.3","text":"CRAN release: 2023-04-20 Removed unnecessary C++ files.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-092","dir":"Changelog","previous_headings":"","what":"gibasa 0.9.2","title":"gibasa 0.9.2","text":"CRAN release: 2023-04-12 Prepare CRAN release.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-081","dir":"Changelog","previous_headings":"","what":"gibasa 0.8.1","title":"gibasa 0.8.1","text":"performance, tokenize now skips resetting output encodings UTF-8.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-080","dir":"Changelog","previous_headings":"","what":"gibasa 0.8.0","title":"gibasa 0.8.0","text":"Breaking Change: Changed numbering style ‘sentence_id’ split FALSE. Added grain_size argument tokenize. Added new bind_lr function.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-074","dir":"Changelog","previous_headings":"","what":"gibasa 0.7.4","title":"gibasa 0.7.4","text":"Use RcppParallel::parallelFor instead tbb::parallel_for. user’s visible changes.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-071","dir":"Changelog","previous_headings":"","what":"gibasa 0.7.1","title":"gibasa 0.7.1","text":"Fix documentations. visible changes.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-070","dir":"Changelog","previous_headings":"","what":"gibasa 0.7.0","title":"gibasa 0.7.0","text":"tokenize can now accept character vector addition data.frame like object. gbs_tokenize now deprecated. Please use tokenize function instead.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-064","dir":"Changelog","previous_headings":"","what":"gibasa 0.6.4","title":"gibasa 0.6.4","text":"Refactored is_blank.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-063","dir":"Changelog","previous_headings":"","what":"gibasa 0.6.3","title":"gibasa 0.6.3","text":"Added partial argument gbs_tokenize tokenize. argument controls partial parsing mode, forces extract given chunks sentences activated.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-062","dir":"Changelog","previous_headings":"","what":"gibasa 0.6.2","title":"gibasa 0.6.2","text":"friendly errors returned invalid dictionary path provided. Added new posDebugRcpp function.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-061","dir":"Changelog","previous_headings":"","what":"gibasa 0.6.1","title":"gibasa 0.6.1","text":"Revert missing examples.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-060","dir":"Changelog","previous_headings":"","what":"gibasa 0.6.0","title":"gibasa 0.6.0","text":"Functions added version ‘0.5.1’ moved ‘audubon’ package (>= 0.4.0).","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-051","dir":"Changelog","previous_headings":"","what":"gibasa 0.5.1","title":"gibasa 0.5.1","text":"bind_tf_idf2 can calculate bind term frequency, inverse document frequency, tf-idf tidy text dataset. collapse_tokens, mute_tokens, lexical_density can used handling tidy text dataset tokens.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-050","dir":"Changelog","previous_headings":"","what":"gibasa 0.5.0","title":"gibasa 0.5.0","text":"gibasa now includes MeCab source, users need pre-install MeCab library building installing package (use tokenize, still requires MeCab dictionaries installed available).","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-041","dir":"Changelog","previous_headings":"","what":"gibasa 0.4.1","title":"gibasa 0.4.1","text":"tokenize now preserves original order docid_field.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-040","dir":"Changelog","previous_headings":"","what":"gibasa 0.4.0","title":"gibasa 0.4.0","text":"Added bind_tf_idf2 function is_blank function.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-031","dir":"Changelog","previous_headings":"","what":"gibasa 0.3.1","title":"gibasa 0.3.1","text":"Updated dependencies.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-030","dir":"Changelog","previous_headings":"","what":"gibasa 0.3.0","title":"gibasa 0.3.0","text":"Changed build process Windows. Added vignette.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-021","dir":"Changelog","previous_headings":"","what":"gibasa 0.2.1","title":"gibasa 0.2.1","text":"prettify now can extract columns specified col_select.","code":""},{"path":"https://paithiov909.github.io/gibasa/news/index.html","id":"gibasa-020","dir":"Changelog","previous_headings":"","what":"gibasa 0.2.0","title":"gibasa 0.2.0","text":"Added NEWS.md file track changes package. tokenize now takes data.frame first argument, returns data.frame . former function gets character vector returns data.frame named list renamed gbs_tokenize.","code":""}]
