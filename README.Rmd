---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
require(gibasa)
```

# gibasa

<!-- badges: start -->
[![gibasa status badge](https://paithiov909.r-universe.dev/badges/gibasa)](https://paithiov909.r-universe.dev)
![GitHub](https://img.shields.io/github/license/paithiov909/gibasa)
[![R-CMD-check](https://github.com/paithiov909/gibasa/workflows/R-CMD-check/badge.svg)](https://github.com/paithiov909/gibasa/actions)
[![codecov](https://codecov.io/gh/paithiov909/gibasa/branch/main/graph/badge.svg)](https://codecov.io/gh/paithiov909/gibasa)
<!-- badges: end -->

## Overview

Gibasa is a plain 'Rcpp' interface to 'MeCab', CJK tokenizer and morphological analysis tool for R.

The main goal of gibasa package is to provide an alternative to `tidytext::unnest_tokens` for CJK text data.
For analyzing CJK text data, it usually requires part-of-speech tagging,
as most of them are not separated with spaces and `tokenizers::tokenize_words` sometimes splits them into erroneous tokens.

Gibasa provides 3 main functions: `gibasa::tokenize`, `gibasa::prettify`, and `gibasa::pack`.

![image](man/figures/tidytext_fig5_1_mod.drawio.png)

- `gibasa::tokenize` retrieves a TIF-compliant data.frame of corpus, returning tokens as format that known as 'tidy text data', so that users can replace `tidytext::unnest_tokens` with it for tokenizing CJK text.
- `gibasa::prettify` turns tagged features into columns.
- `gibasa::pack` retrieves a 'tidy text data', typically returning space-separated corpus.

## Installation

You can install binary package via [r-universe](https://paithiov909.r-universe.dev/ui#package:gibasa).

```r
# Enable repository from paithiov909
options(repos = c(
  paithiov909 = "https://paithiov909.r-universe.dev",
  CRAN = "https://cloud.r-project.org"))

# Download and install gibasa in R
install.packages("gibasa")

# Or build from source package
Sys.setenv(MECAB_DEFAULT_RC = "/fullpath/to/your/mecabrc") # if necessary
remotes::install_github("paithiov909/gibasa")
```

To use gibasa package requires the [MeCab](https://taku910.github.io/mecab/) library and its dictionary installed and available.

In case using Linux or OSX, you can install them with their package managers, or build and install from the source by yourself.

In case using Windows, use installer [built for 32bit](https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7WElGUGt6ejlpVXc) or [built for 64bit](https://github.com/ikegami-yukino/mecab/releases/tag/v0.996.2).
Note that gibasa requires a UTF-8 dictionary, not a Shift-JIS one.

## Usage

### Tokenize sentences

```{r}
res <- gibasa::tokenize(
  data.frame(
    doc_id = seq_along(audubon::polano[5:8]),
    text = audubon::polano[5:8]
  )
)
res
```

### Prettify output

```{r}
gibasa::prettify(res)
gibasa::prettify(res, col_select = 1:3)
gibasa::prettify(res, col_select = c(1, 3, 5))
gibasa::prettify(res, col_select = c("POS1", "Original"))
```

### Pack output

```{r}
res <- gibasa::prettify(res)
gibasa::pack(res)

dplyr::mutate(
  res,
  token = dplyr::if_else(is.na(Original), token, Original),
  token = paste(token, POS1, sep = "/")
) |>
  gibasa::pack() |>
  head(1L)
```

### Change dictionary

IPA, [UniDic](https://clrd.ninjal.ac.jp/unidic/), [CC-CEDICT-MeCab](https://github.com/ueda-keisuke/CC-CEDICT-MeCab), and [mecab-ko-dic](https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/) schemes are supported.

```r
## UniDic 2.1.2
gibasa::tokenize("あのイーハトーヴォのすきとおった風", sys_dic = "/Downloads/unidic-lite") |>
  gibasa::prettify(into = gibasa::get_dict_features("unidic26"))
#> # A tibble: 6 × 30
#>   doc_id sentenc…¹ token…² token POS1  POS2  POS3  POS4  cType cForm lForm lemma
#>   <fct>      <int>   <int> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
#> 1 1              1       1 あの  感動… フィ… <NA>  <NA>  <NA>  <NA>  アノ  あの 
#> 2 1              1       2 イー… 名詞  普通… 一般  <NA>  <NA>  <NA>  <NA>  <NA> 
#> 3 1              1       3 の    助詞  格助… <NA>  <NA>  <NA>  <NA>  ノ    の   
#> 4 1              1       4 すき… 動詞  一般  <NA>  <NA>  五段… 連用… スキ… 透き…
#> 5 1              1       5 た    助動… <NA>  <NA>  <NA>  助動… 連体… タ    た   
#> 6 1              1       6 風    名詞  普通… 一般  <NA>  <NA>  <NA>  カゼ  風   
#> # … with 18 more variables: orth <chr>, pron <chr>, orthBase <chr>,
#> #   pronBase <chr>, goshu <chr>, iType <chr>, iForm <chr>, fType <chr>,
#> #   fForm <chr>, kana <chr>, kanaBase <chr>, form <chr>, formBase <chr>,
#> #   iConType <chr>, fConType <chr>, aType <chr>, aConType <chr>,
#> #   aModeType <chr>, and abbreviated variable names ¹​sentence_id, ²​token_id


## CC-CEDICT
gibasa::tokenize("它可以进行日语和汉语的语态分析", sys_dic = "/Downloads/cc-cedict") |> 
  gibasa::prettify(into = gibasa::get_dict_features("cc-cedict"))
#> # A tibble: 9 × 12
#>   doc_id sentenc…¹ token…² token POS1  POS2  POS3  POS4  pinyi…³ tradi…⁴ simpl…⁵
#>   <fct>      <int>   <int> <chr> <chr> <chr> <chr> <chr> <chr>   <chr>   <chr>  
#> 1 1              1       1 它    <NA>  <NA>  <NA>  <NA>  ta1     它      它     
#> 2 1              1       2 可以  <NA>  <NA>  <NA>  <NA>  ke3 yi3 可以    可以   
#> 3 1              1       3 进行  <NA>  <NA>  <NA>  <NA>  jin4 x… 進行    进行   
#> 4 1              1       4 日语  <NA>  <NA>  <NA>  <NA>  Ri4 yu3 日語    日语   
#> 5 1              1       5 和    <NA>  <NA>  <NA>  <NA>  he2     龢      和     
#> 6 1              1       6 汉语  <NA>  <NA>  <NA>  <NA>  Han4 y… 漢語    汉语   
#> 7 1              1       7 的    <NA>  <NA>  <NA>  <NA>  di4     的      的     
#> 8 1              1       8 语态  <NA>  <NA>  <NA>  <NA>  yu3 ta… 語態    语态   
#> 9 1              1       9 分析  <NA>  <NA>  <NA>  <NA>  fen1 x… 分析    分析   
#> # … with 1 more variable: definition <chr>, and abbreviated variable names
#> #   ¹​sentence_id, ²​token_id, ³​pinyin_pron, ⁴​traditional_char_form,
#> #   ⁵​simplified_char_form


## mecab-ko-dic
gibasa::tokenize("하네다공항한정토트백", sys_dic = "/Downloads/mecab-ko-dic") |> 
  gibasa::prettify(into = gibasa::get_dict_features("ko-dic"))
#> # A tibble: 4 × 12
#>   doc_id sentence_id token_id token  POS   meaning prese…¹ reading type  first…²
#>   <fct>        <int>    <int> <chr>  <chr> <chr>   <chr>   <chr>   <chr> <chr>  
#> 1 1                1        1 하네다 NNP   인명    F       하네다  <NA>  <NA>   
#> 2 1                1        2 공항   NNG   장소    T       공항    <NA>  <NA>   
#> 3 1                1        3 한정   NNG   <NA>    T       한정    <NA>  <NA>   
#> 4 1                1        4 토트백 NNG   <NA>    T       토트백  Comp… <NA>   
#> # … with 2 more variables: last_pos <chr>, expression <chr>, and abbreviated
#> #   variable names ¹​presence, ²​first_pos
```

## License

GPL (>=3).
