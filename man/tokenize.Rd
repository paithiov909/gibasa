% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenize}
\alias{tokenize}
\title{Tokenize sentence for data.frame}
\usage{
tokenize(
  tbl,
  text_field = "text",
  docid_field = "doc_id",
  sys_dic = "",
  user_dic = "",
  split = FALSE
)
}
\arguments{
\item{tbl}{A data.frame.}

\item{text_field}{String or symbol; column name where to get texts.}

\item{docid_field}{String or symbol; column name where to get identifiers of texts.}

\item{sys_dic}{Character scalar; path to the system dictionary for mecab.
Note that the system dictionary is expected to be compiled with UTF-8,
not Shift-JIS or other encodings.}

\item{user_dic}{Character scalar; path to the user dictionary for mecab.}

\item{split}{Logical. If true, the function internally split the sentence
into sub-sentences using \code{stringi::stri_split_boudaries(type = "sentence")}.}
}
\value{
data.frame.
}
\description{
Tokenize sentence for data.frame
}
