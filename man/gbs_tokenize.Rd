% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{gbs_tokenize}
\alias{gbs_tokenize}
\title{Tokenize sentence for character vector}
\usage{
gbs_tokenize(
  sentence,
  sys_dic = "",
  user_dic = "",
  split = FALSE,
  mode = c("parse", "wakati")
)
}
\arguments{
\item{sentence}{Character vector to be tokenized.}

\item{sys_dic}{Character scalar; path to the system dictionary for mecab.
Note that the system dictionary is expected to be compiled with UTF-8,
not Shift-JIS or other encodings.}

\item{user_dic}{Character scalar; path to the user dictionary for mecab.}

\item{split}{Logical. If supplied \code{TRUE}, the function internally splits the sentence
into sub-sentences using \code{stringi::stri_split_boudaries(type = "sentence")}.}

\item{mode}{Character scalar to switch output format.}
}
\value{
data.frame or named list.
}
\description{
Tokenize sentence for character vector
}
\examples{
\dontrun{
df <- gbs_tokenize("\u3053\u3093\u306b\u3061\u306f")
head(df)
}
}
